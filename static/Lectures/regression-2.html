<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="regression-2_files/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
&lt;/style&gt;


## Goals for the week

Circle back to the material we covered and walk through regression examples. Also, we will be doing analyses using `brms`, the primary package we will be using in class. 

---
## Basic linear model
Y_i ~ Normal( `\(\mu\)` , `\(\sigma\)` )  [Likelihood we want to estimate]  
`\(\mu_i\)` ~ Normal(0, 5)  [Prior for mu ]  
`\(\sigma\)` ~ HalfCauchy(0,1)  [Prior for sigma]  

Note that this does not have a predictor, it only describes your DV. It says that Y is distributed normally, with a prior centered around 0 for the mean, and a half Cauchy for sigma.  

---
### Simple regression model as an example
What happens when we want to add a predictor?

DV ~ Normal( `\(\mu_i\)` , `\(\sigma\)` )  [Likelihood]  #Note SD not vary   

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` `\(X_i\)` [linear model]  #Note the = not the ~   

Priors  
`\(\beta_0\)` ~ Normal(10, 4)  [prior for intercept]  
`\(\beta_1\)` ~ Normal(0, 5)  [prior for b1]  
`\(\sigma\)`  ~ HalfCauchy(0,1) [prior for sigma]  

---
### Generative model
Together the above equation makes a *generative* model, one that can simulate new observations and analyze your data. It puts a parameter on ALL components that create the data. Your hypothesized Data Generating Process (DGP) is fully described. 

Standard regression models do not go this far. For example, they merely describes sigma rather than estimate sigma. 


---
### Why is this a different formulation? 

1. Because y = mx + b isn't enough, as it doesn't specify all of our parameters

2. Because `\(\epsilon\)` ~ Normal (0, `\(\sigma\)` ) doesn't generalize. While this is what is often provided in intro stats classes (I did it) and even more advanced MLM classes (I'm sure Mike did it), eventually you need to discuss the estimation of other parameters. 


---
## Example
.pull-left[Let's fit some simple data. We are going to use 

```r
library(psychTools)
galton.data &lt;- galton
library(tidyverse)
glimpse(galton.data)
```

```
## Rows: 928
## Columns: 2
## $ parent &lt;dbl&gt; 70.5, 68.5, 65.5, 64.5, 64.0, 67.5, 67.5, 67.5, 66.5, 66.5, 66.…
## $ child  &lt;dbl&gt; 61.7, 61.7, 61.7, 61.7, 61.7, 62.2, 62.2, 62.2, 62.2, 62.2, 62.…
```
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]

---
## Height data

.pull-left[
![](regression-2_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]


---
## Describe our model

C_Height_i ~ Normal( `\(\mu_i\)` , `\(\sigma\)` )  [Likelihood]   

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` ( `\(\text{P_Height}_i\)` - `\({\overline{\mbox{P_Height}}}\)` ) [linear model]


priors  
`\(\beta_0\)` ~ Normal(68, 5)  [prior for intercept]  
`\(\beta_1\)` ~ Normal(0, 5)  [prior for b1]  
`\(\sigma\)`  ~ HalfCauchy(0,1) [prior for sigma]  

---
### Prior for intercept
.pull-left[
![](regression-2_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

.pull-right[
Says we think the mean of male height is between 5 and 6 feet 4 inches feet, most likely. 

If this was too narrow for our likes (maybe we are living in Belgium) then we could change the SD (and mean). 
]

---
### Prior for regression coefficent

.pull-left[
![](regression-2_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]

.pull-right[
This is centered around  0, saying we think prior to collecting any data that we think there is no effect. 

We also do not know if it will be positive or negative.

We are saying it isn't likely to be a b = 20+
]

---
### Prior for sigma

.pull-left[
![](regression-2_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

.pull-right[
We know that variances are going to be positive. So zero and below is not possible. 

What is an upper bound possibility? 

```
## 
## Attaching package: 'psych'
```

```
## The following objects are masked from 'package:ggplot2':
## 
##     %+%, alpha
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```
]

---
## Prior predictive

What do our priors say about what our model expects? This is a way to check if our priors our too liberal or conservative. 

We can take our "guesses" and estimate what they say about our potential results. Helpful to make sure we do not set up a model that creates unreasonable possibilities. 

Our goal is to create an efficient and useful model. One that makes impossible predictions prior to seeing the data isn't too useful.  

---
## Prior predictive

How do we create it? We sample from the priors. Use that to create regression lines (intercept and slope). Then plot.


```r
pp &lt;- tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = 5)) %&gt;% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %&gt;% 
  mutate(c.height = a + b * (height - mean(galton.data$parent))) 
```


---
## Prior predictive
.pull-left[
![](regression-2_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]

.pull-right[
Our priors are not great, as it says we could expect associations that we know to not be true, a priori ]
---
## Prior predictive
We could do a few things: 

1. Constrain the slope to be positive
2. Reduce the uncertainty (SDs) in our priors
3. Leave as is

It really depends on whether the priors are important and/or costly, computationally

---
## Running the model

Use grid estimation for the moment. But again, Bayesian estimation is just counting. 

Define the grid you want to estimate. 
a) the range of parameters you want and b) the number of values. 

Parameter values with more ways that are consistent with data are more plausible. 

---
## Running the model

.pull-left[
We have 3 parameters to estimate (b0, b1, sigma). With grid approximation it is going to be computationally expensive (~ million+ calculations). Try a simple intercept only model instead (only 40k). 

C.Height_i ~ Normal( `\(\mu\)` , `\(\sigma\)` )  
`\(\mu\)` ~ Normal(68, 5)   
`\(\sigma\)` ~ HalfCauchy(0,10)  

]

.right-pull[
Define my grid

```r
grid &lt;-
  crossing(mu = seq(from = 66, to = 69, length.out = 200), sigma = seq(from = 2, to = 3, length.out = 200))
grid
```

```
## # A tibble: 40,000 × 2
##       mu sigma
##    &lt;dbl&gt; &lt;dbl&gt;
##  1    66  2   
##  2    66  2.01
##  3    66  2.01
##  4    66  2.02
##  5    66  2.02
##  6    66  2.03
##  7    66  2.03
##  8    66  2.04
##  9    66  2.04
## 10    66  2.05
## # … with 39,990 more rows
```
]

---
### Grid approximation

.pull-left[

```r
library(purrr)
grid_function &lt;- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %&gt;%
    sum() 
  }

p_grid &lt;-
  grid %&gt;% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;%
  unnest(log_likelihood) %&gt;% 
  mutate(prior_mu  = dnorm(mu, mean = 68, sd  = 5, log = T), prior_sigma = dcauchy(sigma, 0, 10, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% 
  mutate(probability = exp(product - max(product)))
```
]

.pull-right[
1. After defining the grid space we need to calculate the likelihood (are these numbers likely or not, given some distribution)

2. Likelihood of our data (each child's height) assuming each part of our grid is true. So if we take the first line of our grid (mu = 66, sigma = 2) we can see how likely each child (all 928) by dnorm(height, 66, 2) 

]

---
### Grid approximation


```
## # A tibble: 40,000 × 7
##       mu sigma log_likelihood prior_mu prior_sigma product probability
##    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;
##  1    66  2            -2737.    -2.61       -3.49  -2743.   1.98e-245
##  2    66  2.01         -2733.    -2.61       -3.49  -2739.   9.61e-244
##  3    66  2.01         -2729.    -2.61       -3.49  -2735.   4.48e-242
##  4    66  2.02         -2725.    -2.61       -3.49  -2731.   2.00e-240
##  5    66  2.02         -2721.    -2.61       -3.49  -2727.   8.61e-239
##  6    66  2.03         -2718.    -2.61       -3.49  -2724.   3.56e-237
##  7    66  2.03         -2714.    -2.61       -3.49  -2720.   1.42e-235
##  8    66  2.04         -2710.    -2.61       -3.49  -2716.   5.41e-234
##  9    66  2.04         -2707.    -2.61       -3.49  -2713.   1.99e-232
## 10    66  2.05         -2703.    -2.61       -3.49  -2709.   7.07e-231
## # … with 39,990 more rows
```


---
### Grid approximation

3. The log-likelihood for each data point is averaged across all 928 participants  to get the average log-likelihood for each grid spot 

4. We then incorporate the priors for mu and sigma (just multiplying but done with the log scale for math reasons). 

5. The result is a posterior probability for each grid spot we defined. Ie how likely are these parameters, given the data. 

---
### Grid approximation
![](regression-2_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;


---
## sampling to summarize
We will randomly sample from this joint distribution to learn more about it. What we have now are probabilities for each possible parameter. So we can randomly sample based on the probabilities much like randomly sampling different bags of marbles with different amounts of blues in them. 

With samples we can visualize, create credible intervals, HDIs, etc. 

---
## 10k samples
![](regression-2_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---
## Point Estimate and CIs

```
## # A tibble: 2 × 7
##   name  value .lower .upper .width .point .interval
##   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1 mu    68.1   67.9   68.2    0.95 mode   hdi      
## 2 sigma  2.51   2.40   2.63   0.95 mode   hdi
```


```r
tidy(lm(child ~ 1,data = galton.data))
```

```
## # A tibble: 1 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)     68.1    0.0827      824.       0
```


```r
glance(lm(child ~ 1,data = galton.data))
```

```
## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1         0             0  2.52        NA      NA    NA -2173. 4350. 4360.
## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```

---
## brms
.pull-left[
We are going to fit our y ~ x models with the {brms} package. Uses syntax similar to {lme4}. Requires {rstan}.

```r
library(brms)
```


]

.pull-right[
P_Height `\(_i\)` ~ Normal( `\(\mu_i\)` , `\(\sigma\)` )   

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` ( `\(\text{C_Height}_i\)` - `\({\overline{\mbox{C_Height}}}\)` ) 


`\(\beta_0\)` ~ Normal(68, 5)   
`\(\beta_1\)` ~ Normal(0, 5)   
`\(\sigma\)`  ~ HalfCauchy(0,1)   

]

---
## brms


```r
model.name &lt;- # name your fit
  brm(family = gaussian, # what is your likelihood? 
      Y ~ X, # insert model
      prior = prior, # your priors go here
      data = data,  # your data goes here
      iter = 1000, warmup = 500, chains = 4, cores = 4, # wait for this 
      file = "fits/b04.01") # save your samples
```

---
## brms
You can also set aspects of it separately

```r
#formulas
brmsformula()
brmsformula(x~y, family = gaussian())
bf()

#priors
set_prior()
set_prior("normal(0, 5)", class = "b", coef = "parent")
```


---
## brms

```r
m.1 &lt;- 
  brm(family = gaussian,
      child ~ 1 + parent,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.1")
```

---
## brms

```r
summary(m.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: child ~ 1 + parent 
##    Data: galton.data (Number of observations: 928) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    23.79      2.94    18.02    29.42 1.00     1216      722
## parent        0.65      0.04     0.57     0.73 1.00     1208      701
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.24      0.05     2.14     2.35 1.01     1012      626
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
## compare with lm


```r
summary(lm(child ~ parent, data = galton.data))
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

---

```r
plot(m.1)
```

![](regression-2_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---

.pull-left[

```r
plot(conditional_effects(m.1), points = TRUE)
```

![](regression-2_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;

]

.pull-right[

Lots of options for some quick visualizations. And these are all ggplot compatible, so you can change themes, add labels etc. 

But we will largely not use these, instead favoring {tidybayes}
]

---
## Posterior

The posterior is made up of samples. Much like we did with grid approximation, and then sampled from the prior. Behind the scenes brms is using H-MCMC, which we will describe in more detail later. It is basically an algorithm used to define the posterior. It consists only of samples.

Very simplistically, the algorithm tries different potential values (much like we predefined using our grid approximation). But rather than completely random, it chooses depending on whether the parameter is "likely" given the data.

---

```r
library(tidybayes)
get_variables(m.1)
```

```
##  [1] "b_Intercept"   "b_parent"      "sigma"         "lp__"         
##  [5] "accept_stat__" "stepsize__"    "treedepth__"   "n_leapfrog__" 
##  [9] "divergent__"   "energy__"
```

---

```r
#deprecated but it is easy to see
posterior_samples(m.1)
```

```
##      b_Intercept  b_parent    sigma      lp__
## 1       26.14184 0.6140652 2.255588 -2070.462
## 2       24.77563 0.6346659 2.164023 -2071.284
## 3       20.97912 0.6887697 2.278686 -2071.269
## 4       18.17301 0.7303295 2.237121 -2072.250
## 5       24.91738 0.6314072 2.266709 -2070.478
## 6       27.86428 0.5879314 2.236890 -2071.468
## 7       21.49803 0.6823217 2.223765 -2070.529
## 8       22.39003 0.6679660 2.239930 -2070.693
## 9       27.79304 0.5912204 2.189580 -2072.204
## 10      20.38253 0.6966707 2.281314 -2072.422
## 11      21.40952 0.6818863 2.290470 -2071.880
## 12      21.61639 0.6809168 2.176814 -2071.284
## 13      25.81524 0.6187368 2.171414 -2071.127
## 14      24.79258 0.6343531 2.292363 -2070.817
## 15      24.49360 0.6373361 2.218725 -2070.496
## 16      23.33504 0.6563040 2.266785 -2070.837
## 17      24.19313 0.6437812 2.290800 -2071.205
## 18      17.20150 0.7456255 2.236002 -2073.201
## 19      27.87497 0.5883834 2.230563 -2071.131
## 20      21.30039 0.6854613 2.278713 -2070.969
## 21      22.50707 0.6657299 2.226253 -2071.278
## 22      23.93738 0.6476000 2.249167 -2070.789
## 23      22.17916 0.6720252 2.289353 -2070.796
## 24      27.47340 0.5947692 2.286308 -2071.315
## 25      25.32746 0.6262256 2.257064 -2070.310
## 26      21.90176 0.6756792 2.213522 -2070.535
## 27      25.57925 0.6225397 2.258536 -2070.368
## 28      20.27875 0.6992427 2.192815 -2071.499
## 29      27.82470 0.5895724 2.318724 -2072.193
## 30      28.59028 0.5786682 2.223648 -2071.561
## 31      20.38042 0.6984881 2.238362 -2070.895
## 32      22.27236 0.6691407 2.270771 -2071.519
## 33      24.92732 0.6291585 2.273327 -2073.485
## 34      28.22848 0.5822264 2.233937 -2072.041
## 35      25.21615 0.6283681 2.237537 -2070.416
## 36      19.96572 0.7027035 2.264070 -2072.511
## 37      23.88181 0.6474113 2.191513 -2070.482
## 38      23.51347 0.6528823 2.261853 -2070.271
## 39      19.55179 0.7106562 2.215908 -2071.412
## 40      21.90862 0.6754383 2.238051 -2070.501
## 41      22.08892 0.6726902 2.243899 -2070.528
## 42      20.42837 0.6984555 2.254309 -2071.175
## 43      25.93284 0.6162980 2.235717 -2070.657
## 44      24.32081 0.6402550 2.267929 -2070.386
## 45      25.92731 0.6156878 2.274522 -2071.612
## 46      28.49740 0.5791849 2.182516 -2072.099
## 47      28.13290 0.5859569 2.236241 -2071.618
## 48      18.83657 0.7209260 2.260641 -2071.822
## 49      29.66508 0.5624813 2.185195 -2072.747
## 50      29.98889 0.5577707 2.169682 -2073.390
## 51      22.13837 0.6724760 2.309757 -2071.270
## 52      21.41069 0.6841028 2.332915 -2072.355
## 53      23.43840 0.6526875 2.214734 -2070.592
## 54      23.82047 0.6470418 2.191218 -2070.929
## 55      23.61079 0.6491968 2.184881 -2072.279
## 56      21.92423 0.6764188 2.256749 -2070.585
## 57      22.74212 0.6634538 2.266505 -2070.413
## 58      24.32944 0.6409301 2.163588 -2071.152
## 59      19.77407 0.7074941 2.316292 -2072.284
## 60      17.74276 0.7370491 2.270804 -2072.681
## 61      15.96832 0.7640188 2.259664 -2074.644
## 62      21.29940 0.6856363 2.275195 -2071.002
## 63      21.06616 0.6885322 2.221636 -2070.666
## 64      19.75672 0.7067628 2.279458 -2071.740
## 65      28.19395 0.5852672 2.172684 -2072.711
## 66      19.59287 0.7084481 2.244411 -2072.211
## 67      21.12687 0.6851851 2.295850 -2073.356
## 68      22.41917 0.6691095 2.248453 -2070.392
## 69      24.13346 0.6435824 2.214730 -2070.172
## 70      24.51243 0.6383053 2.271518 -2070.399
## 71      23.61910 0.6495000 2.196604 -2071.400
## 72      23.01457 0.6586101 2.202224 -2071.039
## 73      24.10132 0.6442310 2.264097 -2070.270
## 74      26.21190 0.6145778 2.157718 -2072.673
## 75      27.07843 0.6017389 2.107296 -2075.026
## 76      24.29704 0.6421153 2.297804 -2071.218
## 77      23.63644 0.6522531 2.264623 -2071.197
## 78      25.32642 0.6256882 2.191428 -2070.634
## 79      25.17800 0.6294214 2.305340 -2071.648
## 80      17.61709 0.7417060 2.214825 -2076.448
## 81      26.40538 0.6088274 2.265993 -2071.479
## 82      18.66142 0.7229641 2.155059 -2073.417
## 83      19.01568 0.7176208 2.168869 -2072.834
## 84      25.27769 0.6270225 2.189645 -2070.640
## 85      24.66512 0.6360127 2.242393 -2070.168
## 86      23.40450 0.6539383 2.226225 -2070.138
## 87      26.00617 0.6151355 2.233743 -2070.748
## 88      20.80547 0.6923130 2.232582 -2070.723
## 89      27.20833 0.5980316 2.230257 -2070.864
## 90      23.47487 0.6541558 2.246023 -2070.583
## 91      23.95160 0.6447982 2.198916 -2071.149
## 92      21.84917 0.6760126 2.297483 -2071.359
## 93      20.67017 0.6938524 2.322093 -2072.065
## 94      30.10488 0.5560069 2.285010 -2072.838
## 95      18.05743 0.7324841 2.264562 -2072.389
## 96      21.41328 0.6860937 2.245666 -2073.916
## 97      20.68817 0.6948744 2.347932 -2073.252
## 98      18.78828 0.7220581 2.203223 -2072.082
## 99      21.40081 0.6826264 2.227680 -2070.813
## 100     20.24119 0.6996058 2.216966 -2071.329
## 101     26.24552 0.6136375 2.241160 -2070.905
## 102     25.59137 0.6210906 2.222131 -2070.788
## 103     26.35303 0.6103520 2.219505 -2070.697
## 104     20.83561 0.6916911 2.233408 -2070.701
## 105     22.19820 0.6710907 2.245712 -2070.510
## 106     22.04835 0.6750530 2.252090 -2070.847
## 107     25.83192 0.6167558 2.228226 -2071.863
## 108     22.80808 0.6630452 2.238381 -2070.182
## 109     25.55404 0.6217877 2.305120 -2071.440
## 110     26.06345 0.6166013 2.188797 -2071.628
## 111     23.17405 0.6564741 2.288856 -2071.084
## 112     24.57736 0.6375957 2.166059 -2071.223
## 113     24.28499 0.6418773 2.298636 -2070.957
## 114     22.56321 0.6681036 2.260363 -2071.471
## 115     20.38047 0.6988301 2.321856 -2072.218
## 116     26.57565 0.6080480 2.164262 -2071.585
## 117     27.63099 0.5935684 2.201873 -2071.890
## 118     16.60400 0.7518387 2.154383 -2076.568
## 119     20.12676 0.7015665 2.156884 -2072.425
## 120     21.54619 0.6808837 2.291630 -2071.081
## 121     21.60601 0.6792521 2.207540 -2071.234
## 122     23.83822 0.6485728 2.181190 -2070.924
## 123     20.87364 0.6914689 2.262973 -2070.846
## 124     20.90783 0.6898208 2.195573 -2071.320
## 125     22.21976 0.6704297 2.185340 -2071.265
## 126     20.15246 0.7015368 2.181628 -2071.617
## 127     19.54944 0.7103807 2.185691 -2071.855
## 128     24.09458 0.6432348 2.223609 -2070.405
## 129     24.93973 0.6330629 2.229532 -2070.980
## 130     25.33639 0.6252349 2.228783 -2070.401
## 131     24.47686 0.6391880 2.229368 -2070.342
## 132     22.75595 0.6633117 2.293551 -2070.817
## 133     20.09606 0.7021722 2.303744 -2071.855
## 134     24.27702 0.6423132 2.227707 -2070.482
## 135     25.46858 0.6230687 2.288590 -2071.054
## 136     26.06499 0.6141930 2.217622 -2070.904
## 137     26.73972 0.6073381 2.244718 -2072.286
## 138     21.37684 0.6823272 2.214655 -2071.564
## 139     20.22613 0.6988359 2.226941 -2072.412
## 140     15.08263 0.7741502 2.204894 -2076.736
## 141     16.12312 0.7597947 2.253441 -2074.293
## 142     26.27980 0.6131009 2.241942 -2070.884
## 143     25.53266 0.6213249 2.235130 -2071.481
## 144     27.65452 0.5930235 2.165274 -2072.491
## 145     24.58550 0.6374029 2.222411 -2070.268
## 146     23.09902 0.6591494 2.335060 -2071.959
## 147     25.82990 0.6181885 2.277442 -2070.717
## 148     25.76956 0.6173004 2.362103 -2074.960
## 149     20.41207 0.6976439 2.293195 -2071.467
## 150     22.26600 0.6716138 2.255470 -2070.615
## 151     23.74790 0.6475337 2.240479 -2071.183
## 152     21.96062 0.6765015 2.220241 -2071.048
## 153     25.64153 0.6192745 2.285739 -2072.624
## 154     26.02789 0.6154747 2.229856 -2070.405
## 155     21.32677 0.6845294 2.238043 -2070.521
## 156     24.07438 0.6442466 2.223645 -2070.116
## 157     26.80656 0.6023926 2.271528 -2072.478
## 158     28.63945 0.5772027 2.212507 -2071.669
## 159     23.28974 0.6548108 2.185524 -2071.060
## 160     26.42183 0.6100903 2.211780 -2070.591
## 161     28.62283 0.5779381 2.320544 -2072.650
## 162     23.27486 0.6560244 2.213014 -2070.209
## 163     23.56327 0.6525151 2.272261 -2070.541
## 164     29.62825 0.5640850 2.201621 -2072.853
## 165     18.88251 0.7209099 2.245037 -2071.874
## 166     22.18110 0.6719736 2.237111 -2070.285
## 167     25.04725 0.6308047 2.273542 -2070.624
## 168     23.85472 0.6462981 2.228454 -2070.790
## 169     23.94620 0.6470159 2.268741 -2070.553
## 170     19.19317 0.7150268 2.198650 -2072.062
## 171     24.61934 0.6370289 2.224908 -2070.322
## 172     26.69221 0.6054679 2.224220 -2070.740
## 173     28.32132 0.5828624 2.106048 -2075.091
## 174     25.97256 0.6174069 2.157354 -2071.885
## 175     27.11817 0.5996251 2.254731 -2070.798
## 176     20.54903 0.6955896 2.266431 -2071.019
## 177     22.57269 0.6665105 2.263312 -2070.360
## 178     24.23438 0.6423414 2.208238 -2070.280
## 179     23.88375 0.6479833 2.274061 -2070.658
## 180     24.65026 0.6367374 2.203456 -2070.608
## 181     23.07674 0.6617179 2.229478 -2073.500
## 182     21.60459 0.6800290 2.215300 -2070.604
## 183     26.00155 0.6175708 2.248566 -2071.244
## 184     24.20079 0.6443198 2.269281 -2071.696
## 185     23.47991 0.6543358 2.259963 -2070.918
## 186     22.80571 0.6634846 2.266085 -2070.478
## 187     24.93156 0.6317470 2.234903 -2070.149
## 188     22.91674 0.6621379 2.239833 -2070.472
## 189     19.55563 0.7110891 2.219564 -2071.549
## 190     21.07496 0.6861441 2.178432 -2073.245
## 191     22.85317 0.6630725 2.185201 -2070.985
## 192     21.46831 0.6823139 2.227296 -2070.500
## 193     22.45183 0.6682971 2.287956 -2070.734
## 194     24.71507 0.6347422 2.179605 -2070.751
## 195     23.36732 0.6548913 2.221747 -2070.159
## 196     23.74701 0.6498312 2.249252 -2070.330
## 197     21.23767 0.6857446 2.157379 -2071.788
## 198     24.01583 0.6456398 2.254714 -2070.236
## 199     24.19720 0.6420063 2.246037 -2070.237
## 200     23.13937 0.6595814 2.219595 -2071.238
## 201     23.40405 0.6553155 2.232720 -2070.692
## 202     25.28407 0.6272563 2.203842 -2070.555
## 203     26.13136 0.6153233 2.197672 -2071.183
## 204     20.39163 0.6964674 2.251069 -2072.240
## 205     19.34514 0.7125147 2.276989 -2072.126
## 206     23.37095 0.6541834 2.201188 -2070.421
## 207     23.85055 0.6484178 2.299938 -2071.086
## 208     21.81489 0.6768764 2.152696 -2071.876
## 209     22.21038 0.6714436 2.147484 -2071.840
## 210     24.23304 0.6424965 2.336737 -2071.954
## 211     22.19283 0.6727789 2.222065 -2070.676
## 212     21.58847 0.6819474 2.232707 -2071.100
## 213     24.42508 0.6382469 2.235662 -2070.507
## 214     24.52331 0.6382786 2.342608 -2072.182
## 215     25.63858 0.6212757 2.160509 -2071.398
## 216     27.11651 0.5987299 2.236435 -2071.259
## 217     19.77467 0.7072559 2.247950 -2071.206
## 218     21.20671 0.6868406 2.282353 -2071.060
## 219     18.02034 0.7329031 2.231869 -2072.322
## 220     23.95353 0.6464509 2.232332 -2070.136
## 221     24.03177 0.6453664 2.209817 -2070.279
## 222     23.65976 0.6495916 2.208900 -2070.522
## 223     23.66452 0.6505927 2.256042 -2070.195
## 224     24.16267 0.6432895 2.230900 -2070.116
## 225     23.63481 0.6500791 2.242806 -2070.312
## 226     22.41859 0.6681761 2.180329 -2070.898
## 227     22.19582 0.6717300 2.411867 -2075.361
## 228     27.62966 0.5913659 2.135958 -2073.456
## 229     22.43982 0.6707040 2.258746 -2072.864
## 230     22.37785 0.6705835 2.272398 -2071.327
## 231     25.16382 0.6281326 2.229020 -2070.221
## 232     28.24425 0.5833717 2.305882 -2072.067
## 233     19.38163 0.7140794 2.202915 -2072.167
## 234     24.96951 0.6334362 2.277241 -2072.461
## 235     27.55620 0.5934733 2.225177 -2070.942
## 236     27.17702 0.5977556 2.361366 -2073.935
## 237     24.00502 0.6453013 2.132128 -2072.230
## 238     28.68413 0.5788682 2.308736 -2073.933
## 239     28.23657 0.5842187 2.185524 -2072.051
## 240     28.15350 0.5854092 2.312836 -2072.411
## 241     20.80365 0.6924466 2.203260 -2070.954
## 242     24.62057 0.6356564 2.281846 -2070.706
## 243     26.03608 0.6157753 2.140661 -2072.187
## 244     28.38187 0.5799389 2.175321 -2072.992
## 245     17.30493 0.7430931 2.235749 -2072.918
## 246     30.38224 0.5518376 2.214747 -2072.868
## 247     28.71970 0.5758422 2.172234 -2072.531
## 248     23.77930 0.6478960 2.298241 -2071.031
## 249     26.60065 0.6072975 2.275516 -2070.812
## 250     23.33566 0.6566796 2.177802 -2071.816
## 251     21.59030 0.6803251 2.275498 -2070.771
## 252     24.19726 0.6412724 2.221568 -2070.841
## 253     23.74287 0.6494010 2.247512 -2070.133
## 254     25.85866 0.6185797 2.231833 -2070.370
## 255     24.02842 0.6445639 2.217931 -2070.234
## 256     24.74939 0.6350530 2.243204 -2070.283
## 257     26.17979 0.6141464 2.307880 -2071.457
## 258     24.87543 0.6338636 2.180249 -2071.427
## 259     25.41810 0.6232817 2.319304 -2072.244
## 260     23.75880 0.6502509 2.186045 -2071.308
## 261     24.13100 0.6443475 2.188639 -2070.818
## 262     21.53992 0.6787822 2.177764 -2074.306
## 263     26.87305 0.6061693 2.284603 -2074.217
## 264     20.05727 0.7017858 2.243476 -2071.810
## 265     27.99982 0.5890458 2.237875 -2073.084
## 266     21.28502 0.6839271 2.265937 -2071.327
## 267     24.94480 0.6330241 2.205075 -2071.212
## 268     24.60388 0.6363814 2.211564 -2070.242
## 269     24.50892 0.6365029 2.229920 -2071.074
## 270     27.82099 0.5913062 2.312758 -2073.299
## 271     21.03977 0.6885619 2.342090 -2072.546
## 272     28.48835 0.5806851 2.262466 -2071.851
## 273     24.17205 0.6434430 2.296261 -2070.862
## 274     22.41103 0.6702304 2.189152 -2071.737
## 275     26.48476 0.6062571 2.320017 -2074.936
## 276     25.21764 0.6279392 2.215431 -2070.309
## 277     20.49313 0.6963121 2.239695 -2070.920
## 278     21.71946 0.6789376 2.246617 -2070.428
## 279     22.39647 0.6699193 2.189999 -2071.114
## 280     22.53227 0.6668605 2.206335 -2070.376
## 281     22.58749 0.6663278 2.207240 -2070.378
## 282     21.92643 0.6760085 2.203338 -2070.571
## 283     28.44003 0.5799118 2.268702 -2071.668
## 284     16.35658 0.7584907 2.210132 -2074.622
## 285     25.78794 0.6190899 2.358360 -2072.867
## 286     22.87253 0.6623321 2.234793 -2070.229
## 287     25.37394 0.6260033 2.283051 -2070.804
## 288     21.89560 0.6764314 2.218800 -2070.426
## 289     28.59632 0.5785050 2.258889 -2071.573
## 290     19.32160 0.7132129 2.140011 -2073.591
## 291     29.05854 0.5724278 2.331800 -2073.619
## 292     24.24187 0.6410208 2.290939 -2070.959
## 293     21.53581 0.6828584 2.188641 -2071.743
## 294     17.92017 0.7328224 2.279272 -2073.660
## 295     29.62054 0.5644624 2.172844 -2073.732
## 296     25.32353 0.6262658 2.293960 -2070.827
## 297     24.96889 0.6312850 2.166258 -2071.091
## 298     22.50911 0.6685942 2.218494 -2071.072
## 299     25.44677 0.6233106 2.154441 -2071.962
## 300     30.39324 0.5503905 2.252064 -2073.717
## 301     25.94648 0.6164706 2.223124 -2070.475
## 302     23.49792 0.6531283 2.294937 -2070.779
## 303     18.50391 0.7275212 2.200272 -2073.524
## 304     20.44592 0.6986956 2.217749 -2071.639
## 305     23.78058 0.6473480 2.305951 -2071.653
## 306     27.19064 0.5969216 2.261460 -2072.291
## 307     27.86901 0.5879421 2.319850 -2072.561
## 308     27.10322 0.5994667 2.133035 -2073.039
## 309     20.34869 0.6994165 2.312692 -2072.032
## 310     26.29856 0.6112526 2.189632 -2070.993
## 311     27.00079 0.6026267 2.290305 -2071.682
## 312     26.21688 0.6121805 2.231541 -2070.711
## 313     21.73004 0.6799949 2.200606 -2071.454
## 314     24.83553 0.6347936 2.280539 -2071.544
## 315     24.64053 0.6377827 2.220997 -2071.452
## 316     24.01845 0.6472356 2.220131 -2072.014
## 317     25.87117 0.6198376 2.170475 -2072.609
## 318     26.45900 0.6097254 2.186383 -2070.999
## 319     30.71102 0.5473147 2.315789 -2073.929
## 320     28.45978 0.5794581 2.302792 -2072.329
## 321     19.89086 0.7054230 2.223860 -2071.171
## 322     21.29858 0.6858559 2.261120 -2070.985
## 323     22.65970 0.6637620 2.229187 -2070.913
## 324     23.05200 0.6579520 2.264629 -2071.063
## 325     23.01473 0.6589475 2.254343 -2070.552
## 326     24.44865 0.6396077 2.242091 -2070.343
## 327     25.51394 0.6217580 2.204153 -2071.473
## 328     25.08609 0.6281434 2.167576 -2071.979
## 329     26.00499 0.6158554 2.253804 -2070.439
## 330     23.68529 0.6505465 2.220485 -2070.244
## 331     22.07469 0.6733414 2.233060 -2070.338
## 332     26.41073 0.6118217 2.283254 -2072.005
## 333     24.46450 0.6379113 2.246021 -2070.351
## 334     20.49915 0.6971367 2.172456 -2071.762
## 335     20.64452 0.6950161 2.173179 -2071.680
## 336     26.20526 0.6122621 2.287806 -2071.231
## 337     23.33816 0.6561269 2.176299 -2071.254
## 338     25.62947 0.6208650 2.267536 -2070.674
## 339     29.07215 0.5710993 2.199473 -2072.062
## 340     24.28263 0.6402855 2.272177 -2070.770
## 341     27.36778 0.5958897 2.257002 -2070.933
## 342     26.41012 0.6092656 2.224629 -2070.859
## 343     22.09406 0.6725132 2.182957 -2071.138
## 344     22.09406 0.6725132 2.182957 -2071.138
## 345     22.71227 0.6655180 2.234370 -2070.854
## 346     21.45730 0.6834665 2.184203 -2071.317
## 347     30.01307 0.5567074 2.295321 -2073.179
## 348     20.10752 0.7033690 2.179306 -2072.123
## 349     25.07340 0.6311059 2.220871 -2071.036
## 350     21.41090 0.6842921 2.227718 -2070.926
## 351     19.54879 0.7109012 2.198867 -2071.661
## 352     27.91555 0.5881153 2.257744 -2071.159
## 353     31.15785 0.5411717 2.201578 -2073.799
## 354     17.71018 0.7352261 2.250863 -2074.682
## 355     21.71128 0.6794408 2.194910 -2070.850
## 356     20.42883 0.6982163 2.346334 -2073.024
## 357     20.24085 0.7005775 2.357325 -2073.409
## 358     26.84595 0.6039466 2.142579 -2072.394
## 359     24.41313 0.6395500 2.147019 -2071.663
## 360     25.77049 0.6199740 2.152810 -2071.752
## 361     22.28214 0.6685127 2.171148 -2073.014
## 362     23.50976 0.6505826 2.156126 -2073.247
## 363     19.45883 0.7140267 2.347973 -2075.245
## 364     20.00312 0.7055055 2.307478 -2072.968
## 365     26.24294 0.6117612 2.163518 -2071.798
## 366     24.30224 0.6409453 2.237599 -2070.096
## 367     23.66592 0.6502666 2.204147 -2070.277
## 368     23.22103 0.6559886 2.282157 -2070.806
## 369     23.04399 0.6588082 2.210930 -2070.417
## 370     30.27919 0.5548676 2.425529 -2078.736
## 371     29.34758 0.5682659 2.360438 -2074.844
## 372     21.31786 0.6842319 2.040518 -2078.928
## 373     22.46235 0.6680619 2.107649 -2073.594
## 374     21.04244 0.6882488 2.350759 -2072.927
## 375     23.73693 0.6491267 2.089130 -2074.549
## 376     23.42020 0.6541515 2.124874 -2072.599
## 377     25.74981 0.6191140 2.332736 -2072.126
## 378     26.83894 0.6041866 2.244223 -2070.662
## 379     22.10110 0.6727349 2.226098 -2070.419
## 380     25.36433 0.6265020 2.253339 -2070.720
## 381     22.32941 0.6692440 2.203205 -2070.630
## 382     21.27719 0.6821169 2.265446 -2074.883
## 383     26.57712 0.6073406 2.383825 -2074.214
## 384     20.84324 0.6909765 2.136752 -2072.896
## 385     18.36171 0.7294101 2.245755 -2073.019
## 386     21.44532 0.6838973 2.273929 -2071.234
## 387     28.12510 0.5839694 2.184479 -2072.313
## 388     20.95148 0.6927773 2.285180 -2074.199
## 389     25.31267 0.6227644 2.241530 -2075.394
## 390     27.79804 0.5870236 2.214515 -2074.711
## 391     21.11666 0.6895357 2.287919 -2072.589
## 392     23.67187 0.6501658 2.229903 -2070.098
## 393     26.63396 0.6080759 2.236817 -2071.142
## 394     23.09161 0.6583910 2.248149 -2070.209
## 395     23.40420 0.6549323 2.291556 -2070.919
## 396     21.61978 0.6818269 2.192795 -2071.900
## 397     31.49174 0.5354534 2.318910 -2074.724
## 398     30.55688 0.5474120 2.212020 -2074.973
## 399     22.66293 0.6637488 2.364421 -2073.603
## 400     27.86087 0.5882993 2.221656 -2071.293
## 401     22.24804 0.6719464 2.271536 -2070.825
## 402     16.39299 0.7571507 2.148834 -2075.573
## 403     20.70856 0.6950393 2.207984 -2071.846
## 404     20.64838 0.6943366 2.289710 -2071.278
## 405     23.88033 0.6473937 2.230153 -2070.109
## 406     23.62097 0.6510396 2.263969 -2070.242
## 407     26.00472 0.6154963 2.215612 -2070.597
## 408     19.81076 0.7077590 2.248773 -2071.651
## 409     24.86873 0.6315873 2.202529 -2070.925
## 410     22.44018 0.6694379 2.275421 -2071.100
## 411     25.26175 0.6272002 2.266358 -2070.388
## 412     22.75096 0.6642228 2.175485 -2070.999
## 413     25.30129 0.6254202 2.276774 -2070.907
## 414     25.85629 0.6188777 2.319997 -2071.698
## 415     20.56067 0.6969507 2.164826 -2072.501
## 416     20.96323 0.6893945 2.319414 -2071.933
## 417     24.30086 0.6429591 2.293676 -2072.226
## 418     23.32375 0.6552029 2.186164 -2070.584
## 419     22.05957 0.6739473 2.266973 -2070.493
## 420     21.72766 0.6776223 2.186871 -2071.369
## 421     25.83412 0.6198961 2.276577 -2071.310
## 422     22.68516 0.6634484 2.192925 -2071.208
## 423     21.35095 0.6856088 2.278146 -2071.668
## 424     22.59880 0.6660232 2.220506 -2070.246
## 425     22.59880 0.6660232 2.220506 -2070.246
## 426     25.95316 0.6177648 2.196297 -2071.009
## 427     26.15599 0.6160877 2.203610 -2072.744
## 428     21.71378 0.6780073 2.327383 -2072.155
## 429     25.84685 0.6175742 2.209574 -2070.761
## 430     23.39946 0.6559852 2.282572 -2071.799
## 431     23.17955 0.6578594 2.226779 -2070.214
## 432     25.82791 0.6190319 2.168432 -2071.253
## 433     26.89247 0.6043082 2.253613 -2071.292
## 434     26.88533 0.6043880 2.251493 -2071.254
## 435     20.00206 0.7026911 2.191743 -2072.161
## 436     20.00206 0.7026911 2.191743 -2072.161
## 437     26.83254 0.6036336 2.284478 -2071.080
## 438     22.62088 0.6660997 2.184187 -2070.816
## 439     24.71824 0.6330567 2.258564 -2071.708
## 440     24.08611 0.6438290 2.246169 -2070.160
## 441     21.28480 0.6853862 2.274344 -2070.816
## 442     20.73921 0.6933748 2.281992 -2071.125
## 443     25.20591 0.6281797 2.301714 -2071.022
## 444     23.79863 0.6480705 2.202943 -2070.329
## 445     23.24628 0.6564685 2.259693 -2070.224
## 446     25.35132 0.6269222 2.312318 -2071.884
## 447     23.03739 0.6597406 2.246452 -2070.181
## 448     23.33536 0.6552323 2.202113 -2070.322
## 449     21.74407 0.6778560 2.294676 -2071.143
## 450     28.37456 0.5806633 2.310567 -2072.490
## 451     27.14029 0.5994942 2.183545 -2071.283
## 452     21.66987 0.6782265 2.112653 -2074.344
## 453     24.43394 0.6386844 2.189049 -2070.587
## 454     24.50608 0.6378796 2.281267 -2070.493
## 455     23.87454 0.6455725 2.296148 -2071.928
## 456     23.90236 0.6452766 2.269416 -2071.355
## 457     23.85623 0.6480277 2.317390 -2071.361
## 458     27.28265 0.5983000 2.291008 -2071.638
## 459     21.72344 0.6785087 2.262126 -2070.544
## 460     23.09836 0.6583547 2.227249 -2070.175
## 461     25.22692 0.6263081 2.276123 -2071.066
## 462     24.36387 0.6393526 2.308494 -2071.273
## 463     21.89062 0.6762375 2.274628 -2070.624
## 464     25.61247 0.6229384 2.219255 -2070.839
## 465     20.03317 0.7022724 2.257434 -2071.734
## 466     20.05056 0.7037160 2.304146 -2071.931
## 467     20.92457 0.6910025 2.253434 -2070.856
## 468     29.04518 0.5706888 2.329973 -2073.510
## 469     19.30564 0.7141338 2.155210 -2072.823
## 470     25.62182 0.6218390 2.365935 -2073.136
## 471     24.41080 0.6416475 2.377521 -2075.383
## 472     25.50779 0.6237755 2.282784 -2070.704
## 473     28.09861 0.5839438 2.269794 -2072.348
## 474     24.92710 0.6322744 2.261920 -2070.343
## 475     24.58891 0.6367471 2.199687 -2070.357
## 476     21.98340 0.6749215 2.261144 -2070.445
## 477     24.99107 0.6307306 2.210619 -2070.291
## 478     22.63077 0.6653855 2.250557 -2070.239
## 479     25.61021 0.6198542 2.233802 -2072.041
## 480     26.56705 0.6079500 2.200747 -2070.766
## 481     19.88910 0.7063317 2.278459 -2071.664
## 482     19.46856 0.7117178 2.263324 -2071.468
## 483     21.28412 0.6854618 2.310670 -2071.539
## 484     25.85574 0.6182722 2.140189 -2072.148
## 485     25.95495 0.6164032 2.077523 -2075.707
## 486     19.98558 0.7048983 2.263562 -2071.429
## 487     18.92214 0.7219885 2.310780 -2074.670
## 488     19.67786 0.7101733 2.278651 -2072.484
## 489     17.63589 0.7404021 2.293680 -2074.521
## 490     21.59139 0.6799853 2.216397 -2070.718
## 491     23.15091 0.6570700 2.302912 -2071.184
## 492     22.96279 0.6595579 2.182906 -2071.183
## 493     25.60378 0.6199281 2.215622 -2072.181
## 494     24.60921 0.6374493 2.298825 -2071.179
## 495     27.08493 0.6013652 2.259950 -2071.286
## 496     20.59970 0.6950685 2.118816 -2073.641
## 497     25.76564 0.6168381 2.275779 -2073.785
## 498     25.56109 0.6221175 2.234364 -2070.351
## 499     22.52157 0.6673579 2.282865 -2070.650
## 500     27.15006 0.5987896 2.200566 -2071.130
## 501     27.82495 0.5877901 2.299208 -2072.880
## 502     20.71355 0.6956924 2.191704 -2073.281
## 503     26.75038 0.6027597 2.272284 -2073.311
## 504     26.19304 0.6118147 2.230834 -2071.440
## 505     16.65545 0.7526799 2.243196 -2073.463
## 506     18.43233 0.7259941 2.256828 -2072.401
## 507     28.57081 0.5790337 2.165488 -2072.590
## 508     23.12855 0.6592980 2.215284 -2070.752
## 509     29.16609 0.5701273 2.242013 -2071.845
## 510     28.38353 0.5816946 2.166096 -2072.426
## 511     21.23165 0.6846945 2.233809 -2071.221
## 512     22.97937 0.6605728 2.283278 -2070.572
## 513     24.24352 0.6406723 2.306278 -2071.565
## 514     27.00699 0.6006514 2.257817 -2071.028
## 515     26.30978 0.6126121 2.270660 -2071.049
## 516     25.15625 0.6285449 2.337331 -2071.966
## 517     24.15688 0.6428859 2.226902 -2070.130
## 518     23.28555 0.6561025 2.230534 -2070.137
## 519     23.97109 0.6452888 2.198304 -2070.493
## 520     25.25803 0.6276443 2.274215 -2070.622
## 521     24.00377 0.6447287 2.255237 -2070.339
## 522     23.14578 0.6575107 2.313180 -2071.259
## 523     24.86360 0.6337001 2.243924 -2070.500
## 524     25.18718 0.6272331 2.282272 -2070.864
## 525     25.04455 0.6303616 2.200866 -2070.410
## 526     17.98500 0.7342080 2.219033 -2072.688
## 527     27.11092 0.5986113 2.256880 -2071.527
## 528     21.92933 0.6776951 2.239796 -2072.017
## 529     20.84632 0.6928092 2.148241 -2072.970
## 530     21.66956 0.6809087 2.127356 -2073.714
## 531     21.68538 0.6796609 2.114036 -2073.514
## 532     22.72889 0.6649414 2.265807 -2070.695
## 533     25.04937 0.6297330 2.259041 -2070.316
## 534     23.26333 0.6563371 2.280618 -2070.489
## 535     23.33427 0.6569835 2.215633 -2071.627
## 536     26.78381 0.6047497 2.198482 -2070.874
## 537     23.60390 0.6524208 2.271477 -2070.925
## 538     20.97255 0.6885805 2.265492 -2071.349
## 539     26.36366 0.6120461 2.205676 -2071.265
## 540     24.31805 0.6407580 2.255413 -2070.168
## 541     26.25320 0.6128176 2.208393 -2070.623
## 542     23.62769 0.6508245 2.266001 -2070.264
## 543     23.73088 0.6489946 2.228799 -2070.158
## 544     23.73088 0.6489946 2.228799 -2070.158
## 545     19.40549 0.7118636 2.239616 -2071.659
## 546     32.67441 0.5187984 2.261050 -2074.963
## 547     19.95255 0.7041601 2.270797 -2071.393
## 548     20.53978 0.6959485 2.304896 -2071.637
## 549     27.36554 0.5964863 2.247154 -2070.881
## 550     20.81452 0.6914607 2.287339 -2071.301
## 551     27.19872 0.5972209 2.245961 -2071.636
## 552     23.56170 0.6517237 2.199113 -2070.352
## 553     24.65030 0.6361459 2.230549 -2070.143
## 554     26.51615 0.6093686 2.255625 -2070.808
## 555     20.91894 0.6895244 2.233156 -2071.090
## 556     21.89414 0.6753930 2.235819 -2070.665
## 557     21.92200 0.6760491 2.187009 -2070.827
## 558     27.15806 0.5990567 2.287311 -2071.208
## 559     24.24174 0.6402149 2.268717 -2071.486
## 560     22.28163 0.6706422 2.237460 -2070.263
## 561     22.12351 0.6737045 2.226479 -2070.604
## 562     23.56505 0.6531008 2.176841 -2071.536
## 563     25.25363 0.6258869 2.280570 -2071.166
## 564     29.71259 0.5626552 2.266031 -2072.577
## 565     25.66086 0.6208602 2.226944 -2070.320
## 566     26.22906 0.6115280 2.224834 -2071.176
## 567     21.72608 0.6802050 2.211463 -2071.514
## 568     21.27874 0.6845303 2.277325 -2071.051
## 569     25.06999 0.6286415 2.223152 -2070.764
## 570     25.31108 0.6287474 2.208262 -2073.107
## 571     20.91870 0.6904084 2.268292 -2070.851
## 572     20.34652 0.6986716 2.246286 -2070.940
## 573     24.00012 0.6465826 2.185949 -2071.155
## 574     25.42737 0.6251052 2.166208 -2071.311
## 575     22.29483 0.6691833 2.278704 -2071.188
## 576     26.02663 0.6180722 2.125949 -2075.313
## 577     18.53578 0.7233338 2.337447 -2075.234
## 578     23.29003 0.6537788 2.171415 -2072.829
## 579     27.00344 0.6027969 2.185089 -2071.962
## 580     22.40210 0.6672826 2.297355 -2071.871
## 581     26.28938 0.6110664 2.270291 -2070.967
## 582     21.57940 0.6812327 2.247356 -2070.527
## 583     23.11934 0.6567989 2.273840 -2071.363
## 584     23.65795 0.6513257 2.331083 -2071.993
## 585     23.95552 0.6448631 2.186499 -2071.223
## 586     22.71444 0.6627729 2.165477 -2072.141
## 587     20.94684 0.6881067 2.257580 -2072.426
## 588     26.69131 0.6080220 2.289846 -2072.659
## 589     27.96771 0.5894395 2.290496 -2073.343
## 590     20.20021 0.6993916 2.224021 -2072.176
## 591     26.97351 0.6049928 2.257647 -2074.726
## 592     18.76176 0.7215016 2.342003 -2073.723
## 593     22.52173 0.6645148 2.289937 -2073.420
## 594     22.82530 0.6603534 2.282891 -2072.704
## 595     25.89359 0.6191192 2.255313 -2071.214
## 596     26.26582 0.6136571 2.175227 -2072.009
## 597     20.60409 0.6932374 2.298327 -2072.906
## 598     21.93977 0.6740107 2.295886 -2071.985
## 599     24.26109 0.6433449 2.185105 -2071.937
## 600     26.41488 0.6092858 2.184411 -2071.297
## 601     22.61307 0.6671540 2.217935 -2071.156
## 602     23.55004 0.6529316 2.193869 -2070.801
## 603     20.67493 0.6944760 2.194418 -2071.181
## 604     27.27272 0.5983471 2.215950 -2071.147
## 605     22.99731 0.6594509 2.217176 -2070.393
## 606     27.76137 0.5910637 2.224294 -2071.230
## 607     19.64325 0.7086362 2.220565 -2071.442
## 608     28.97621 0.5721768 2.195562 -2072.148
## 609     21.65537 0.6819854 2.148739 -2074.309
## 610     21.73446 0.6809816 2.151196 -2074.527
## 611     16.46978 0.7553716 2.242515 -2073.640
## 612     30.08734 0.5574137 2.261464 -2073.002
## 613     24.46692 0.6400133 2.256692 -2071.031
## 614     20.83523 0.6913060 2.236619 -2070.778
## 615     25.78605 0.6201684 2.253591 -2070.676
## 616     26.79711 0.6048328 2.280155 -2070.985
## 617     26.08792 0.6152971 2.274049 -2070.707
## 618     21.87026 0.6769440 2.225221 -2070.434
## 619     24.58929 0.6365885 2.258020 -2070.227
## 620     21.76956 0.6768375 2.232605 -2071.043
## 621     25.64904 0.6212816 2.224583 -2070.293
## 622     27.34391 0.5955878 2.290750 -2071.687
## 623     18.46274 0.7246815 2.308695 -2074.082
## 624     27.88166 0.5897969 2.170486 -2072.557
## 625     19.91407 0.7046317 2.277487 -2071.538
## 626     28.06109 0.5864981 2.144297 -2073.012
## 627     29.17793 0.5698621 2.138757 -2073.866
## 628     21.81948 0.6774279 2.345145 -2072.419
## 629     23.79620 0.6460671 2.284017 -2072.804
## 630     26.29490 0.6118410 2.249072 -2070.466
## 631     27.78116 0.5884280 2.286620 -2072.633
## 632     26.30524 0.6125266 2.234670 -2070.728
## 633     21.77111 0.6777394 2.214549 -2070.513
## 634     23.48071 0.6525110 2.188940 -2070.636
## 635     23.67550 0.6507589 2.331094 -2071.812
## 636     26.08839 0.6156319 2.237587 -2070.620
## 637     21.75019 0.6777112 2.221533 -2070.609
## 638     25.47265 0.6243280 2.203389 -2070.517
## 639     22.23837 0.6706715 2.274362 -2070.659
## 640     26.75540 0.6034945 2.217291 -2071.828
## 641     19.29352 0.7153315 2.154502 -2073.362
## 642     27.41415 0.5940848 2.287663 -2072.119
## 643     26.91877 0.6005477 2.254163 -2072.742
## 644     32.06401 0.5280168 2.189022 -2075.001
## 645     20.79419 0.6926983 2.161356 -2071.896
## 646     21.18011 0.6863333 2.208847 -2070.771
## 647     25.89616 0.6164843 2.213865 -2071.067
## 648     22.78660 0.6636264 2.192885 -2070.607
## 649     22.10478 0.6740310 2.219736 -2070.682
## 650     22.10478 0.6740310 2.219736 -2070.682
## 651     23.60784 0.6506040 2.264873 -2070.387
## 652     24.45405 0.6412414 2.355452 -2074.901
## 653     21.59430 0.6792400 2.196296 -2071.607
## 654     28.96683 0.5742101 2.281351 -2072.863
## 655     28.26585 0.5826570 2.236384 -2071.329
## 656     26.83644 0.6034667 2.268594 -2070.895
## 657     20.41888 0.6978930 2.264848 -2071.014
## 658     28.38504 0.5815565 2.206474 -2071.561
## 659     23.43410 0.6540901 2.205504 -2070.334
## 660     24.61662 0.6359705 2.202070 -2070.413
## 661     15.42869 0.7692153 2.244783 -2075.786
## 662     27.61628 0.5959606 2.260400 -2076.027
## 663     28.39016 0.5801956 2.246576 -2071.791
## 664     24.79751 0.6340552 2.252827 -2070.222
## 665     22.96390 0.6607524 2.311031 -2071.161
## 666     26.08079 0.6139928 2.194688 -2071.153
## 667     21.61041 0.6814883 2.234559 -2070.952
## 668     25.44163 0.6252747 2.182665 -2071.165
## 669     22.95945 0.6594832 2.324285 -2072.074
## 670     21.52297 0.6803154 2.235280 -2071.256
## 671     25.09316 0.6292903 2.250843 -2070.222
## 672     22.86881 0.6625281 2.251525 -2070.332
## 673     26.58163 0.6088608 2.287844 -2071.590
## 674     23.43737 0.6521346 2.335192 -2072.756
## 675     24.70083 0.6358161 2.154595 -2071.600
## 676     25.37694 0.6242399 2.299523 -2071.392
## 677     26.76361 0.6047714 2.208494 -2070.763
## 678     21.15709 0.6873577 2.208776 -2070.773
## 679     21.15709 0.6873577 2.208776 -2070.773
## 680     23.32706 0.6546961 2.258841 -2070.355
## 681     17.42461 0.7421363 2.242731 -2072.886
## 682     26.24598 0.6117493 2.141774 -2072.523
## 683     20.45148 0.6978269 2.140575 -2072.832
## 684     26.11916 0.6144932 2.222759 -2070.420
## 685     21.58577 0.6807412 2.225950 -2070.458
## 686     19.40326 0.7141624 2.258809 -2072.397
## 687     22.59233 0.6653501 2.258983 -2070.496
## 688     20.05772 0.7029031 2.253394 -2071.107
## 689     27.50301 0.5960404 2.255014 -2072.409
## 690     20.07815 0.7030083 2.222648 -2071.092
## 691     28.82642 0.5747282 2.212865 -2071.728
## 692     20.12565 0.7025266 2.321957 -2072.321
## 693     26.29171 0.6109255 2.171958 -2071.683
## 694     24.29823 0.6417227 2.213166 -2070.374
## 695     24.00903 0.6447838 2.284292 -2070.627
## 696     20.36133 0.6986939 2.141623 -2072.727
## 697     24.21143 0.6416754 2.232630 -2070.284
## 698     26.68808 0.6060107 2.213663 -2070.665
## 699     23.33325 0.6541949 2.299396 -2071.241
## 700     23.45604 0.6538640 2.214763 -2070.279
## 701     20.14590 0.7021876 2.327517 -2072.462
## 702     21.93075 0.6769501 2.273071 -2071.240
## 703     18.01828 0.7319369 2.185336 -2073.361
## 704     27.08234 0.6022916 2.225314 -2072.398
## 705     22.69518 0.6623748 2.277629 -2072.432
## 706     24.49368 0.6403374 2.185199 -2072.639
## 707     24.33375 0.6410088 2.247661 -2070.212
## 708     23.24526 0.6557909 2.237319 -2070.320
## 709     23.77696 0.6493258 2.259315 -2070.359
## 710     24.11073 0.6449725 2.277456 -2070.967
## 711     27.15018 0.5977401 2.249812 -2071.876
## 712     20.29182 0.6998388 2.240174 -2070.943
## 713     27.82955 0.5894159 2.191365 -2071.454
## 714     21.99772 0.6733999 2.258416 -2071.167
## 715     25.09150 0.6284718 2.250752 -2070.635
## 716     24.19445 0.6430584 2.234314 -2070.184
## 717     22.55065 0.6663219 2.284671 -2070.681
## 718     21.57829 0.6823568 2.196920 -2071.737
## 719     23.00493 0.6592354 2.260703 -2070.500
## 720     18.43722 0.7281217 2.303398 -2073.414
## 721     28.15181 0.5851284 2.346998 -2073.315
## 722     20.64208 0.6940241 2.074018 -2076.515
## 723     27.18539 0.5996199 2.241237 -2071.021
## 724     21.29094 0.6847197 2.160373 -2071.725
## 725     26.96194 0.6023883 2.264875 -2070.841
## 726     20.37853 0.6981486 2.230051 -2070.932
## 727     27.04749 0.6008437 2.281335 -2071.052
## 728     20.86327 0.6901917 2.267765 -2071.402
## 729     29.09363 0.5695845 2.231688 -2072.549
## 730     27.65802 0.5931078 2.327326 -2072.928
## 731     20.38974 0.6973986 2.201378 -2071.457
## 732     24.71329 0.6355368 2.290455 -2070.782
## 733     21.65221 0.6798767 2.185442 -2070.919
## 734     27.56137 0.5940285 2.245553 -2071.135
## 735     22.98489 0.6602648 2.216425 -2070.212
## 736     20.83974 0.6913660 2.247862 -2070.758
## 737     27.03321 0.6011575 2.262368 -2070.813
## 738     20.18978 0.7011741 2.232767 -2070.983
## 739     27.37161 0.5966454 2.291065 -2071.467
## 740     23.94062 0.6442282 2.263781 -2072.054
## 741     25.14152 0.6279910 2.393081 -2074.504
## 742     21.74656 0.6761190 2.390177 -2076.317
## 743     23.41849 0.6538435 2.218869 -2070.158
## 744     25.12879 0.6288831 2.208015 -2070.319
## 745     21.64559 0.6784849 2.215939 -2071.355
## 746     28.36628 0.5799176 2.222299 -2072.554
## 747     26.94649 0.6010615 2.232669 -2071.358
## 748     23.15249 0.6577638 2.281915 -2070.515
## 749     24.96680 0.6308495 2.231071 -2070.240
## 750     22.05363 0.6728181 2.238618 -2070.828
## 751     22.64809 0.6664434 2.241293 -2070.851
## 752     26.31741 0.6106944 2.178175 -2071.416
## 753     23.75917 0.6497090 2.199125 -2070.590
## 754     26.78871 0.6039134 2.188583 -2071.279
## 755     24.30148 0.6401141 2.266729 -2070.622
## 756     26.00450 0.6157118 2.239317 -2070.425
## 757     23.02550 0.6604504 2.234041 -2070.389
## 758     24.90277 0.6316989 2.181335 -2070.838
## 759     23.30022 0.6563116 2.228520 -2070.299
## 760     21.91959 0.6762978 2.232082 -2070.428
## 761     21.91959 0.6762978 2.232082 -2070.428
## 762     24.30412 0.6407814 2.330702 -2071.687
## 763     23.16992 0.6571266 2.248438 -2070.242
## 764     21.35388 0.6836396 2.239406 -2070.624
## 765     23.59146 0.6519241 2.305968 -2071.080
## 766     23.82901 0.6468953 2.167868 -2071.476
## 767     18.73456 0.7235631 2.286386 -2072.698
## 768     25.96784 0.6152757 2.154592 -2072.530
## 769     22.32235 0.6705732 2.204479 -2070.596
## 770     23.73546 0.6498089 2.290078 -2070.728
## 771     25.89907 0.6172882 2.283761 -2070.793
## 772     24.78725 0.6349311 2.283464 -2070.971
## 773     24.15555 0.6422378 2.276559 -2070.749
## 774     24.44973 0.6396448 2.217669 -2070.435
## 775     21.27533 0.6840787 2.158681 -2072.416
## 776     24.44741 0.6397213 2.289822 -2070.917
## 777     22.07538 0.6721861 2.280566 -2071.489
## 778     21.02322 0.6880545 2.273905 -2071.239
## 779     25.62194 0.6207950 2.318972 -2071.809
## 780     24.92639 0.6316761 2.284385 -2070.593
## 781     22.98736 0.6611092 2.248366 -2070.491
## 782     24.78663 0.6332075 2.185533 -2070.846
## 783     23.28978 0.6555820 2.196836 -2070.423
## 784     23.73136 0.6490915 2.258012 -2070.214
## 785     24.43734 0.6387355 2.188654 -2070.562
## 786     22.56652 0.6679990 2.211669 -2071.438
## 787     24.24909 0.6420851 2.153953 -2071.442
## 788     22.74163 0.6616394 2.289416 -2072.705
## 789     20.10948 0.7021420 2.188792 -2071.496
## 790     22.22658 0.6707545 2.272373 -2070.678
## 791     24.06168 0.6472656 2.331365 -2074.663
## 792     26.34892 0.6098935 2.279660 -2071.370
## 793     28.27167 0.5831275 2.244313 -2071.294
## 794     21.68127 0.6788399 2.277799 -2070.835
## 795     25.75248 0.6186813 2.146639 -2072.460
## 796     21.70846 0.6801350 2.277546 -2071.290
## 797     25.92644 0.6160693 2.163270 -2072.019
## 798     23.44469 0.6553831 2.230743 -2071.554
## 799     16.40942 0.7581124 2.307406 -2075.466
## 800     25.12756 0.6301432 2.250867 -2070.837
## 801     23.15409 0.6573052 2.276031 -2070.529
## 802     18.34044 0.7280497 2.282885 -2072.416
## 803     27.66550 0.5916576 2.250186 -2071.003
## 804     26.73185 0.6051673 2.350663 -2072.841
## 805     26.19482 0.6125946 2.401168 -2075.103
## 806     25.81447 0.6187033 2.392050 -2074.371
## 807     26.15527 0.6139455 2.353942 -2072.765
## 808     23.64872 0.6501761 2.150089 -2071.607
## 809     19.04569 0.7180898 2.267821 -2071.770
## 810     24.20765 0.6445922 2.255179 -2072.215
## 811     22.63394 0.6659505 2.274036 -2070.579
## 812     22.69508 0.6652255 2.284400 -2070.815
## 813     21.47626 0.6821396 2.227144 -2070.508
## 814     19.86669 0.7058501 2.252079 -2071.178
## 815     20.24882 0.6999902 2.279618 -2071.315
## 816     22.54949 0.6672378 2.226598 -2070.372
## 817     25.47897 0.6231861 2.298684 -2071.092
## 818     21.01703 0.6889815 2.242038 -2070.638
## 819     23.58651 0.6525059 2.211257 -2070.666
## 820     23.98181 0.6452742 2.369710 -2073.202
## 821     20.15691 0.6999338 2.288187 -2072.683
## 822     26.40901 0.6112081 2.185029 -2071.446
## 823     22.15108 0.6716737 2.286510 -2071.021
## 824     24.67990 0.6358740 2.274597 -2070.459
## 825     23.25475 0.6574205 2.259370 -2070.718
## 826     27.12147 0.5991885 2.218610 -2070.937
## 827     23.42960 0.6547456 2.247002 -2070.527
## 828     26.98357 0.6024402 2.257256 -2070.930
## 829     27.69033 0.5906696 2.335988 -2072.891
## 830     26.25118 0.6112730 2.215334 -2071.171
## 831     26.38384 0.6107094 2.252567 -2070.521
## 832     19.59529 0.7103933 2.265137 -2071.530
## 833     28.99829 0.5718875 2.193671 -2072.183
## 834     22.97557 0.6613472 2.206039 -2070.691
## 835     22.17500 0.6732938 2.198293 -2071.156
## 836     22.17500 0.6732938 2.198293 -2071.156
## 837     24.64652 0.6363123 2.270063 -2070.382
## 838     22.05135 0.6727458 2.221909 -2070.978
## 839     21.88990 0.6770352 2.270690 -2070.796
## 840     21.88990 0.6770352 2.270690 -2070.796
## 841     26.08578 0.6137265 2.259285 -2071.080
## 842     21.89591 0.6782785 2.290212 -2072.633
## 843     29.41972 0.5674822 2.226672 -2072.796
## 844     16.32039 0.7567590 2.202846 -2074.554
## 845     29.29631 0.5683503 2.324675 -2073.220
## 846     22.97019 0.6603491 2.280673 -2070.522
## 847     25.57039 0.6227432 2.176603 -2070.969
## 848     25.83776 0.6207639 2.205139 -2072.678
## 849     26.67999 0.6077497 2.238826 -2071.562
## 850     18.81575 0.7200653 2.248199 -2072.397
## 851     28.18742 0.5849354 2.222720 -2071.524
## 852     25.02090 0.6303995 2.254631 -2070.230
## 853     21.21930 0.6862661 2.214416 -2070.658
## 854     25.00398 0.6290015 2.221817 -2071.529
## 855     23.22428 0.6584716 2.182600 -2071.954
## 856     22.79549 0.6616326 2.377267 -2074.317
## 857     22.56965 0.6658613 2.231020 -2070.317
## 858     17.74034 0.7362861 2.266513 -2072.860
## 859     24.16708 0.6430187 2.216086 -2070.159
## 860     22.55571 0.6665299 2.211418 -2070.319
## 861     28.26763 0.5836160 2.210617 -2071.581
## 862     28.01511 0.5873804 2.207910 -2071.514
## 863     26.12267 0.6140341 2.264649 -2070.586
## 864     20.10599 0.7036388 2.262505 -2071.780
## 865     25.88668 0.6176021 2.219589 -2070.398
## 866     22.56116 0.6679508 2.240652 -2071.139
## 867     24.28390 0.6401100 2.338736 -2072.479
## 868     20.42352 0.6965180 2.285869 -2071.932
## 869     26.42146 0.6114089 2.255162 -2071.374
## 870     25.59779 0.6222474 2.224963 -2070.296
## 871     27.60895 0.5956573 2.210234 -2075.121
## 872     27.39267 0.5990967 2.211211 -2075.778
## 873     24.59625 0.6386613 2.212082 -2071.898
## 874     24.55508 0.6378626 2.134165 -2072.310
## 875     24.52895 0.6360247 2.200572 -2071.589
## 876     24.30628 0.6424446 2.248998 -2071.079
## 877     23.66800 0.6491796 2.200992 -2070.869
## 878     25.00583 0.6313567 2.257260 -2070.416
## 879     20.18344 0.7005431 2.273025 -2071.432
## 880     22.93082 0.6626190 2.274761 -2071.425
## 881     28.12088 0.5845757 2.225013 -2071.366
## 882     22.87562 0.6601246 2.207276 -2071.686
## 883     28.01358 0.5869406 2.226590 -2071.180
## 884     27.52813 0.5939238 2.239567 -2070.905
## 885     23.16242 0.6580829 2.238529 -2070.193
## 886     23.92101 0.6461852 2.166473 -2071.090
## 887     23.70179 0.6493256 2.172674 -2070.957
## 888     26.86042 0.6032797 2.364543 -2073.431
## 889     19.52120 0.7115246 2.281574 -2071.795
## 890     30.96042 0.5418808 2.228549 -2074.538
## 891     24.23619 0.6424902 2.288822 -2070.710
## 892     23.13746 0.6575620 2.273191 -2070.486
## 893     19.66169 0.7072746 2.237815 -2072.394
## 894     18.62180 0.7238003 2.293752 -2072.442
## 895     22.40929 0.6695099 2.195377 -2070.836
## 896     26.11707 0.6155081 2.217825 -2070.928
## 897     26.24446 0.6135764 2.219040 -2070.896
## 898     24.85907 0.6318868 2.249615 -2070.587
## 899     26.23354 0.6123659 2.250690 -2070.523
## 900     22.52448 0.6679400 2.287529 -2071.045
## 901     20.55850 0.6958451 2.253671 -2070.864
## 902     28.77191 0.5753014 2.314540 -2072.607
## 903     23.27186 0.6564983 2.284257 -2070.612
## 904     25.40075 0.6241827 2.183525 -2071.008
## 905     20.14133 0.7022549 2.217064 -2071.141
## 906     24.93950 0.6301817 2.184493 -2071.684
## 907     23.55447 0.6527367 2.308821 -2071.295
## 908     24.36179 0.6404100 2.332416 -2071.756
## 909     21.23736 0.6857351 2.345592 -2072.599
## 910     22.73743 0.6617401 2.351879 -2074.326
## 911     22.25994 0.6701190 2.358107 -2073.043
## 912     19.18305 0.7173856 2.361225 -2074.917
## 913     27.25256 0.5960965 2.120549 -2075.016
## 914     18.37185 0.7274140 2.255640 -2072.151
## 915     20.20175 0.7005528 2.257901 -2071.135
## 916     25.99652 0.6151563 2.282457 -2071.217
## 917     28.68473 0.5772897 2.225055 -2071.611
## 918     26.42692 0.6090677 2.228308 -2070.813
## 919     26.42692 0.6090677 2.228308 -2070.813
## 920     22.82923 0.6611123 2.237992 -2071.076
## 921     23.09818 0.6606507 2.301928 -2072.586
## 922     22.88780 0.6607363 2.219964 -2070.615
## 923     23.36288 0.6546682 2.258453 -2070.207
## 924     29.70606 0.5609868 2.277492 -2072.829
## 925     30.18204 0.5537622 2.287711 -2073.562
## 926     18.45171 0.7278344 2.122614 -2075.512
## 927     31.94748 0.5290432 2.334022 -2075.505
## 928     16.18042 0.7596196 2.165051 -2075.153
## 929     25.58282 0.6222607 2.142975 -2071.972
## 930     29.76973 0.5595308 2.194166 -2073.653
## 931     13.37007 0.8033760 2.219710 -2079.926
## 932     33.31861 0.5064830 2.272229 -2078.621
## 933     26.14020 0.6120911 2.251583 -2072.203
## 934     22.67604 0.6667843 2.269580 -2072.047
## 935     25.37471 0.6246113 2.281800 -2070.809
## 936     23.43938 0.6546090 2.222262 -2070.549
## 937     22.83463 0.6630557 2.292809 -2070.883
## 938     20.59337 0.6947933 2.208769 -2071.051
## 939     25.37121 0.6246335 2.162730 -2071.513
## 940     22.38034 0.6699460 2.268913 -2070.721
## 941     22.96723 0.6582147 2.170757 -2073.458
## 942     18.49723 0.7274709 2.220183 -2073.071
## 943     28.94467 0.5714734 2.208279 -2073.021
## 944     17.30331 0.7444805 2.313802 -2074.248
## 945     25.83350 0.6185693 2.295152 -2070.937
## 946     27.21058 0.5989239 2.347287 -2072.930
## 947     20.15503 0.7018986 2.184511 -2071.559
## 948     17.29146 0.7440697 2.276446 -2073.200
## 949     24.25433 0.6440699 2.299714 -2073.100
## 950     25.84669 0.6198654 2.233439 -2071.225
## 951     21.90111 0.6747698 2.172385 -2072.001
## 952     25.15656 0.6289520 2.142987 -2071.978
## 953     23.70041 0.6528198 2.260573 -2074.027
## 954     20.74809 0.6957484 2.161471 -2075.332
## 955     27.26069 0.5946391 2.278537 -2075.087
## 956     24.60406 0.6375714 2.146335 -2072.137
## 957     23.52248 0.6514545 2.277012 -2070.800
## 958     21.81738 0.6768580 2.260870 -2070.594
## 959     23.21504 0.6578840 2.289762 -2071.032
## 960     30.51224 0.5504869 2.265468 -2072.960
## 961     24.97676 0.6334939 2.301042 -2073.171
## 962     22.08082 0.6710224 2.192245 -2073.465
## 963     25.09372 0.6324235 2.274148 -2074.182
## 964     24.30392 0.6379398 2.225026 -2074.182
## 965     18.98043 0.7199481 2.215080 -2072.270
## 966     21.41153 0.6806029 2.219378 -2073.754
## 967     20.44857 0.6949402 2.207925 -2073.692
## 968     28.15767 0.5870256 2.253553 -2073.744
## 969     26.91863 0.6048308 2.235092 -2072.549
## 970     23.22417 0.6572344 2.263901 -2070.351
## 971     24.61430 0.6353487 2.226861 -2070.655
## 972     23.60084 0.6523407 2.199212 -2070.852
## 973     23.20452 0.6566481 2.158055 -2071.383
## 974     25.15005 0.6263020 2.174289 -2073.350
## 975     23.58999 0.6504336 2.188087 -2070.976
## 976     20.75265 0.6926337 2.260287 -2070.872
## 977     24.28013 0.6413757 2.213608 -2070.182
## 978     30.01484 0.5561367 2.246104 -2073.153
## 979     18.29389 0.7303839 2.190484 -2073.560
## 980     21.48642 0.6840839 2.190487 -2072.462
## 981     26.34187 0.6132556 2.174949 -2073.164
## 982     26.34187 0.6132556 2.174949 -2073.164
## 983     21.68017 0.6771811 2.310947 -2073.345
## 984     23.58411 0.6496964 2.286339 -2071.936
## 985     23.46339 0.6550052 2.292916 -2071.909
## 986     27.60627 0.5930912 2.254083 -2071.060
## 987     27.13637 0.5999557 2.243977 -2070.811
## 988     22.52798 0.6650457 2.237476 -2071.816
## 989     20.86039 0.6911457 2.245056 -2070.722
## 990     26.15734 0.6136037 2.249197 -2070.459
## 991     21.81901 0.6779580 2.187030 -2071.007
## 992     22.36160 0.6698916 2.184541 -2070.858
## 993     21.66422 0.6791999 2.249600 -2070.519
## 994     26.69865 0.6063907 2.235438 -2070.649
## 995     20.00731 0.7036952 2.217263 -2071.157
## 996     21.66100 0.6797444 2.221208 -2070.461
## 997     20.34756 0.7002265 2.263128 -2071.807
## 998     22.02000 0.6734015 2.313108 -2071.772
## 999     23.02294 0.6595630 2.226481 -2070.166
## 1000    25.22328 0.6272257 2.186026 -2070.707
```


---

```r
library(tidybayes)
tidy_draws(m.1)
```

```
## # A tibble: 1,000 × 13
##    .chain .iteration .draw b_Intercept b_parent sigma   lp__ accept_stat__
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;
##  1      1          1     1        26.1    0.614  2.26 -2070.         0.807
##  2      1          2     2        24.8    0.635  2.16 -2071.         0.742
##  3      1          3     3        21.0    0.689  2.28 -2071.         0.740
##  4      1          4     4        18.2    0.730  2.24 -2072.         0.920
##  5      1          5     5        24.9    0.631  2.27 -2070.         0.809
##  6      1          6     6        27.9    0.588  2.24 -2071.         0.882
##  7      1          7     7        21.5    0.682  2.22 -2071.         0.918
##  8      1          8     8        22.4    0.668  2.24 -2071.         0.978
##  9      1          9     9        27.8    0.591  2.19 -2072.         0.876
## 10      1         10    10        20.4    0.697  2.28 -2072.         0.996
## # … with 990 more rows, and 5 more variables: stepsize__ &lt;dbl&gt;,
## #   treedepth__ &lt;dbl&gt;, n_leapfrog__ &lt;dbl&gt;, divergent__ &lt;dbl&gt;, energy__ &lt;dbl&gt;
```


---
### Pairs plot

.pull-left[
marginal posteriors and the covariances

```r
pairs(m.1)
```
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
]

---

{brms} assumes that the intercept is the expected response value when all predictors are at their means. This is done to improve sampling efficiency. 

We can either 
1. center
2. change prior &amp; use 0 + Intercept

---

```r
galton.data &lt;-
  galton.data %&gt;% 
  mutate(parent_c = parent - mean(parent))

m.2 &lt;- 
  brm(family = gaussian,
      child ~ 1 + parent_c,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.2")
```

---

```r
pairs(m.2)
```

![](regression-2_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;

---


```r
summary(m.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: child ~ 1 + parent_c 
##    Data: galton.data (Number of observations: 928) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    68.09      0.08    67.94    68.23 1.00      638      697
## parent_c      0.64      0.04     0.56     0.72 1.00      851      842
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.24      0.05     2.14     2.34 1.00     1242      837
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---


```r
m.2a &lt;- 
  brm(family = gaussian,
      child ~ 0 + Intercept + parent,
      prior = c(prior(normal(28, 5), class = b, coef ="Intercept" ),
                prior(normal(0, 5), class = b, coef ="parent"),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.2a")
```

---


```r
pairs(m.2a)
```

![](regression-2_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;


---
## Posterior

Again the posterior is just made up of samples. In reality is it some hyperplaned space, but we don't have access to that so we must sample. 

Now the difficulty is getting those samples into a shape that playes nicely with what we want. We will primarily use {tidybayes} and {posterior} to do so. {brms} has some built in functions but they do not extend well for certain situations. 


---

```r
post.tidy &lt;- m.2 %&gt;% 
spread_draws(b_Intercept, b_parent_c)
post.tidy
```

```
## # A tibble: 1,000 × 5
##    .chain .iteration .draw b_Intercept b_parent_c
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1      1          1     1        68.1      0.631
##  2      1          2     2        68.1      0.661
##  3      1          3     3        68.1      0.617
##  4      1          4     4        68.1      0.590
##  5      1          5     5        68.1      0.667
##  6      1          6     6        68.1      0.564
##  7      1          7     7        68.1      0.696
##  8      1          8     8        68.0      0.684
##  9      1          9     9        68.0      0.644
## 10      1         10    10        68.1      0.678
## # … with 990 more rows
```

---

```r
post.tidy %&gt;% 
  ggplot(aes(x = b_parent_c)) +
  stat_dotsinterval()
```

![](regression-2_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;

---

```r
post.tidy %&gt;% 
  select(b_parent_c) %&gt;% 
  mean_qi(.width = c(.5, .89, .99))
```

```
## # A tibble: 3 × 6
##   b_parent_c .lower .upper .width .point .interval
##        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1      0.643  0.615  0.671   0.5  mean   qi       
## 2      0.643  0.578  0.707   0.89 mean   qi       
## 3      0.643  0.544  0.747   0.99 mean   qi
```


```r
post.tidy %&gt;% select(b_parent_c) %&gt;% 
  mode_hdi(.width = c(.5, .89, .99))
```

```
## # A tibble: 3 × 6
##   b_parent_c .lower .upper .width .point .interval
##        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1      0.644  0.614  0.669   0.5  mode   hdi      
## 2      0.644  0.578  0.707   0.89 mode   hdi      
## 3      0.644  0.544  0.748   0.99 mode   hdi
```

---
## fitted values

Much like in regular regression, we may be interested in the predicted values (Y-hats) at certain values of X. These Y-hats are called fitted values. We use them a lot to calculate residuals and other fit metrics. This gives us the predicted mean at a given X

```r
library(broom)
augment(lm(child ~ parent, galton.data))
```

```
## # A tibble: 928 × 8
##    child parent .fitted .resid    .hat .sigma .cooksd .std.resid
##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
##  1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49
##  2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91
##  3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05
##  4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76
##  5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62
##  6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40
##  7  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40
##  8  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40
##  9  62.2   66.5    66.9  -4.72 0.00218   2.23 0.00487      -2.11
## 10  62.2   66.5    66.9  -4.72 0.00218   2.23 0.00487      -2.11
## # … with 918 more rows
```

---
## fitted values

.pull-left[
Bayesian analysis also has fitted values, but now we have many samples of parameters rather than just a single estimate for each value.


```r
mu_at_64 &lt;- 
  post.tidy %&gt;% 
  select(b_Intercept,b_parent_c) %&gt;% 
  mutate(mu_at_64 = b_Intercept + (b_parent_c * -4.3))
 mu_at_64
```

]

.pull-right[

```
## # A tibble: 1,000 × 3
##    b_Intercept b_parent_c mu_at_64
##          &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;
##  1        68.1      0.631     65.4
##  2        68.1      0.661     65.3
##  3        68.1      0.617     65.4
##  4        68.1      0.590     65.6
##  5        68.1      0.667     65.2
##  6        68.1      0.564     65.6
##  7        68.1      0.696     65.1
##  8        68.0      0.684     65.1
##  9        68.0      0.644     65.3
## 10        68.1      0.678     65.2
## # … with 990 more rows
```
]


---
## fitted values
.pull-left[
This is nice because we can calculate not only the mean but also the dispersion. In lm land we had to use a funky equation to calculate the CI around some predicted value of X. Now we can use samples. 



```r
mu_at_64 %&gt;% 
mean_hdi(mu_at_64)
```

```
## # A tibble: 1 × 6
##   mu_at_64 .lower .upper .width .point .interval
##      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1     65.3   65.0   65.7   0.95 mean   hdi
```
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-45-1.png)&lt;!-- --&gt;

]

---
## fitted values

We can also pass new data to this function. You can do this with {brms} `poseterior_predict` function, but I like the {tidybayes} 'epred_draws()' better, especially when paired  with {modelr}::`data_grid`, as it is easier to pipe into ggplot



```r
galton.data %&gt;% 
 add_epred_draws(m.2)
```

```
## # A tibble: 928,000 × 8
## # Groups:   parent, child, parent_c, .row [928]
##    parent child parent_c  .row .chain .iteration .draw .epred
##     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
##  1   70.5  61.7     2.19     1     NA         NA     1   69.5
##  2   70.5  61.7     2.19     1     NA         NA     2   69.6
##  3   70.5  61.7     2.19     1     NA         NA     3   69.4
##  4   70.5  61.7     2.19     1     NA         NA     4   69.4
##  5   70.5  61.7     2.19     1     NA         NA     5   69.6
##  6   70.5  61.7     2.19     1     NA         NA     6   69.3
##  7   70.5  61.7     2.19     1     NA         NA     7   69.6
##  8   70.5  61.7     2.19     1     NA         NA     8   69.5
##  9   70.5  61.7     2.19     1     NA         NA     9   69.4
## 10   70.5  61.7     2.19     1     NA         NA    10   69.6
## # … with 927,990 more rows
```

---
### many ways to predict
epred = fitted = uncertainty in the fixed coefficients and the uncertainty in the variance parameters for the grouping factors (for MLMs). Can be thought of as expected values or the mean prediction 

"predict" accounts for the residual (observation-level) variance, plus the other sources of variance in epred. This is used not to describe an expected value/mean but observation/individual level data. eg what is plausible when we collect a new subject. 

Whereas expectation values were good for inference, predictions are more for model checking. 

---
## fitted values
.pull-left[



```r
galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
 add_epred_draws(m.2) %&gt;% 
  ggplot(aes(x = parent_c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.99), color = "grey") +
  geom_point(data = galton.data, size = 2)
```
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;
]

---


```r
galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
 add_epred_draws(m.2)
```

```
## # A tibble: 101,000 × 6
## # Groups:   parent_c, .row [101]
##    parent_c  .row .chain .iteration .draw .epred
##       &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
##  1    -4.31     1     NA         NA     1   65.4
##  2    -4.31     1     NA         NA     2   65.3
##  3    -4.31     1     NA         NA     3   65.4
##  4    -4.31     1     NA         NA     4   65.6
##  5    -4.31     1     NA         NA     5   65.2
##  6    -4.31     1     NA         NA     6   65.6
##  7    -4.31     1     NA         NA     7   65.1
##  8    -4.31     1     NA         NA     8   65.1
##  9    -4.31     1     NA         NA     9   65.3
## 10    -4.31     1     NA         NA    10   65.2
## # … with 100,990 more rows
```


---
## fitted values

.pull-left[

```r
labels &lt;-  c(-2.5, 0, 2.5, 5) + mean(galton.data$parent) %&gt;%   round(digits = 0)

galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
 add_epred_draws(m.2, ndraws = 100) %&gt;% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  geom_point(data = galton.data, size = 2) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels) 
```

]

.pull-right[


![](regression-2_files/figure-html/unnamed-chunk-52-1.png)&lt;!-- --&gt;

]

---


```r
galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
 add_epred_draws(m.2, ndraws = 100)
```

```
## # A tibble: 10,100 × 6
## # Groups:   parent_c, .row [101]
##    parent_c  .row .chain .iteration .draw .epred
##       &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
##  1    -4.31     1     NA         NA     1   65.3
##  2    -4.31     1     NA         NA     2   65.3
##  3    -4.31     1     NA         NA     3   65.5
##  4    -4.31     1     NA         NA     4   65.3
##  5    -4.31     1     NA         NA     5   65.4
##  6    -4.31     1     NA         NA     6   65.0
##  7    -4.31     1     NA         NA     7   65.3
##  8    -4.31     1     NA         NA     8   65.5
##  9    -4.31     1     NA         NA     9   65.3
## 10    -4.31     1     NA         NA    10   65.4
## # … with 10,090 more rows
```
why only 10,100 rows? 

---
## fitted values

.pull-left[
Posterior samples provides us an estimate of different parameter values proportional to their likelihood (and prior) ie more samples near the mean. 

They also give an indication of the spread of the posterior, which is useful to create confidence intervals and do NHST like statistical tests of parameters.
]

.pull-right[
We use them to: 
1. create model implied estimates at a specific value of X, ( `\(\hat{Y}\)` | X )  
2. create CIs around these values
3. visualize model implied fits

In other words, if it involves a specific value or different values of X we need to use fitted values. 

]


---
## predicted values
.pull-left[

```r
galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
  add_predicted_draws(m.2) %&gt;% 
  ggplot(aes(x = parent_c, y = child)) +
     stat_lineribbon(aes(y = .prediction), 
                  .width = seq(.5, .99, by = .01), 
                  alpha = .5,
                  show.legend = F) +
  geom_point(data = galton.data, size = 2,alpha = .5) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels)
```
]

.pull-right[
![](regression-2_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;
]

--- 
## predicted values

The plotted predictions can show you the potential spread in the cases. As opposed to epred/fitted values which are specific to `\(\mu\)` s or other parameters we are trying to estimate, predicted values serve as simulated new data. 

If a model is a good fit we should be able to use it to generate data that resemble the data we observed. This is the basis of the posterior predictive distribution and PP checks. 

This is also what is meant by a `generative` model. 


---

```r
galton.data %&gt;% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %&gt;% 
  add_predicted_draws(m.2)
```

```
## # A tibble: 101,000 × 6
## # Groups:   parent_c, .row [101]
##    parent_c  .row .chain .iteration .draw .prediction
##       &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;
##  1    -4.31     1     NA         NA     1        64.1
##  2    -4.31     1     NA         NA     2        67.0
##  3    -4.31     1     NA         NA     3        66.6
##  4    -4.31     1     NA         NA     4        68.1
##  5    -4.31     1     NA         NA     5        63.8
##  6    -4.31     1     NA         NA     6        64.2
##  7    -4.31     1     NA         NA     7        62.8
##  8    -4.31     1     NA         NA     8        68.4
##  9    -4.31     1     NA         NA     9        65.0
## 10    -4.31     1     NA         NA    10        66.0
## # … with 100,990 more rows
```
Where does 101,000 come from? 


---
## 3 types of predictions

`\(\hat{Y}_{prediction}\)` ~ `\(b_o\)` + `\(b_1\)`X, where we put in new values of X (often our data Xs we collected or values across the range of X)

1. lm style, where there is 1 value, our estimate, of `\(b_o\)` &amp; `\(b_1\)`

2. epred/fitted style, where we are propagating uncertainty in `\(b_o\)` &amp; `\(b_1\)`, as these parameters can take on many values according to the posterior distribution. The mean of this is equal to #1. 

3. prediction style, where we are propagating uncertainty in `\(b_o\)`, `\(b_1\)` &amp; `\(\hat{\sigma}\)` . 

---
## posterior predictive distribution 
.pull-left[If a model is a good fit we should be able to use it to generate data that resemble the data we observed. We can simulate from the posterior predictive distribution. 

This is just #3 we did previously. Taking samples of those possible "datasets" and plugging them into the model to get Y. Replications of Y (Yrep) from the posterior predictive distribution through `pp_check`
]

.pull-right[]
![](regression-2_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;
]

---
## posterior predictive distribution 


```r
library(shinystan)
launch_shinystan(m.2)
```

---


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
