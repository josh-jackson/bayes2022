---
title: "regression intro"
author: Josh Jackson
date: "1-31-22"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)

library(tidyverse)
library(broom)
```

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
</style>


## Goals for the week

Circle back to the material we covered and walk through regression examples. Also, we will be doing analyses using `brms`, the primary package we will be using in class. 

---
## Distributions and model descriptions
Because we are going to be working with a lot of distributions, priors, and different analytic machinery we need to have a new way to describe out models. Much like y = b0 + b1X but more elaborate.  

1. Describe how our DV is related to the data via a likelihood distribution. 

DV ~ Normal( $\mu_i$ , $\sigma$)

Here we say that our DGP is normal, with two parameters. The mean differs among i people. The first step will always be describing the DGP for our DV.  

---
Many possibilities
$$ y_{i} \sim \operatorname{Bernoulli}(\theta_i) $$

$$ y_{i} \sim \operatorname{Binomial}(1, p_i) $$
$$ y_i \sim \text{Poisson}(\lambda_i) $$

$$ y_i  \sim \operatorname{BetaBinomial}(n_i, \bar p_i, \phi) $$
$$ y_i  \sim \operatorname{ZIPoisson}({p_i}, {\lambda_i})\\
 $$
$$ y_i \sim \operatorname{Categorical} (\mathbf p) \\ $$ where P is a vector of probabilities (this is for ordered logits like in IRT or when you have likert rating scales as your DV)


---
## Design the model

.pull-left[
'2. We can predict or decompose those parameters through other variables. For example, we want to understand why i people differ on the mean of the DV.  

DV ~ Normal( $\mu_i$ , $\sigma$ )  

$\mu_i$ = $\beta$ X $X_i$
]

.pull-right[
At this point, no different that a normal regression you are familiar with, just a different way of writing it. 

You also have a parameter that is estimated called sigma, which in R output is hidden under Residual Standard Error. This way is more explicit about 1. your data generating process and 2. what parameters you are modeling. 
]

---
## Design the model

.pull-left[
'3. We will use this same nomenclature to describe our priors on each of the parameters we are modeling. For example, we are estimating $\beta$ and $\sigma$, and thus we need two priors. 
]

.pull-right[
DV ~ Normal( $\mu_i$ , $\sigma$ )  
$\mu_i$ = $\beta$ X $X_i$

$\beta$ ~ Normal(0, 5)  
$\sigma$ ~ HalfCauchy(0,10)
]

---
### Why is this a different formulation? 

1. Because y = mx + b isn't enough, as it doesn't specify all of our parameters

2. Because $\epsilon$ ~ Normal (0, $\sigma$ ) doesn't generalize. While this is what is often provided in intro stats classes (I did it) and even more advanced MLM classes (I'm sure Mike did it), eventually you need to discuss the estimation of other parameters. 



---
## Example
.pull-left[Let's fit some simple data. We are going to use 
```{r}
library(psychTools)
galton.data <- galton
library(tidyverse)
glimpse(galton.data)
```
]

.pull-right[
```{r, echo = FALSE}
galton.data %>% 
  ggplot(aes(x = parent, y = child)) +
  geom_jitter(alpha = 1/2) 

```
]

---
## Height data

.pull-left[
```{r, echo = FALSE}
g1 <- galton.data %>% 
ggplot(aes(x = child)) +geom_density()
g1
```
]

.pull-right[
```{r, echo = FALSE}
g2<-galton.data %>% 
ggplot(aes(x = parent)) +geom_histogram(bins=10)
g2
```
]


---
## Describe our model

C_Height_i ~ Normal( $\mu_i$ , $\sigma$ )  [Likelihood]   

$\mu_i$ = $\beta_0$ + $\beta_1$ ( $\text{P_Height}_i$ - ${\overline{\mbox{P_Height}}}$ ) [linear model]


priors  
$\beta_0$ ~ Normal(68, 5)  [prior for intercept]  
$\beta_1$ ~ Normal(0, 5)  [prior for b1]  
$\sigma$  ~ HalfCauchy(0,1) [prior for sigma]  

---
### Prior for intercept
.pull-left[
```{r, echo = FALSE}
p.0 <-
  tibble(x = seq(from = 0, to = 100, by = .1)) %>% 
  
  ggplot(aes(x = x, y = dnorm(x, mean = 68, sd = 5))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +
  labs(title = "mu ~ dnorm(68, 5)",
       y = "density")

p.0
```
]

.pull-right[
Says we think the mean of male height is between 5 and 6 feet 4 inches feet, most likely. 

If this was too narrow for our likes (maybe we are living in Belgium) then we could change the SD (and mean). 
]

---
### Prior for regression coefficent

.pull-left[
```{r, echo = FALSE}
p.1 <-
  tibble(x = seq(from = -15, to = 15, by = .1)) %>% 
  
  ggplot(aes(x = x, y = dnorm(x, mean = 0, sd = 5))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = -15, to = 15, by = 3)) +
  labs(title = "mu ~ dnorm(0, 5)",
       y = "density")

p.1
```
]

.pull-right[
This is centered around  0, saying we think prior to collecting any data that we think there is no effect. 

We also do not know if it will be positive or negative.

We are saying it isn't likely to be a b = 20+
]

---
### Prior for sigma

.pull-left[
```{r, echo = FALSE}
p.s <- ggplot(data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +
  labs(title = "sigma ~ HalfCauchy(0,1)")
p.s
```
]

.pull-right[
We know that variances are going to be positive. So zero and below is not possible. 

What is an upper bound possibility? 
```{r, echo = FALSE}
library(psych)
describe(galton.data$child)
```
]

---
## Prior predictive

What do our priors say about what our model expects? This is a way to check if our priors our too liberal or conservative. 

We can take our "guesses" and estimate what they say about our potential results. Helpful to make sure we do not set up a model that creates unreasonable possibilities. 

Our goal is to create an efficient and useful model. One that makes impossible predictions prior to seeing the data isn't too useful.  

---
## Prior predictive

How do we create it? We sample from the priors. Use that to create regression lines (intercept and slope). Then plot.

```{r, eval = FALSE}
pp <- tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = 5)) %>% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %>% 
  mutate(c.height = a + b * (height - mean(galton.data$parent))) 
```


---
## Prior predictive
.pull-left[
```{r, echo = FALSE}

pp <-
  tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = 5)) %>% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %>%   
  mutate(c.height = a + b * (height - mean(galton.data$parent))) 

g.pp <- pp %>% 
  ggplot(aes(x = height, y = c.height, group = n)) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(36, 96)) 

g.pp


```
]

.pull-right[
Our priors are not great, as it says we could expect associations that we know to not be true, a priori ]
---
## Prior predictive
We could do a few things: 

1. Constrain the slope to be positive
2. Reduce the uncertainty (SDs) in our priors
3. Leave as is

It really depends on whether the priors are important and/or costly, computationally

---
## Running the model

Use grid estimation for the moment. But again, Bayesian estimation is just counting. 

Define the grid you want to estimate. 
a) the range of parameters you want and b) the number of values. 

Parameter values with more ways that are consistent with data are more plausible. 

---
## Running the model

.pull-left[
We have 3 parameters to estimate (b0, b1, sigma). With grid approximation it is going to be computationally expensive (~ million+ calculations). Try a simple intercept only model instead (only 40k). 

C.Height_i ~ Normal( $\mu$ , $\sigma$ )  
$\mu$ ~ Normal(68, 5)   
$\sigma$ ~ HalfCauchy(0,1)  

]

.right-pull[
Define my grid
```{r}
grid <-
  crossing(mu = seq(from = 66, to = 69, length.out = 200), sigma = seq(from = 2, to = 3, length.out = 200))
grid
```
]

---
### Grid approximation

.pull-left[
```{r, eval=FALSE}
library(purrr)
grid_function <- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %>%
    sum() 
  }

p_grid <-
  grid %>% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %>%
  unnest(log_likelihood) %>% 
  mutate(prior_mu  = dnorm(mu, mean = 68, sd  = 5, log = T), prior_sigma = dcauchy(sigma, 0, 1, log = T)) %>% mutate(product = log_likelihood + prior_mu + prior_sigma) %>% 
  mutate(probability = exp(product - max(product)))
  
```
]

.pull-right[
1. After defining the grid space we need to calculate the likelihood (are these numbers likely or not, given some distribution)

2. Likelihood of our data (each child's height) assuming each part of our grid is true. So if we take the first line of our grid (mu = 66, sigma = 2) we can see how likely each child (all 928) by dnorm(height, 66, 2) 

]

---

3. The log-likelihood for each data point is averaged across all 928 participants to get the average log-likelihood for each grid spot 

```{r}
dnorm(galton.data$child, mean = 66, sd = 2, log = T)
```

---

```{r}
dnorm(galton.data$child, mean = 66, sd = 2, log = T) %>%  sum()
```
Higher values (closer to zero) indicate better fit


---
### Grid approximation

```{r, echo=FALSE}
library(purrr)
grid_function <- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %>%
    sum() 
  }

p_grid <-
  grid %>% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %>%
  unnest(log_likelihood) %>% 
  mutate(prior_mu  = dnorm(mu, mean = 68, sd  = 5, log = T), prior_sigma = dcauchy(sigma, 0, 10, log = T)) %>% mutate(product = log_likelihood + prior_mu + prior_sigma) %>% 
  mutate(probability = exp(product - max(product)))

p_grid

```

---
### Grid approximation

4. We then incorporate the priors for mu and sigma (just multiplying but done with the log scale for math reasons). 

5. The result is a posterior probability for each grid spot we defined. Ie how likely are these parameters, given the data. 

---
### Grid approximation
```{r, echo=FALSE, message=FALSE}
library(viridis)
p_grid %>% 
  ggplot(aes(x = mu, y = sigma, fill = probability)) + 
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "C") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```


---
## sampling to summarize
We will randomly sample from this joint distribution to learn more about it. What we have now are probabilities for each possible parameter. So we can randomly sample based on the probabilities much like randomly sampling different bags of marbles with different amounts of blues in them. 

With samples we can visualize, create credible intervals, HDIs, etc. 

---
## 10k samples
```{r, echo = FALSE}
p_grid_samples <- 
  p_grid %>% 
  sample_n(size = 1e4, replace = T, weight = probability) %>% 
  pivot_longer(mu:sigma) 

gg_grid_samples <- p_grid_samples %>% 
  ggplot(aes(x = value)) + 
  geom_density() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~name, scales = "free",
             labeller = label_parsed)

gg_grid_samples
```

---
## Point Estimate and CIs
```{r, echo = FALSE}
library(tidybayes)
p_grid_samples %>% 
  group_by(name) %>% 
  mode_hdi(value)
```

```{r}
tidy(lm(child ~ 1,data = galton.data))
```

```{r}
glance(lm(child ~ 1,data = galton.data))
```

---
## brms
.pull-left[
We are going to fit our y ~ x models with the {brms} package. Uses syntax similar to {lme4}. Requires {rstan}.
```{r, message = FALSE}
library(brms)
```


]

.pull-right[
P_Height $_i$ ~ Normal( $\mu_i$ , $\sigma$ )   

$\mu_i$ = $\beta_0$ + $\beta_1$ ( $\text{C_Height}_i$ - ${\overline{\mbox{C_Height}}}$ ) 


$\beta_0$ ~ Normal(68, 5)   
$\beta_1$ ~ Normal(0, 5)   
$\sigma$  ~ HalfCauchy(0,1)   

]

---
## brms

```{r, eval = FALSE}
model.name <- # name your fit
  brm(family = gaussian, # what is your likelihood? 
      Y ~ X, # insert model
      prior = prior, # your priors go here
      data = data,  # your data goes here
      iter = 1000, warmup = 500, chains = 4, cores = 4, # wait for this 
      file = "fits/b04.01") # save your samples
```

---
## brms
You can also set aspects of it separately
```{r, eval = FALSE}

#formulas
brmsformula()
brmsformula(x~y, family = gaussian())
bf()

#priors
set_prior()
set_prior("normal(0, 5)", class = "b", coef = "parent")

```


---
## brms
```{r, messages = FALSE}
m.1 <- 
  brm(family = gaussian,
      child ~ 1 + parent,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.1")
```


---
## brms
```{r}
summary(m.1)
```

---
## compare with lm

```{r}
summary(lm(child ~ parent, data = galton.data))
```

---
```{r}
plot(m.1)
```

---

.pull-left[
```{r}
plot(conditional_effects(m.1), points = TRUE)

```

]

.pull-right[

Lots of options for some quick visualizations. And these are all ggplot compatible, so you can change themes, add labels etc. 

But we will largely not use these, instead favoring {tidybayes}
]

---
## Posterior

The posterior is made up of samples. Much like we did with grid approximation, and then sampled from the prior. Behind the scenes brms is using H-MCMC, which we will describe in more detail later. It is basically an algorithm used to define the posterior. It consists only of samples.

Very simplistically, the algorithm tries different potential values (much like we predefined using our grid approximation). But rather than completely random, it chooses depending on whether the parameter is "likely" given the data.

---
```{r}
library(tidybayes)
get_variables(m.1)
```

---

{tidybayes} is a helpful package to work with your posterior. It consists of a number of helper functions to put your posterior in a format to neatly graph and or compute values. Once we get to more complicated models this will be invaluable. 

Please take a look at the overview and download the package http://mjskay.github.io/tidybayes/articles/tidy-brms.html

Note that {ggdist} is a package that is loaded with {tidybayes}. It was spun off of the original {tidybayes} to assist with frequentist model visualization too. 


---
```{r, warning=FALSE, message=FALSE}
#deprecated but it is easy to see. Parameters more consistent with the data will appear more, just like with our grid approximation sampling. 
posterior_samples(m.1)
```


---
1k rows indicate 1k parameter samples. All of these parameters represent *possible* states of the world given our data. Parameters more consistent with the data will appear more often. 

```{r}
library(tidybayes)
tidy_draws(m.1)
```


---
Instead of looking at all of our "draws" we often want to look at just some of the parameters we estimated. Spread draws is an oft used function similar to select in {dplyr} to get at what parameters we care about
```{r}
 m.1 %>% 
spread_draws(b_Intercept, b_parent)
```


---
### Pairs plot

.pull-left[
marginal (i.e., averaged over the other parameters) posteriors and the covariances across draws (e.g., when the algorithm found a high value for bo did it also find a high value for sigma)
```{r, eval = FALSE}
pairs(m.1)
```
]

.pull-right[
```{r, echo = FALSE}
pairs(m.1)
```
]

---

Intercept and slope are correlated. This is often the case when we do not center! (This is one of the reasons people center when running an interaction). This means that no new information is provided with this extra parameter (ie it is redundant)

{brms} assumes that the intercept is the expected response value when all predictors are at their means. This is done to improve sampling efficiency. 

For now we will center, and later we will see how not centering impacts our estimation. 


---
```{r, messages = FALSE}

galton.data <-
  galton.data %>% 
  mutate(parent_c = parent - mean(parent))

m.2 <- 
  brm(family = gaussian,
      child ~ 1 + parent_c,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data, 
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.2")
```

---
```{r}
pairs(m.2)
```

---

```{r}
summary(m.2)
```


---
## Posterior

Again the posterior is just made up of samples. In reality is it some hyperplanned space, but we don't have access to that so we must sample. Think of it as us doing `rnorm` with me hiding what the mean and SD are, but then figuring out what the mean and SD are through counting the samples. Same as drawing marbles from a bag to find % blue.  

Now the difficulty is getting those samples into a shape that plays nicely with what we want. {brms} has some built in functions but they do not extend well for certain situations so we will primarily use {tidybayes}. 


---
Briefly: chain is how many of these sampling processes we want to do do simultaneously. Default is 4. Iterations are how many parameter values we have per chain. Draws tells us how many total parameter estimates we have. 
```{r}
post.tidy <- m.2 %>% 
spread_draws(b_Intercept, b_parent_c)
post.tidy
```

---
Once we have the posterior in an easy to use format, we can: 
1. Visualize it. 
  - Distributions
  - Predicted values
2. Calculate values from it. 
  - Summary statistics
  - New parameters

Anything we want to accomplish with our model is IN the posterior
---
```{r}
post.tidy %>% 
  ggplot(aes(x = b_parent_c)) +
  stat_dotsinterval()
```

---
```{r}
post.tidy %>% 
  select(b_parent_c) %>% 
  mean_qi(.width = c(.5, .89, .99))
```

```{r}
post.tidy %>% select(b_parent_c) %>% 
  mode_hdi(.width = c(.5, .89, .99))
```

---

.pull-left[ We can even go and easily plot these values we calculated

```{r, eval = FALSE}
  post.tidy %>% select(b_parent_c) %>% 
  mode_hdi(.width = c(.5, .89, .99)) %>%  
ggplot(aes(y = as.factor(.width), x = b_parent_c, xmin = .lower, xmax = .upper)) + geom_pointinterval() 
```
]

.pull-right[
```{r, echo=FALSE}
  post.tidy %>% select(b_parent_c) %>% 
  mode_hdi(.width = c(.5, .89, .99)) %>%  
ggplot(aes(y = as.factor(.width), x = b_parent_c, xmin = .lower, xmax = .upper)) + geom_pointinterval() 
```

]

---

```{r}
post.tidy %>% 
  ggplot(aes(x = b_parent_c)) +
  stat_halfeye()
```


---

.pull-left[ Prior and posterior together at the same time. 
```{r, eval = FALSE}
post.tidy %>% 
  ggplot(aes(x = b_parent_c)) +
  stat_slab() +
  stat_function(data = data.frame(x = c(-10, 10)), aes(x), fun = dnorm, n = 100, args = list(0, 5)) 
```
]

.pull-right[

```{r, echo = FALSE}
post.tidy %>% 
  ggplot(aes(x = b_parent_c)) +
  stat_slab() +
  stat_function(data = data.frame(x = c(-10, 10)), aes(x), fun = dnorm, n = 100, args = list(0, 5)) 
```


]


---
## fitted values

Much like in regular regression, we may be interested in the fitted/expected/predicted values (Y-hats) at certain values of X. We use them a lot for graphing, to calculate residuals, and other fit metrics. This gives us the predicted mean at a given X
```{r}
library(broom)
augment(lm(child ~ parent, galton.data))
```

---
## fitted values

Bayesian analysis also has fitted values, but now we have many samples of parameters rather than just a single estimate for each value.


$$ \hat{Y}_{prediction} = b_o + b_1X $$
If we previous had 928 fitted values. We now have 928 participants * 1000 samples = 928,000 fitted values to work with. (Though people with the same X have the same yhat so it is effectively less)


---
.pull-left[ 
One reason fitted values are helpful is to showcase uncertainty. That is what our posterior is highlighting: that there is no ONE result, that there are many possible results. 

```{r, eval = FALSE}

ggplot(galton.data, aes(x = child, y = parent)) + 
  geom_point() +
  stat_smooth(method = "lm")
```
]

.pull-right[

```{r, echo = FALSE}

ggplot(galton.data, aes(x = child, y = parent)) + 
  geom_point() +
  stat_smooth(method = "lm")
```

]


---
.pull-left[ If we examine a expected/predicted mean at a certain value across all of our samples we directly compute our uncertainty. In contrast to frequentist where we have to use an ugly equation, which also uses the SEE equation, which has big assumptions. 

```{r, eval = FALSE}
mu_at_64 <- 
  post.tidy %>% 
  select(b_Intercept,b_parent_c) %>% 
  mutate(mu_at_64 = b_Intercept + (b_parent_c * -4.3))
 mu_at_64
```
]

.pull-right[

$$  \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}  $$



]
---

```{r, echo = FALSE}
mu_at_64 <- 
  post.tidy %>% 
  select(b_Intercept,b_parent_c) %>% 
  mutate(mu_at_64 = b_Intercept + (b_parent_c * -4.3))
 mu_at_64
```



---
## fitted values
.pull-left[
We can calculate not only the mean but also the dispersion. In lm land we had to use a funky equation to calculate the CI around some predicted value of X. Now we can use samples. It is just counting up where the xx% of samples fall. 


```{r, messages = FALSE, warning=FALSE}

mu_at_64 %>% 
mean_hdi(mu_at_64)
```
]

.pull-right[
```{r, echo = FALSE}
mu_at_64 %>%
  ggplot(aes(x = mu_at_64)) +
  geom_density() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["C-height at parent 64"])) 

```

]

---
.pull-left[ You can use standard ggplot geoms but I recommend using tidybayes (ggdist) specialty ones

```{r, eval=FALSE}

mu_at_64 %>%
  ggplot(aes(x = mu_at_64)) +
  stat_slab() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["C-height at parent 64"])) 

```
]

.pull-right[
```{r, echo = FALSE}

mu_at_64 %>%
  ggplot(aes(x = mu_at_64)) +
  stat_slab() +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["C-height at parent 64"])) 

```
]

---

.pull-left[
```{r, eval=FALSE}
mu_at_64 %>%
  ggplot(aes(x = mu_at_64)) +
  stat_halfeye(aes(fill = stat(cut_cdf_qi(cdf, 
    .width = c(.5, .8, .95),
    labels = scales::percent_format())))) +
  scale_fill_brewer(direction = -1, na.translate = FALSE) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(fill = "Interval")
```
]

.pull-right[
```{r, echo=FALSE}
mu_at_64 %>%
  ggplot(aes(x = mu_at_64)) +
  stat_halfeye(aes(fill = stat(cut_cdf_qi(
    cdf, 
    .width = c(.5, .8, .95),
    labels = scales::percent_format())))) +
  scale_fill_brewer(direction = -1, na.translate = FALSE) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(fill = "Interval")
```
]





---
## fitted values

We can also pass new data to this function. You can do this with {brms} `poseterior_predict` function, but I like the {tidybayes} `epred_draws()` better, especially when paired  with {modelr}::`data_grid`, as it is easier to pipe into ggplot and scales better


```{r}
galton.data %>% 
 add_epred_draws(m.2)
```

---
Fitted or expected values has 928,000 rows in the dataframe. Where does this number come from?  

Our posterior had 1000 rows (samples/draws). So this is taking each person and feeding them into the 1000 different regression equations and spitting out a fitted/expected/predicted value. 

Our posterior previously provided parameter level information. Here we are looking at fitted values for each individual/subject. 


---
## fitted values
.pull-left[
```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(modelr)
```

```{r, message=FALSE, warning=FALSE, eval = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.99), color = "grey") +
  geom_point(data = galton.data, size = 2)
```
]

.pull-right[
```{r, message=FALSE, warning=FALSE, echo = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.99), color = "grey") +
  geom_point(data = galton.data, size = 2)
```
]

---
Why data_grid from the modelr package? Because it makes feeding in values to our model very easy. What sort of values would we want to feed into it? Well if we want to graph across X, we should feed in all X values we want to plot against. 

```{r}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101))
```


---

Why are there 101,000 rows? 

```{r}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2)
```


---

.pull-left[
```{r, message=FALSE, warning=FALSE, eval = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.95), color = "grey") +
  geom_point(data = galton.data, size = 2)
```
]

.pull-right[
```{r, message=FALSE, warning=FALSE, echo = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  stat_lineribbon(aes(y = .epred), .width = c(.95), color = "grey") +
  geom_point(data = galton.data, size = 2)
```
]


---
## fitted values

.pull-left[
```{r, eval = FALSE}

labels <-  c(-2.5, 0, 2.5, 5) + mean(galton.data$parent) %>%   round(digits = 0)

galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2, ndraws = 100) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  geom_point(data = galton.data, size = 2) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels) 

```

]

.pull-right[


```{r, echo = FALSE}

labels <-  c(-2.5, 0, 2.5, 5) + mean(galton.data$parent) %>%   round(digits = 0)

galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2, ndraws = 100) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  geom_point(data = galton.data, size = 2) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels) 

```

]

---

Different from before is we are only adding a random subject of draws. Instead of all of them we are only doing 100. 

```{r}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2, ndraws = 100)
```
why only 10,100 rows whereas previously we had 101,000? 

---

Prevously we used: `stat_lineribbon(aes(y = .epred), .width = c(.95), color = "grey") ` to get the full confidence band. 

Here we are using `geom_line(aes(y = .epred, group = .draw), alpha = .1)` and creating a fitted value for each draw. 

This shows us that the posterior is just different possible regression lines. 

```{r, eval = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2, ndraws = 100) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  geom_point(data = galton.data, size = 2) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels)
```

---
```{r, echo = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2, ndraws = 100) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  geom_point(data = galton.data, size = 2) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels)
```

---
## fitted values

.pull-left[
Posterior samples provides us an estimate of different parameter values proportional to their likelihood (and prior) ie more samples near the mean. 

They also give an indication of the precision of the estimate, which is useful to create confidence intervals and do NHST like statistical tests of parameters. All uncertainty is captured in the posterior.
]

.pull-right[
Rather than just visualizing the marginal distribution, we use these samples to calculated fitted/expected values to: 
1. create model implied estimates at a specific value of X, ( $\hat{Y}$ | X )  
2. create CIs around these values
3. visualize model implied fits

]


---
### Multiple ways to predict
epred = fitted = uncertainty in the fixed coefficients and the uncertainty in the variance parameters for the grouping factors (for MLMs). Can be thought of as expected values or the mean prediction 

"predict" accounts for the residual (observation-level) variance, plus the other sources of variance in epred. This is used not to describe an expected value/mean but observation/individual level data. eg what is plausible when we collect a new subject. 

Whereas expectation values were good for inference, predictions are more for model checking. 

---
## predicted values
.pull-left[
```{r, eval = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
  add_predicted_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
     stat_lineribbon(aes(y = .prediction), 
                  .width = seq(.5, .99, by = .01), 
                  alpha = .5,
                  show.legend = F) +
  geom_point(data = galton.data, size = 2,alpha = .5) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels)
```
]

.pull-right[
```{r, echo = FALSE}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
  add_predicted_draws(m.2) %>% 
  ggplot(aes(x = parent_c, y = child)) +
     stat_lineribbon(aes(y = .prediction), 
                  .width = seq(.5, .99, by = .01), 
                  alpha = .5,
                  show.legend = F) +
  geom_point(data = galton.data, size = 2,alpha = .5) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels)
```
]

--- 
## predicted values

The plotted predictions can show you the potential spread in the cases. As opposed to epred/fitted values which are specific to $\mu$ s or other parameters we are trying to estimate, predicted values serve as simulated new data. 

If a model is a good fit we should be able to use it to generate data that resemble the data we observed. This is the basis of the posterior predictive distribution and PP checks. 

This is also what is meant by a `generative` model. 


---
```{r}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
  add_predicted_draws(m.2)
```
Where does 101,000 come from? 




---
# prediction in frequentist land
$$ \hat{Y}_i = \beta_0 + \beta_1 X     $$

```{r, eval = FALSE}
predict(lm_object, interval = confidence)
```

 
$$ \hat{Y}_i = \beta_0 + \beta_1 X + \epsilon_i $$

```{r, eval = FALSE}
predict(lm_object, interval = prediction)
```

The first method does not incorporate any error at all. The second incorporates error at the individual level. However, both assume Beta is true (they draw the best fit line) and do not incorporate what we know about sampling variability in our estimate of beta (outside of calculating CIs centered around that estimate). Bayesian methods take into account we do not know beta. 

---
## 3 types of predictions
$\hat{Y}_{prediction}$ ~ $b_o$ + $b_1$X, where we put in new values of X (often our data Xs we collected or values across the range of X)

1. lm style, where there is 1 value, our estimate, of $b_o$ & $b_1$

2. epred/fitted style, where we are propagating uncertainty in $b_o$ & $b_1$, as these parameters can take on many values according to the posterior distribution. The mean of this is equal to #1. 

3. prediction style, where we are propagating uncertainty in $b_o$, $b_1$ & $\hat{\sigma}$ . 



---
## Posterior predictive distribtion
.pull-left[

We can make a distribution based on this later form of prediction. 
]

.pull-right[
```{r, echo = FALSE}
# how many grid points would you like?
n <- 1001
n_success <- 6
n_trials  <- 9


  d <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n),
         # note we're still using a flat uniform prior
         prior  = 1) %>% 
  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))

  d %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = posterior),
              color = "grey67", fill = "grey67") +
  geom_segment(data = . %>% 
                 filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)),
               aes(xend = p_grid,
                   y = 0, yend = posterior, size = posterior),
               color = "grey33", show.legend = F) +
  geom_point(data = . %>%
               filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)),
             aes(y = posterior)) +
  annotate(geom = "text", x = .08, y = .0025,
           label = "Posterior probability") +
  scale_size_continuous(range = c(0, 1)) +
  scale_x_continuous("probability of water", breaks = c(0:10) / 10) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```
]


---
## Posterior predictive

.pull-left[
Now lets assume each of these parameter values are true. What would we expect? But we know they are not true; we know .1 while possible is very unlikely. So we need to weight that evidence. 
]

.pull-right[
```{r, echo = FALSE}
n_draws <- 1e5

simulate_binom <- function(probability) {
  set.seed(3)
  rbinom(n_draws, size = 9, prob = probability) 
}

d_small <-
  tibble(probability = seq(from = .1, to = .9, by = .1)) %>% 
  mutate(draws = purrr::map(probability, simulate_binom)) %>% 
  unnest(draws) %>% 
  mutate(label = str_c("p = ", probability))

d_small %>%
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Sampling distributions") +
  coord_cartesian(xlim = c(0, 9)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ label, ncol = 9) 

```
]

---
## Posterior predictive

.pull-left[
```{r, echo = FALSE}
samples <-
  d %>% 
  sample_n(size = 10000, weight = posterior, replace = T) %>% 
  mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9))

samples %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("number of water samples",
                     breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(xlim = c(0, 9),
                  ylim = c(0, 3000)) +
  theme(panel.grid = element_blank())

```
]

.pull-right[
This is the prediction if we include every value of Î¸ to make the prediction, not just our best estimate. This prediction incorporates all the uncertainty in the estimation of our parameter.
]


---
## posterior predictive distribution 
.pull-left[
How can we use this distribution? If a model is a good fit we should be able to generate data that resemble the data we observed. 

Taking samples of those possible "datasets" and plugging them into the model to get Y. Replications of Y (Yrep) from the posterior predictive distribution through `pp_check`
]

.pull-right[
```{r, message = FALSE, echo = FALSE}
pp_check(m.2)
```
]

---
# prior predictive by only sampling prior
This posterior predictive distribution is similar to the prior predictive distribution. Instead of taking our results and asking what could be, we are takin our priors and asking what could be. 
```{r}

m.2p <- 
  brm(family = gaussian,
      child ~ 1 + parent_c,
      prior = c(prior(normal(68, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 1), class = sigma)),
      data = galton.data,
      sample_prior = "only",
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "m.2p")

```


---

```{r}
summary(m.2p)
```

---

```{r}
galton.data %>% 
data_grid(parent_c = seq_range(parent_c, n = 101)) %>% 
 add_epred_draws(m.2p, ndraws = 100) %>% 
  ggplot(aes(x = parent_c, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5), labels = labels) 
```

