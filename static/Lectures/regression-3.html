<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Regression adv</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="regression-3_files/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
&lt;/style&gt;


## Goals for this section
Go deeper into basic linear models to become comfortable with standard regressions

Again, we will walk through an example or two to help us along

---
## Example data




```r
data &lt;- "https://raw.githubusercontent.com/josh-jackson/bayes/master/week3.csv"

week3 &lt;- read.csv(data) %&gt;% 
  select(-ID, -SES)

week3
```

```
##          happy   friends mood.group     health
## 1   1.86099188  4.176523          0  4.5640566
## 2   0.21912408  3.759815          0 10.6597953
## 3   3.54842009  9.497278          1 14.8440198
## 4   3.28845198  4.963567          2  4.9978947
## 5   1.24927862  1.335338          0  6.9382498
## 6   1.50027416  4.870580          1  3.7980809
## 7   2.92433313  6.215720          1  8.0270505
## 8   1.66733572  5.815054          2  9.9626835
## 9   3.11005982  7.683533          1  9.2059870
## 10  2.72914277  2.943047          1  8.4923032
## 11  4.39897865  4.032585          1 12.1208403
## 12  3.53437370  6.205217          1 12.4331327
## 13  3.65986297  9.261576          0  8.5555276
## 14  4.65817214  6.763594          1 11.3686611
## 15  2.27560297  4.851453          2 11.5694196
## 16  2.95159865  6.240207          0  5.4711721
## 17  4.10218047  6.973998          2  8.4519734
## 18  1.12059240  5.036057          2  4.5667653
## 19  2.05833415  7.740862          1 11.7526501
## 20  0.19305280  7.759608          1 10.8226415
## 21  3.50355773  8.749197          2  9.8105699
## 22  4.78418731  4.756792          1  9.1277596
## 23  1.06676001  6.561631          1  6.5318595
## 24  3.30530750  3.175915          1 10.0237012
## 25  4.61659441 11.228795          0 11.7420222
## 26  3.97859881  3.957705          0  7.2737794
## 27  0.35606277  7.813100          1  8.3704646
## 28  1.94703884  5.468486          2 12.0994488
## 29  2.03225608  5.250680          1  8.1637595
## 30  3.29677539  4.551704          1  9.5443353
## 31  2.11673573  6.521937          1  9.9945814
## 32  1.60492223  7.823219          1 12.8755751
## 33  0.98865367  4.883255          2  4.6909984
## 34  0.81585047  2.253814          2  8.9651992
## 35  2.61655537  5.785562          2  9.1560838
## 36  4.56739326  4.336673          0  5.4488116
## 37  1.03386359  6.607520          1 10.6606187
## 38  4.07141508  6.115068          1  7.2597276
## 39  0.10083599  3.711709          0  7.2679814
## 40  4.62402207  3.625557          1  8.6482218
## 41  2.17480873  6.198989          1  5.1070680
## 42  2.21027342  9.453457          0  9.1593706
## 43  3.80344728  8.985688          0  7.4339090
## 44  1.66480650  4.398902          2  9.0928075
## 45  1.97178576  3.670528          1  5.7313981
## 46  1.16647141  5.246911          0  5.3507789
## 47  0.35803829  4.411584          0  6.6060095
## 48  4.56691743  2.898346          0  2.6521072
## 49  3.85846787  4.868912          1  9.8621705
## 50  0.53768320  7.167517          1  7.0237089
## 51  0.39606125  2.217562          2  9.0207288
## 52  2.16944724  6.561022          0  3.4553030
## 53  3.39887123  5.997563          2  7.8377788
## 54  3.67178120  1.874784          2  8.6336387
## 55  2.26255437  7.119586          0 11.5985175
## 56  3.91892858  4.148590          0  5.2031040
## 57  3.39917769  1.847190          1  8.7228487
## 58  2.59351826  8.145468          0 11.9085631
## 59  3.45402877  4.329517          1  9.7333939
## 60  2.94137825 10.324863          0 11.4859977
## 61  4.07294834  4.787144          1 12.5438459
## 62  4.05022473  4.609969          1 14.2980607
## 63  3.04967951  8.279238          2 12.1461175
## 64  4.96704822  6.431736          0 10.7821185
## 65  4.21620271  6.172160          0  8.6486218
## 66  3.57751702  6.128896          0  8.3563001
## 67  0.09540594  6.771640          2  9.6833816
## 68  1.52540126  3.844246          1 11.6272520
## 69  4.41473992  5.363370          1  9.2779219
## 70  4.70673408  6.610789          1 10.3634385
## 71  1.17209005  6.380328          2  4.2802691
## 72  4.68332914  5.253042          0 13.0007468
## 73  2.83350988  5.942500          0 10.1358263
## 74  4.21395202  3.962200          1 11.1562617
## 75  4.10650590  6.360323          1  8.5981369
## 76  1.39986427  9.929426          2 14.2260172
## 77  0.23650569  5.856582          0  4.3491779
## 78  1.12470314  5.811277          0  8.8571921
## 79  3.36546324  6.502768          1  9.7491380
## 80  4.79465429  7.123532          2 14.0765217
## 81  3.42629986  1.933902          2 10.0404019
## 82  3.87901492  2.988022          1  7.6101350
## 83  3.87919245  3.052267          1  6.6400716
## 84  4.91397400  7.339483          0  6.1480330
## 85  0.05094323  8.305993          1  8.6992735
## 86  4.76292926  8.465961          1 10.9683070
## 87  1.61467931  6.698720          0 10.0373042
## 88  2.14002672  1.257154          0  9.4460877
## 89  0.67248346  5.999466          1  5.2918581
## 90  0.09164156  3.679235          1  7.7655590
## 91  3.28249751  7.978303          1  6.3372499
## 92  4.57366668  8.871538          1 16.5109072
## 93  3.57649926  6.245307          1  3.3817140
## 94  0.91475723  3.505397          1  6.1956627
## 95  1.20034086  4.489978          2  6.2224089
## 96  4.18224540  4.200512          1  7.7401760
## 97  1.92936728  8.733855          0  7.8723238
## 98  1.16286902  5.670159          2  7.9856544
## 99  0.34460320  7.566552          1  8.8051692
## 100 0.31112264  8.245090          0 10.4014561
## 101 0.62509687  7.125878          1  8.7113863
## 102 0.11663346  5.159979          1  8.2499019
## 103 1.95930638  4.705842          1  6.6148716
## 104 4.29799283  6.150367          1 10.5030389
## 105 3.59167261  9.891258          1 12.1594813
## 106 1.69697514  4.104596          2  0.7066834
## 107 0.40610714 10.638080          1  4.4338189
## 108 0.18617163  5.980704          1  3.0972955
## 109 3.86544394  3.717763          0  9.7471429
## 110 4.97541129  5.423782          1  9.8573015
## 111 0.73293216  5.734030          1 11.8521843
## 112 0.19922433  7.690971          1  5.8622057
## 113 2.82838948  6.739700          2 12.4677636
## 114 4.44292936  5.478127          0 12.2742321
## 115 4.35360831  5.753884          0  5.5666706
## 116 4.90874716  6.339427          1  7.2931286
## 117 4.39873448  7.578514          0  5.5900820
## 118 2.55200868  9.937304          1  7.9032039
## 119 1.67187673  4.689934          0 11.3572772
## 120 3.06667657  6.842705          2  5.9220865
## 121 1.98754107  7.124963          2 11.6018189
## 122 0.69967554  4.787041          1  2.4838889
## 123 0.39548398  8.782787          0  9.4450330
## 124 2.75110764  5.254739          1  5.9642775
## 125 1.30322599  6.656962          0  7.5035014
## 126 4.04764926  7.798535          2 11.2762001
## 127 2.72384837  8.493368          1 13.7008502
## 128 2.37246631  9.073152          0  6.8080581
## 129 3.31854011  6.922207          1  9.6155361
## 130 0.46088128  6.629294          0  5.7457813
## 131 3.25280462  1.379784          1  6.5731088
## 132 1.84239543  9.349398          1 10.8295921
## 133 1.22894949  8.168235          2  3.7612117
## 134 1.49301856  8.113731          0  9.4065741
## 135 2.79592354  6.344285          2 14.2740437
## 136 2.40177790  6.216542          2 10.1128493
## 137 2.38506903  6.401344          2 12.1275318
## 138 4.67767311  4.378284          0  4.8474773
## 139 2.35088812  8.547497          2  8.5433200
## 140 3.39098321  8.220029          1  4.9781351
## 141 4.67232602  5.438038          0  3.4850653
## 142 1.36999726  5.239475          1  9.4172956
## 143 4.73688548  5.607044          0  4.0916033
## 144 1.56651031  8.808961          0 10.2134654
## 145 4.37788633  5.328912          1 10.7817952
## 146 0.83679943  5.755841          0  3.9134767
## 147 2.34535844  4.309261          0  4.1947270
## 148 3.26114870  5.213310          0  4.5979312
## 149 0.17230356  8.239418          1  6.7310373
## 150 2.17726158  5.316205          0  7.7475791
## 151 0.74882179  6.152522          0  7.0976454
## 152 2.29452922  8.025877          0 12.6225560
## 153 3.09430787  3.590427          1  9.9609391
## 154 4.78146670  4.334285          0 11.0856280
## 155 0.50317101  8.461995          2  8.0226430
## 156 1.13857693  7.139152          1 10.1686641
## 157 2.77506881  7.776015          2  8.8173456
## 158 3.85514822 10.987422          1  9.0744789
## 159 2.39834542  7.423034          0  8.9792152
## 160 4.40518654  5.621692          2  6.3223738
## 161 4.83998839  4.824513          2  8.6954419
## 162 3.45249020  5.820828          0  9.0318686
## 163 4.33430550  7.831526          0  9.5766306
## 164 2.80141790  7.110392          0  9.6103342
## 165 1.52272068  4.976290          2  5.9412234
## 166 4.99643776  6.782215          1 13.1183355
## 167 1.46630822  2.831823          2  9.8071108
## 168 4.51685685  5.514937          2  9.9470844
## 169 0.20955528  5.732482          1  5.2052072
## 170 2.99262897  7.927739          2  9.8307414
## 171 3.40545098  1.673944          1  6.9373695
## 172 4.91519878  6.897450          1  9.1871176
## 173 2.51074571  5.406566          0  3.1179416
## 174 3.71420533  2.430032          0  9.4137709
## 175 4.55589083  5.358929          0  8.9082071
## 176 4.94190076  6.763785          2 12.3765379
## 177 3.82748409  3.726258          0  4.6645082
## 178 4.10263621  8.176609          0 10.0199091
## 179 4.69867281  5.024115          1  8.3312329
## 180 3.35813491  5.992240          1  7.1345108
## 181 4.53746158  4.527805          2  9.0434508
## 182 3.81145062  4.309662          1  2.9235078
## 183 2.43166442  5.913238          1  5.9675206
## 184 1.25104343  3.911480          2 10.9791082
## 185 1.79522359  6.340832          2  8.2827677
## 186 0.04296803  7.223363          1  7.5511441
## 187 1.17855788  5.719613          0  7.5014892
## 188 0.53117367  9.412720          0 10.8890809
## 189 3.05516987  6.772035          1  6.6605963
## 190 1.02348501  3.012060          1  7.0083721
## 191 1.07305212  4.489611          2  7.2772011
## 192 0.08245872  1.610812          1  9.7877898
## 193 1.64097183  7.661981          1  8.3123487
## 194 1.34797638  5.813819          2  9.4293247
## 195 4.56936203  5.510436          0  4.2547183
## 196 2.08935902  2.683754          1  7.5454356
## 197 3.45282825  4.584966          1  3.7862375
## 198 4.50006446  8.179837          0 13.4403083
## 199 1.03854465  6.469114          2 11.8725661
## 200 2.30516580  4.046544          0  9.5992624
```

---

![](regression-3_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---
## Model

Health ~ Normal( `\(\mu_i\)` , `\(\sigma\)` )  

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` `\(Happy_i - {\overline{\mbox{Happy}}}\)`    

`\(\beta_0\)` ~ Normal(0, 5)   
`\(\beta_1\)` ~ Normal(0, 5)   
`\(\sigma\)`  ~ HalfCauchy(0,10) 

---
## Prior Predictive Distribution


```r
library(brms)

week3 &lt;- week3 %&gt;% 
  mutate(happy_c = happy - mean(happy))

prior.1 &lt;- prior(normal(0, 5), class = Intercept) +
                prior(normal(0, 5), class = b) +
                prior(cauchy(0, 10), class = sigma)


h.1p &lt;- 
  brm(family = gaussian,
      health ~ 1 + happy_c,
      prior = prior.1,
      data = week3,
      sample_prior = "only",
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.1p")
```

---

.pull-left[

```r
library(tidybayes)
prior.1 %&gt;% 
  parse_dist(prior) %&gt;% 
  ggplot(aes(y=class, dist =.dist, args=.args)) +
  stat_dist_halfeye()+
  scale_x_continuous( limits = c(-50, 50))+
  labs(title="Priors")
```
]

.pull-right[
![](regression-3_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]


---

.pull-left[

```r
pp_check(h.1p) + xlim(-50,50)
```

![](regression-3_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

.pull-right[
This graphs 10 simulated datasets from posterior predictive distribution, compared to the our actual density distribution  

How is this the posterior predictive distribution if we just sampled from the prior? We treat the priors as if they were the posteriors.  

]

---

```r
pp_check(h.1p,
         type = 'intervals')
```

![](regression-3_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---


```r
library(modelr)
labels &lt;-  c(-2, -1, 0, 1, 2) + mean(week3$happy) %&gt;%   round(digits = 0)
week3 %&gt;% 
data_grid(happy_c = seq_range(happy_c, n = 101)) %&gt;% 
 add_epred_draws(h.1p, ndraws = 100) %&gt;% 
  ggplot(aes(x = happy_c, y = health)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .1) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2), labels = labels) +
  xlab("happy")
```

---


```
## 
## Attaching package: 'modelr'
```

```
## The following object is masked from 'package:broom':
## 
##     bootstrap
```

![](regression-3_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;


---


```r
prior.2 &lt;- prior(normal(0, 2), class = Intercept) +
                prior(normal(0, 2), class = b) +
                prior(cauchy(0, 1), class = sigma)
```




```r
h.2p &lt;- 
  brm(family = gaussian,
      health ~ 1 + happy_c,
      prior = prior.2,
      data = week3,
      sample_prior = "only",
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.2p")
```


---

.pull-left[
![](regression-3_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

]

.pull-right[
![](regression-3_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

---
.pull-left[
![](regression-3_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]

.pull-right[
![](regression-3_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;
]


---
## Fit model


```r
h.1 &lt;- 
  brm(family = gaussian,
      health ~ 1 + happy_c,
      prior = c(prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 10), class = sigma)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.1")
```


---
### view priors


```r
prior_summary(h.1)
```

```
##          prior     class    coef group resp dpar nlpar bound       source
##   normal(0, 5)         b                                             user
##   normal(0, 5)         b happy_c                             (vectorized)
##   normal(0, 5) Intercept                                             user
##  cauchy(0, 10)     sigma                                             user
```

---


```r
summary(h.1, prob = .99)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: health ~ 1 + happy_c 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-99% CI u-99% CI Rhat Bulk_ESS Tail_ESS
## Intercept     8.51      0.20     8.01     9.03 1.00      961      672
## happy_c       0.33      0.13    -0.03     0.68 1.00      968      670
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-99% CI u-99% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.81      0.14     2.47     3.19 1.00      827      722
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
library(broom)
tidy(lm(health ~ happy_c, data = week3))
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    8.51      0.198     43.1  1.53e-102
## 2 happy_c        0.338     0.132      2.56 1.13e-  2
```


```r
glance(lm(health ~ happy_c, data = week3))
```

```
## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    0.0320        0.0271  2.79      6.54  0.0113     1  -488.  982.  992.
## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```

---

```r
plot(h.1)
```

![](regression-3_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;

---
### what are chains? 

Chains are the different sampling processes occurring simultaneously. The default is 4 chains, with 1000 samples from each chain, resulting in 40,000 samples of the posterior. 

Each chain is independent of other chains so as to ensure that the procedure can replicate 

---
## The posterior is made of samples


```r
library(tidybayes)
p.h1 &lt;- h.1 %&gt;% 
spread_draws(b_Intercept, b_happy_c, sigma)
p.h1
```

```
## # A tibble: 1,000 × 6
##    .chain .iteration .draw b_Intercept b_happy_c sigma
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
##  1      1          1     1        8.73    0.331   2.69
##  2      1          2     2        8.36    0.403   2.65
##  3      1          3     3        8.66    0.256   2.95
##  4      1          4     4        8.15    0.415   2.71
##  5      1          5     5        8.88    0.314   2.94
##  6      1          6     6        8.38    0.360   2.70
##  7      1          7     7        8.64    0.224   2.88
##  8      1          8     8        8.67    0.0712  2.70
##  9      1          9     9        8.41    0.0570  3.17
## 10      1         10    10        8.42    0.371   2.67
## # … with 990 more rows
```

---

```r
p.h1.long &lt;- h.1 %&gt;% 
gather_draws(b_Intercept, b_happy_c, sigma)
p.h1.long
```

```
## # A tibble: 3,000 × 5
## # Groups:   .variable [3]
##    .chain .iteration .draw .variable   .value
##     &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1      1          1     1 b_Intercept   8.73
##  2      1          2     2 b_Intercept   8.36
##  3      1          3     3 b_Intercept   8.66
##  4      1          4     4 b_Intercept   8.15
##  5      1          5     5 b_Intercept   8.88
##  6      1          6     6 b_Intercept   8.38
##  7      1          7     7 b_Intercept   8.64
##  8      1          8     8 b_Intercept   8.67
##  9      1          9     9 b_Intercept   8.41
## 10      1         10    10 b_Intercept   8.42
## # … with 2,990 more rows
```

---

```r
p.h1.long %&gt;% 
    mean_qi()
```

```
## # A tibble: 3 × 7
##   .variable   .value .lower .upper .width .point .interval
##   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1 b_happy_c    0.333 0.0799  0.594   0.95 mean   qi       
## 2 b_Intercept  8.51  8.11    8.88    0.95 mean   qi       
## 3 sigma        2.81  2.56    3.09    0.95 mean   qi
```

---


```r
p.h1.long %&gt;% 
ggplot(aes(y = .variable, x = .value)) +
    stat_halfeye()
```


![](regression-3_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;

---


```r
h.1 %&gt;% 
gather_draws(b_happy_c) %&gt;% 
ggplot(aes(y = .variable, x = .value)) +
    stat_halfeye(aes(fill = stat(x) &lt; 0)) +
  scale_fill_manual(values = c("gray85", "skyblue"))
```

![](regression-3_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;


---
### samples imply multiple possible regression lines



&lt;img src="../media/p-is-lines.gif" width="850" height="550" /&gt;


---
## model fit

So far the model is the same as frequentist. The major difference is in the posterior, but even then we can use what we know about sampling to get estimates, fitted values, CIs, etc. 

How can we evaluate our model like we normally do? R2 is most common and we have an equivalent. However, we will have additional tricks up our sleeve as the semester progresses in temrs of model comparison (eg waic, loo -- similar to AIC/BIC, likelihood ratio tests etc).


---

```r
head(bayes_R2(h.1, summary = F)) # use = FALSE to get samples of each R2
```

```
##              R2
## [1,] 0.03080611
## [2,] 0.04482768
## [3,] 0.01855082
## [4,] 0.04751632
## [5,] 0.02768777
## [6,] 0.03608203
```



---
## Bayesian R2


```r
bayes_R2(h.1, summary = T) # use = true to get a summary
```

```
##      Estimate  Est.Error        Q2.5      Q97.5
## R2 0.03503015 0.02384865 0.002083316 0.09120802
```
Typically it is the variance of the predicted values divided by total variance. 

But for Bayes we have 1) many predicted values (1 for each sample) and 2) we want to incorporate uncertainty in the estimates


---

## Bayesian R2

(Predicted) explained variance over (predicted) explained variance + unexplained variance. But what is explained variance? What is unexplained variance? In frequentist land it was specific to our sample. But we know our sample estimates are subject to sampling variability.

We use: posterior predictive distribution &amp; sigma


---



```r
pp_check(h.1)
```

![](regression-3_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;



---
## Variable coding? 

Of course anytime we use group variables we need to assign them numeric values. How does that work in Bayesian land? Mostly the same but 
1. we need to be careful about our priors. 
2. need to work with the prior to give us what we want
3. There is an addition coding option that is often helpful  

---
## Categorical data

.pull-left[

```r
week3 &lt;-week3 %&gt;% 
mutate(mood.group.d = recode(mood.group, 
                             '0' = "control", 
                             '1' = "tx", 
                             '2' = "tx")) 
       
table(week3$mood.group.d)
```

```
## 
## control      tx 
##      66     134
```
]

.pull-right[

```r
h.2 &lt;- 
  brm(family = gaussian,
      health ~ 1 + mood.group.d,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 10), class = sigma)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.2")
```

]

---

```r
summary(h.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: health ~ 1 + mood.group.d 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          8.01      0.35     7.33     8.70 1.00      927      661
## mood.group.dtx     0.76      0.44    -0.18     1.66 1.01      820      585
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.83      0.15     2.57     3.13 1.00     1239      682
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---


```r
week3 %&gt;% 
  group_by(mood.group.d) %&gt;% 
  summarise(r = mean(health)) %&gt;% 
  mutate(r=round(r,2))
```

```
## # A tibble: 2 × 2
##   mood.group.d     r
##   &lt;chr&gt;        &lt;dbl&gt;
## 1 control       7.99
## 2 tx            8.77
```

---


```r
get_variables(h.2)
```

```
##  [1] "b_Intercept"      "b_mood.group.dtx" "sigma"            "lp__"            
##  [5] "accept_stat__"    "stepsize__"       "treedepth__"      "n_leapfrog__"    
##  [9] "divergent__"      "energy__"
```



---
.pull-left[
Add the parameters to get group scores

```r
posth.2 &lt;- h.2 %&gt;% 
  spread_draws(b_Intercept, b_mood.group.dtx) %&gt;% 
  rename(control = b_Intercept) %&gt;% 
  mutate(tx = control + b_mood.group.dtx) %&gt;% 
  select(control, tx) %&gt;% 
  gather() %&gt;% 
  ggplot(aes(y = key, x = value)) +
  stat_halfeye() 
```
]

.pull-right[

![](regression-3_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;
]


---

![](regression-3_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;

---
### standardized difference in means

.pull-left[
We can also manipulate the posterior to get effect size estimates. 

```r
  ph2.1 &lt;- h.2 %&gt;% 
  spread_draws(b_Intercept, b_mood.group.dtx, sigma) %&gt;% 
  rename(control = b_Intercept) %&gt;% 
  mutate(tx = control + b_mood.group.dtx) %&gt;% 
  select(control, tx, sigma) %&gt;% 
  mutate(`difference` = tx - control) %&gt;%  # or just b_mood.group.dtx
  mutate(`std.effect` =  difference / sigma) 
  ph2.1
```

```
## # A tibble: 1,000 × 5
##    control    tx sigma difference std.effect
##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1    8.02  8.94  2.76      0.921     0.334 
##  2    8.56  8.90  2.89      0.339     0.117 
##  3    8.54  8.84  2.95      0.304     0.103 
##  4    8.35  8.69  2.99      0.336     0.113 
##  5    7.75  8.96  2.72      1.21      0.445 
##  6    8.11  8.55  2.91      0.433     0.149 
##  7    7.69  8.46  2.78      0.771     0.278 
##  8    8.72  8.89  2.83      0.173     0.0610
##  9    8.52  8.08  2.62     -0.442    -0.169 
## 10    8.32  8.65  3.05      0.329     0.108 
## # … with 990 more rows
```
]

.pull-right[


```r
ph2.1 %&gt;% 
  select(std.effect, difference) %&gt;% 
  gather() %&gt;% 
  ggplot(aes(y = key, x = value)) +
  stat_dotsinterval() 
```

![](regression-3_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

]

---



```r
 h.2 %&gt;% 
  spread_draws(b_Intercept, b_mood.group.dtx, sigma) %&gt;% 
  rename(control = b_Intercept) %&gt;% 
  mutate(tx = control + b_mood.group.dtx) %&gt;% 
  mutate(`difference` = tx - control) %&gt;%  
  mutate(`std.effect` =  difference / sigma) %&gt;% 
  gather_draws(difference, std.effect) %&gt;% 
  median_qi()
```

```
## # A tibble: 2 × 7
##   .variable  .value  .lower .upper .width .point .interval
##   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1 difference  0.771 -0.181   1.66    0.95 median qi       
## 2 std.effect  0.275 -0.0608  0.588   0.95 median qi
```

---
## Index variables

One issue with standard coding approaches is that there are two priors for the group coded 1. One prior on the difference and one on the intercept. This is a little awkward in that more of the prior is being allocated to that group. 

What we could do instead is not fit an intercept and instead directly model two group means. This a more intuitive approach in that when you first learn regression this is what most people think occurs. 

While possible in a frequentist framework it is a little awkward because you need to test the hypothesis that the groups differ

---

```r
h.3 &lt;- 
  brm(family = gaussian,
      health ~ 0 + mood.group.d,
      prior = c(prior(normal(0, 10), class = b),
                prior(cauchy(0, 10), class = sigma)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.3")
```


We suppress the intercept by putting in a 0. This "no intercept" model is a general strategy for working with categorical data within Bayes. 

---
Note the ridiculous R2. Formula doesn't work without an intercept 

```r
test2 &lt;- lm(health ~ 0 + mood.group.d, data = week3)
summary(test2)
```

```
## 
## Call:
## lm(formula = health ~ 0 + mood.group.d, data = week3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.0622 -2.1352  0.1223  1.9220  7.7420 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## mood.group.dcontrol   7.9873     0.3465   23.05   &lt;2e-16 ***
## mood.group.dtx        8.7689     0.2432   36.06   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.815 on 198 degrees of freedom
## Multiple R-squared:  0.9024,	Adjusted R-squared:  0.9015 
## F-statistic: 915.8 on 2 and 198 DF,  p-value: &lt; 2.2e-16
```

---


```r
summary(h.3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: health ~ 0 + mood.group.d 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## mood.group.dcontrol     7.96      0.36     7.23     8.65 1.00      850      666
## mood.group.dtx          8.76      0.24     8.27     9.21 1.00     1114      610
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.83      0.14     2.56     3.12 1.00      823      613
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

.pull-left[

```r
h.3 %&gt;%
  gather_draws(b_mood.group.dcontrol, b_mood.group.dtx) %&gt;%
  ggplot(aes(x= .value, y = .variable)) +
  stat_halfeye() 
```

![](regression-3_files/figure-html/unnamed-chunk-48-1.png)&lt;!-- --&gt;

]


.pull-right[

![](regression-3_files/figure-html/unnamed-chunk-49-1.png)&lt;!-- --&gt;
]

---
Index variables

In addition to getting everything you want from a normal dummy treatment of variables, an additional benefit is fewer priors! Think about a prior for 4 group variable typical way where you have to have a new dummy for each of them. That dummy would be a new parameter where you would have to do a new prior on. Here there is one prior for all categories AND they are on the mean of the category rather than the difference.  


---
## Hypothesis function
a way to set up contrasts within brms



```r
hyp.1 &lt;- hypothesis(h.3, "mood.group.dtx = mood.group.dcontrol")
hyp.1
```

```
## Hypothesis Tests for class b:
##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
## 1 (mood.group.dtx)-... = 0     0.79      0.43    -0.04     1.68         NA
##   Post.Prob Star
## 1        NA     
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```
Equivalent* to our dummy coded regression coefficient (*within sampling error)
---


```r
hyp.2 &lt;- hypothesis(h.3, "mood.group.dtx - mood.group.dcontrol = 0")
hyp.2
```

```
## Hypothesis Tests for class b:
##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
## 1 (mood.group.dtx-m... = 0     0.79      0.43    -0.04     1.68         NA
##   Post.Prob Star
## 1        NA     
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```

---


```r
hyp.3 &lt;- hypothesis(h.3, "mood.group.dtx &gt; mood.group.dcontrol")
hyp.3
```

```
## Hypothesis Tests for class b:
##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
## 1 (mood.group.dtx)-... &gt; 0     0.79      0.43      0.1     1.48      33.48
##   Post.Prob Star
## 1      0.97    *
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```

---

.pull-left[

```r
plot(hyp.1)
```

![](regression-3_files/figure-html/unnamed-chunk-53-1.png)&lt;!-- --&gt;
]


.pull-right[

![](regression-3_files/figure-html/unnamed-chunk-54-1.png)&lt;!-- --&gt;


]

---


```r
hyp.1 &lt;- hypothesis(h.3, "mood.group.dtx &lt; 9")
hyp.1
```

```
## Hypothesis Tests for class b:
##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
## 1 (mood.group.dtx)-(9) &lt; 0    -0.24      0.24    -0.64     0.14       5.29
##   Post.Prob Star
## 1      0.84     
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```

---
There are other packages that we will use to further explore our groups. Two of note: 
1. the {easystats} suite, specifically {bayestestR}
2. {emmeans}

---

```r
library(easystats)
```

```
## # Attaching packages: easystats 0.4.3 (red = needs update)
## ✖ insight     0.14.5.1     ✖ datawizard  0.2.1.9001
## ✔ bayestestR  0.11.5.1     ✔ performance 0.8.0.1   
## ✖ parameters  0.15.0.1     ✖ effectsize  0.5.0.10  
## ✖ modelbased  0.7.0.1      ✔ correlation 0.7.1.1   
## ✔ see         0.7.0.1      ✖ report      0.4.0.1   
## 
## Restart the R-Session and update packages in red with 'easystats::easystats_update()'.
```

```r
describe_posterior(h.3)
```

```
## Summary of Posterior Distribution
## 
## Parameter           | Median |       95% CI |   pd |          ROPE | % in ROPE |  Rhat |    ESS
## -----------------------------------------------------------------------------------------------
## mood.group.dcontrol |   7.97 | [7.19, 8.60] | 100% | [-0.28, 0.28] |        0% | 0.998 | 826.00
## mood.group.dtx      |   8.76 | [8.29, 9.22] | 100% | [-0.28, 0.28] |        0% | 1.001 | 978.00
```


---
## posterior predictive checks

*If a model is a good fit we should be able to use it to generate data that resemble the data we observed.*

To evaluate our model we want to take our posteriors we estimated and plug them into our equation to simulate different Y values across our X variable(s). This is the posterior predictive distribution. It incorporates uncertainty about each of the parameters. 


---

.pull-left[We can compare a distribution of simulated Ys to our actual Ys. Do they look similar? To the extent they don't this represents misfit in our model. 
]

.pull-right[

```r
pp_check(h.3)
```

```
## Using 10 posterior draws for ppc type 'dens_overlay' by default.
```

![](regression-3_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;

]

---


```r
pp_check(h.3,"violin_grouped", group = "mood.group.d")
```

```
## Using all posterior draws for ppc type 'violin_grouped' by default.
```

![](regression-3_files/figure-html/unnamed-chunk-58-1.png)&lt;!-- --&gt;


---
## Extending the basic model

Now we have the basics down we can extend the model, tweaking different components of it. We have already seen what happens if we tweak the priors a little. Thus far they have been mostly inconsequential. 

What if we tweak the likelihood? 

---
## Robust regression

In psych we have a lot of error prone people/experiments/data-entry that leads to "outliers"

Trimming them is one option. But why not incorporate our model to expect outliers, especially when working with low N situations. 

Enter the t-distribution. Modeling with heavy tailed distributions like the t helps minimize the effect of outliers on central tendency. Other "robust" approaches outside of Bayes use medians instead of means. 

(Note that a frequentist t-test does not assume a t DGP -- only that the sampling distribution is distributed as a t, just like any linear model)

---
### Model

Health ~ T( `\(\nu\)` , `\(\mu_i\)` , `\(\sigma\)` )  

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` `\(G_i\)`    

`\(\nu\)` ~ Gamma(2,.1)  
`\(\beta_0\)` ~ Normal(0, 10)   
`\(\beta_1\)` ~ Normal(0, 10)   
`\(\sigma\)`  ~ HalfCauchy(0,10) 


---

### gamma prior
.pull-left[

```r
prior(gamma(2,.1), class = nu) %&gt;% 
  parse_dist(prior) %&gt;% 
  ggplot(aes(y=class, dist =.dist, args=.args)) +
  stat_dist_halfeye()+
  labs(title="Priors")
```
]


.pull-rihght[
![](regression-3_files/figure-html/unnamed-chunk-60-1.png)&lt;!-- --&gt;
]

---


```r
prior(student_t(2,0,1)) %&gt;% 
  parse_dist(prior) %&gt;% 
  ggplot(aes(y=class, dist =.dist, args=.args)) +
  stat_dist_halfeye()+
  labs(title="Priors")
```

![](regression-3_files/figure-html/unnamed-chunk-61-1.png)&lt;!-- --&gt;

```r
expand.grid(
  df = c(1,3,5,10,30),
  scale = c(1)) %&gt;%
  ggplot(aes(y = 0, dist = "student_t", arg1 = df, arg2 = 0, arg3 = scale, color = ordered(df))) +
  stat_dist_slab(p_limits = c(.01, .99), fill = NA) +
  scale_y_continuous(breaks = NULL) +
  facet_grid( ~ scale)+ xlim(-5,5)
```

![](regression-3_files/figure-html/unnamed-chunk-61-2.png)&lt;!-- --&gt;



---
We are going to use a t-distribution as our likelihood. 


```r
h.4 &lt;- 
  brm(family = student,
      health ~ 1 + mood.group.d,
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1),  class = sigma)),
                file = "h.4.rds",
                data = week3)
```


---
### fixing nu
Instead of estimating nu we could also fix it to be low, thereby ensuring that we have fat tails. The previous approach allowed us to have skinnier tails if the data suggested skinnier tails. 


```r
h.5 &lt;- 
  brm(family = student,
      bf(health ~ 1 + mood.group.d, nu = 4),
      prior =   prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1),  class = sigma)),
                file = "h.5.rds",
                data = week3)
```


---
## Welches t-test?

.pull-left[
Robust and unequal variances

Health ~ T( `\(\nu\)` , `\(\mu_i\)` , `\(\sigma_i\)` )  

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` `\(Group_i\)`    
`\(\sigma_i\)` = `\(\gamma_0\)` + `\(\gamma_1\)` `\(Group_i\)` 

`\(\nu\)` ~     Gamma(2,.1)  
`\(\beta_0\)` ~ Normal(0, 10)   
`\(\beta_1\)` ~ Normal(0, 10)   
`\(\gamma_0\)` ~ Normal(0, 3)  
`\(\gamma_1\)` ~ Normal(0, 3)  
]

.pull-right[
Welches, but not robust

Health ~ Normal( `\(\mu_i\)` , `\(\sigma_i\)` )  

`\(\mu_i\)` = `\(\beta_0\)` + `\(\beta_1\)` `\(Group_i\)`    
`\(\sigma_i\)` = `\(\gamma_0\)` + `\(\gamma_1\)` `\(Group_i\)` 

`\(\nu\)` ~     Gamma(2,.1)  
`\(\beta_0\)` ~ Normal(0, 10)   
`\(\beta_1\)` ~ Normal(0, 10)   
`\(\gamma_0\)` ~ Normal(0, 3)  
`\(\gamma_1\)` ~ Normal(0, 3) 
]


---

```r
h.6 &lt;- 
  brm(family = student,
     bf( health ~ 1 + mood.group.d,
         sigma ~ 1 + mood.group.d),
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 10), class = Intercept),
                prior(normal(0, 3), class = b),
                prior(normal(0, 10), class = Intercept, dpar = sigma),
                prior(normal(0, 3), class = b, dpar = sigma)),
                file = "h.6.rds",
                data = week3)
```


---


```r
summary(h.6)
```

```
##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: health ~ 1 + mood.group.d 
##          sigma ~ 1 + mood.group.d
##    Data: week3 (Number of observations: 200) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## Intercept                8.00      0.35     7.32     8.71 1.00     4633
## sigma_Intercept          1.01      0.09     0.84     1.20 1.00     5154
## mood.group.dtx           0.77      0.43    -0.07     1.58 1.00     4896
## sigma_mood.group.dtx    -0.01      0.11    -0.24     0.20 1.00     5091
##                      Tail_ESS
## Intercept                3037
## sigma_Intercept          2717
## mood.group.dtx           3111
## sigma_mood.group.dtx     3404
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## nu    29.03     14.63     9.33    64.83 1.00     4711     3016
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
plot(h.6)
```

![](regression-3_files/figure-html/unnamed-chunk-66-1.png)&lt;!-- --&gt;


---


```r
h.7 &lt;- 
  brm(family = student,
     bf( health ~ 0 + mood.group.d,
         sigma ~ 0 + mood.group.d),
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 3), class = b),
                prior(normal(0, 3), class = b, dpar = sigma)),
                file = "h.7.rds",
                data = week3)
```

---


```r
summary(h.7)
```

```
##  Family: student 
##   Links: mu = identity; sigma = log; nu = identity 
## Formula: health ~ 0 + mood.group.d 
##          sigma ~ 0 + mood.group.d
##    Data: week3 (Number of observations: 200) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## mood.group.dcontrol           7.89      0.35     7.17     8.55 1.00     4519
## mood.group.dtx                8.72      0.24     8.26     9.19 1.00     4201
## sigma_mood.group.dcontrol     1.01      0.09     0.84     1.20 1.00     4456
## sigma_mood.group.dtx          1.00      0.07     0.86     1.13 1.00     4725
##                           Tail_ESS
## mood.group.dcontrol           3100
## mood.group.dtx                2482
## sigma_mood.group.dcontrol     2833
## sigma_mood.group.dtx          2880
## 
## Family Specific Parameters: 
##    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## nu    29.09     14.60     9.56    65.06 1.00     3868     3198
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---
The sigmas are modeled through a log-link (to make them positive). Convert them back to the scale by exponential.


```r
post.h7 &lt;- h.7 %&gt;% 
  spread_draws(b_mood.group.dcontrol,b_mood.group.dtx, b_sigma_mood.group.dcontrol,b_sigma_mood.group.dtx) %&gt;% 
  transmute(`Control`     = b_mood.group.dcontrol,
            `Tx`  = b_mood.group.dtx,
            `Sigma Control`    = b_sigma_mood.group.dcontrol%&gt;% exp(),
            `Sigma Tx` = b_sigma_mood.group.dtx %&gt;% exp()) %&gt;% 
  mutate(`Differences`  = `Tx` - `Control`,
         `Sigma Difference` = `Sigma Tx` - `Sigma Control`,
         `Effect Size` = (`Differences`) / sqrt((`Sigma Control`^2 + `Sigma Tx`^2) / 2))


post.h7 
```

```
## # A tibble: 4,000 × 7
##    Control    Tx `Sigma Control` `Sigma Tx` Differences `Sigma Difference`
##      &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;              &lt;dbl&gt;
##  1    7.98  8.63            2.31       2.54       0.652             0.233 
##  2    8.14  8.79            2.61       2.89       0.653             0.274 
##  3    8.11  9.02            2.47       2.79       0.912             0.315 
##  4    7.83  8.74            3.01       2.46       0.912            -0.545 
##  5    8.17  8.65            2.54       3.00       0.487             0.463 
##  6    7.82  8.62            2.85       2.76       0.799            -0.0883
##  7    7.28  8.76            3.01       2.68       1.48             -0.322 
##  8    8.11  8.74            2.79       2.75       0.623            -0.0455
##  9    7.74  8.81            2.58       2.63       1.07              0.0556
## 10    7.81  8.80            3.06       2.58       0.995            -0.487 
## # … with 3,990 more rows, and 1 more variable: Effect Size &lt;dbl&gt;
```

---


```r
post.h7 %&gt;% 
  gather() %&gt;% 
  ggplot(aes(y = key, x = value)) +
  stat_halfeye()
```

![](regression-3_files/figure-html/unnamed-chunk-70-1.png)&lt;!-- --&gt;

---
## Robust metric variables


```r
h.8 &lt;- 
  brm(family = student,
      health ~ 1 + happy_c,
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(cauchy(0, 10), class = sigma)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.8")
```


---


```r
summary(h.8)
```

```
##  Family: student 
##   Links: mu = identity; sigma = identity; nu = identity 
## Formula: health ~ 1 + happy_c 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     8.50      0.20     8.13     8.91 1.01      953      828
## happy_c       0.34      0.13     0.09     0.60 1.01     1218      744
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.72      0.15     2.43     3.02 1.00     1037      706
## nu       29.86     15.49     9.40    68.08 1.00      965      745
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

```r
pp_check(h.8)
```

```
## Using 10 posterior draws for ppc type 'dens_overlay' by default.
```

![](regression-3_files/figure-html/unnamed-chunk-73-1.png)&lt;!-- --&gt;


---


```r
summary(h.1)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: health ~ 1 + happy_c 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     8.51      0.20     8.11     8.88 1.00      961      672
## happy_c       0.33      0.13     0.08     0.59 1.00      968      670
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     2.81      0.14     2.56     3.09 1.00      827      722
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---
## Correlation 
multivariate model preview


```r
h.9 &lt;- 
  brm(family = gaussian,
      mvbind(health, happy) ~ 1 ,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = sigma, resp = health),
                prior(normal(0, 10), class = sigma, resp = happy),
                prior(lkj(1), class = rescor)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.9")
```

---

```r
summary(h.9)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: health ~ 1 
##          happy ~ 1 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## health_Intercept     8.51      0.20     8.11     8.88 1.00     1358      816
## happy_Intercept      2.62      0.11     2.40     2.83 1.00     1297      774
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_health     2.85      0.14     2.59     3.13 1.01     1251      699
## sigma_happy      1.51      0.08     1.37     1.67 1.01     1035      658
## 
## Residual Correlations: 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## rescor(health,happy)     0.18      0.07     0.04     0.30 1.00     1063
##                      Tail_ESS
## rescor(health,happy)      798
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---



```r
library(psych)
week3 %&gt;% 
  select(health,happy) %&gt;% 
  describe()
```

```
##        vars   n mean   sd median trimmed  mad  min   max range  skew kurtosis
## health    1 200 8.51 2.83   8.72    8.54 2.88 0.71 16.51 15.80 -0.08    -0.32
## happy     2 200 2.62 1.50   2.73    2.65 1.96 0.04  5.00  4.95 -0.11    -1.25
##          se
## health 0.20
## happy  0.11
```





```r
week3 %&gt;% 
  select(health,happy) %&gt;% 
  cor()
```

```
##           health     happy
## health 1.0000000 0.1787622
## happy  0.1787622 1.0000000
```


---

```r
plot(h.9)
```

![](regression-3_files/figure-html/unnamed-chunk-79-1.png)&lt;!-- --&gt;


---
## Robust correlations

```r
h.9a &lt;- 
  brm(family = student,
      mvbind(health, happy) ~ 1 ,
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = sigma, resp = health),
                prior(normal(0, 10), class = sigma, resp = happy),
                prior(lkj(1), class = rescor)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.9a")
```


---


```
##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: health ~ 1 
##          happy ~ 1 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## health_Intercept     8.52      0.20     8.16     8.91 1.00     1149      739
## happy_Intercept      2.62      0.11     2.41     2.82 1.00     1350      786
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_health     2.79      0.15     2.51     3.12 1.00     1175      767
## sigma_happy      1.50      0.07     1.35     1.65 1.00     1607      788
## nu              42.35     16.65    18.54    85.39 1.00      958      866
## nu_health        1.00      0.00     1.00     1.00   NA       NA       NA
## nu_happy         1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Residual Correlations: 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## rescor(health,happy)     0.18      0.07     0.04     0.31 1.00     1259
##                      Tail_ESS
## rescor(health,happy)      693
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```


---


```r
bf_health &lt;- bf(health ~ 1)
bf_happy &lt;- bf(happy ~ 1)


h.9b &lt;- 
  brm(family = student,
      bf_health + bf_happy,
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = sigma, resp = health),
                prior(normal(0, 10), class = sigma, resp = happy),
                prior(lkj(1), class = rescor)),
      data = week3,
      iter = 1000, warmup = 500, chains = 2, cores = 2, 
      file = "h.9b")
```


---


```
##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: health ~ 1 
##          happy ~ 1 
##    Data: week3 (Number of observations: 200) 
##   Draws: 2 chains, each with iter = 1000; warmup = 500; thin = 1;
##          total post-warmup draws = 1000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## health_Intercept     8.53      0.20     8.15     8.90 1.00     1511      771
## happy_Intercept      2.62      0.10     2.42     2.82 1.00     1554      900
## 
## Family Specific Parameters: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_health     2.80      0.15     2.54     3.10 1.01     1127      689
## sigma_happy      1.50      0.08     1.36     1.68 1.01     1574      893
## nu              42.06     16.05    18.36    77.33 1.00     1372      796
## nu_health        1.00      0.00     1.00     1.00   NA       NA       NA
## nu_happy         1.00      0.00     1.00     1.00   NA       NA       NA
## 
## Residual Correlations: 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## rescor(health,happy)     0.18      0.07     0.06     0.31 1.00     1444
##                      Tail_ESS
## rescor(health,happy)      773
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

![](regression-3_files/figure-html/unnamed-chunk-84-1.png)&lt;!-- --&gt;![](regression-3_files/figure-html/unnamed-chunk-84-2.png)&lt;!-- --&gt;


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
