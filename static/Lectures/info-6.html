<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Information 2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <script src="info-6_files/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small{ 
  font-size: 80%}
.tiny{
  font-size: 65%}
&lt;/style&gt;



## This time: 

Model and parameter evaluation. 
How do we test our coefficients? 
How do we choose the best model?  
What sort of indexes can we use to evaluate fit?  
  




---
## NHST

Bayesian typically do not use "standard" NHST 

Why?
1. There is no null and thus no p-values
2. NHST is associated with tons of problems eg dichotomous thinking
3. Because we are interested in the posterior distribution, not a single point estimate
4. Model comparison is more elegant



---
## Standard fit indicies and tests

.pull-left[
`\(R^2\)` 
AIC
BIC
Likelihood ratio test (LRT)



```r
mr.10 &lt;- readRDS("mr.10.rds")

bayes_R2(mr.10, summary = F) %&gt;% 
  data.frame() %&gt;% 
  ggplot(aes(x = R2)) +
  geom_density() +
  scale_y_continuous(NULL, breaks = NULL) 
```
]

.pull-right[
![](info-6_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

]

---
## What is likelihood?

.pull-left[
Model fit indexes like likelihood ratio test, AIC, BIC rely on likelihoods.

The distribution of the likelihood of various hypothesis. p(data | `\(\theta\)` ). For likelihood, the data are treated as a given, and value of theta varies. Probability of the data that you got, assuming a particular theta is true.

]

.pull-right[
binomial ~ `\(p(3|10,p)\)` ?
![](info-6_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

]

---
## Log likelihood
.pull-left[
The log of the likelihood is easier to work with (adding rather than multiplying small values). It will always be negative, with higher values (closer to zero) indicating a better fitting model. In frequentist estimation, the log likelihood is a single number, one that indicates the maximum, thus maximum likelihood estimation.
 
]

.pull-right[
Revisiting grid approximation from week 2, we hand calculated a regression likelihood.

```r
library(psychTools)
galton.data &lt;- galton
grid &lt;-
  crossing(mu = seq(from = 66, to = 69, length.out = 200), sigma = seq(from = 2, to = 3, length.out = 200))
grid
```

```
## # A tibble: 40,000 × 2
##       mu sigma
##    &lt;dbl&gt; &lt;dbl&gt;
##  1    66  2   
##  2    66  2.01
##  3    66  2.01
##  4    66  2.02
##  5    66  2.02
##  6    66  2.03
##  7    66  2.03
##  8    66  2.04
##  9    66  2.04
## 10    66  2.05
## # … with 39,990 more rows
```
]

---

.pull-left[
1. After defining the grid space we need to calculate the likelihood (are these numbers likely or not, given some distribution)

2. Likelihood of our data (each child's height) assuming each part of our grid is true. So if we take the first line of our grid (mu = 66, sigma = 2) we can see how likely each child (all 928) by dnorm(height, 66, 2) 
]

.pull-right[

```r
library(purrr)
grid_function &lt;- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %&gt;%
    sum() 
  }

p_grid &lt;-
  grid %&gt;% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;%
  unnest(log_likelihood)
  p_grid
```


]


---

We averaged across all 928 participants for each spot in the grid to get the average log-likelihood for each grid spot 


```
## # A tibble: 40,000 × 3
##       mu sigma log_likelihood
##    &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;
##  1    66  2            -2737.
##  2    66  2.01         -2733.
##  3    66  2.01         -2729.
##  4    66  2.02         -2725.
##  5    66  2.02         -2721.
##  6    66  2.03         -2718.
##  7    66  2.03         -2714.
##  8    66  2.04         -2710.
##  9    66  2.04         -2707.
## 10    66  2.05         -2703.
## # … with 39,990 more rows
```

---
.pull-left[

```
## Loading required package: viridisLite
```

![](info-6_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

.pull-right[
![](info-6_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---
## LRT

The likelihood ratio test compares the likelihood ratios of two models (usually likelihood evaluated at the MLE/MAP and at the null). 

If we multiply the difference in log-likelihood by -2, it follows a `\(\chi^2\)` distribution with 1 degrees of freedom. 

`\(LR = -2 ln\left(\frac{L(m_1)}{L(m_2)}\right) = 2(loglik(m_2)-loglik(m_1))\)`


---
## Information theory
Ideally we want to learn something new, gain some additional *information* about the world. But how do you quantify learning or information? Not dichotomous decision rules, not variance explained which can be gamed via overfitting, nor by amassing more data if most of it is just noise, nor by picking the most obvious outcome. 

First step is to define what it means to be accurate in our predictions--cannot learn if we don't have some criterion. We above defined accuracy as the log probability of the data.

Second step is to define what a perfect prediction would look like, and thus how much *uncertainty* we are dealing with. While the log probability of the data defines what parameter is most "accurate", it does not tell us how much we are learning. 

---
## Information theory

The reduction uncertainty by learning the outcome is how much *information* we have learned. If we know how much uncertainty we have we can know how much improvement is possible in our prediction. 

"Information is the resolution of uncertainty" - C.S.

Claude Shannon (1916-2001) developed the field of information theory by making an analogy to bits of information, just like within computers. Prior to this, information, and thus learning was poorly defined. Information theory is responsible for many computer/AI advances like jpeg and facial recognition. 

---
## What is information? 
The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. Unlikely = informative. 

information(x) = -log2( p(x) ), where log() is the base-2 logarithm and p(x) is the probability of the event x. Units are bits, where 1 bit halves the possibilities. 


```r
## coin flip
 -log2(.5)
```

```
## [1] 1
```

```r
# dice roll
 -log2(1/6)
```

```
## [1] 2.584963
```

---
# bits

Bits of information are the minimum number of questions required to determine an unknown. Bits  effectively chops down the possible outcomes by a factor of 2^bit. So 2 bits means you have a 1/4 of what you started out with. 

1/2^bit = p 
2^bit = 1/p
bit = log2(1/p)
bit = -1log2(p)

We use bits rather than direct probabilities because it is easier to add bits than very small probabilities multiplied by other very small probabilities. 


---

Calculating the information for a random variable is the same as calculating the information for the probability distribution of the events for the random variable. This is called entropy.

The intuition for entropy is that it is the average number of bits required to represent an event drawn from the probability distribution for the random variable. That is the expected value of a distribution. 

Flatter distributions will have higher entropy. Because all values likely any one value is "surprising" and gives you more information compared to a peaked distribution where knowing a value will give you less new information. Distributions with more possibilities will also be higher in entropy than those with fewer possibilities. 

This is opposite of how we set up priors were if we had more "information" we would create a more narrow distribution. e^-0.265

---
## Maximum information entropy

We can use information theory to help us pick our likelihood distributions. Given what you know, what is the least surprising distribution ie that can arise the most ways with our data? Maximizing information entropy yields the flattest distribution given your data constraints. Gaussian is a maximum entropy distribution (as are uniform, binomial, exponential). 

---
## Information entropy

Information entropy, H(p), tells us how hard it is to make an accurate prediction. 

H(p) = `\(-\sum p_ilog(p_i)\)`

Think of tossing rocks into 10 buckets. Each bucket can have a probability associated with getting a rock. H(p) is maximized when ps are equal. This gives us the least surprising distribution ie that can arise the most ways with our data

---

But how far away is our model from an accurate prediction? Divergence (KL) is the uncertainty created by using probabilities from one distribution to describe another distribution. 

`\(D(m1, m2) = -\sum p_i(log(p_i) - log(q_i))\)`

The difference between two entropies... or average difference in log probability between model 1 (target) and model 2 (actual)  

We never know the target but that is okay. We can still compute divergences and compare with other divergences because the target is a constant. This means we cannot look at a divergence and know if it is good or bad, but we can use them to compare! The result is called a deviance. 

D(q) = -2 `\(\sum_i\)` `\(\log\)` (q_i)

---
## How is this different from LRT? 

Deviance is a model of relative fit, but with a constant specific to the model. Log likelihoods thus only differ by a constant. Comparing two models with LRT should yield similar scores as comparing two deviances.  

But Bayesians don't like NHST, so they sometimes leave off the -2, which simplifies the deviance equation but then does not allow a chi-square difference test to be completed

D(q) = `\(\sum_i\)` `\(\log\)` (q_i),

Further, with Bayes we need to use the entire posterior to define deviance! If not, we are throwing away information. Often referred to Log Pointwise Predictive Density (lpd or lppd)

---
## Two options that use information

1. Cross validation
2. Information indexes

---
## Cross validation
Compare different models in their prediction in and out of sample (train and a test sample)  

Predictions to NEW DATA are key, as this separates the sample specific influence (irregular features) from population influence (regular features)  

Identify the model with the lowest test set error (deviance). In frequentist we typically use MSE.  

But leaving out data is not necessary ideal, so we split them into folds. 

---
## LOO &amp; PSIS-LOO

Leaving out 1 observation per fold results in Leave One Out cross validation. 

Great in theory, but computationally difficult. Pareto Smoothed Importance Sampling (PSIS). Importance  is similar to "deleted residuals" where on a case by case basis we asked whether this data point impacts the posterior. Instead of rerunning your model N  data points times, importance scores are used to weight each datapoint, resulting in an equivalent to LOO without actually doing LOO. 

---
## PSIS-LOO example

.pull-left[

```r
mr.12 &lt;- readRDS("mr.12.rds")
mr.12 &lt;- add_criterion(mr.12, "loo")
mr.12$criteria$loo
```

```
## 
## Computed from 4000 by 118 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -224.6  7.3
## p_loo         5.1  0.8
## looic       449.1 14.5
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help('pareto-k-diagnostic') for details.
```
]

.pull-right[

elpd theoretical expected log pointwise predictive density for a new dataset (an lppd equivalent). Larger the better. 

p_loo is effective number of parameters

looic is -2(elpd). Low scores better. 

]

---


```r
mr.11 &lt;- readRDS("mr.11.rds")
mr.11 &lt;- add_criterion(mr.11, "loo")
loo(mr.11)
```

```
## 
## Computed from 4000 by 118 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -227.9  6.8
## p_loo         3.9  0.5
## looic       455.9 13.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help('pareto-k-diagnostic') for details.
```


```r
loo_compare(mr.11, mr.12, criterion = "loo")
```

```
##       elpd_diff se_diff
## mr.12  0.0       0.0   
## mr.11 -3.4       3.4
```
recommend the elpd_diff &gt; 4(se_diff). While the interaction fits better (mr.12), it is negligibly so. 

---
## Information Criteria

AIC, BIC, WAIC, DIC, etc. Information criteria are a theoretical fit for out of sample cross validation, not unlike psis-loo

AIC = deviance(training) + 2p (p = number of parameters). Low scores better. 

DIC (Deviance information criteria) is more flexible for Bayesian models, but assumes a multivariate Gaussian posterior

---
## WAIC

`$$\text{WAIC}(y, \theta) = -2 \big (\text{lppd} - \underbrace{\sum_i \operatorname{var}_\theta \log p(y_i | \theta)}_\text{penalty term} \big)$$`

lppd = log pointwise predictive density = deviance over the entire posterior = 

`$$\text{lppd}(y, \theta) = \sum_i \log \frac{1}{S} \sum_sp(y_i|\theta_S)$$`

`$$\text{AIC}(y, \theta) = -2(\text{llpd}) + 2p$$`


---
## example


```r
waic(mr.12)
```

```
## 
## Computed from 4000 by 118 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -224.5  7.3
## p_waic         5.1  0.8
## waic         449.1 14.5
```

---
### elpd_waic, llpd, p_waic 

pwaic = penality term (similar to the penality term in AIC)

elpd_waic = lppd - pwaic


---


```r
waic(mr.11)
```

```
## 
## Computed from 4000 by 118 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -227.9  6.8
## p_waic         3.8  0.5
## waic         455.9 13.6
```


```r
w.11&lt;-waic(mr.11)
w.11
```

```
## 
## Computed from 4000 by 118 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -227.9  6.8
## p_waic         3.8  0.5
## waic         455.9 13.6
```

---



```r
mr.11 &lt;- add_criterion(mr.11, "waic")
mr.12 &lt;- add_criterion(mr.12, "waic")
loo_compare(mr.11, mr.12, criterion = "waic")
```

```
##       elpd_diff se_diff
## mr.12  0.0       0.0   
## mr.11 -3.4       3.4
```
Note: looks similar to loo results, even though this is with waic. Both are computed from the llpd, so it makes sense they can be similar. It will not always be this way, however. 

---

## Using information critera

Each of these criteria do not have scales that are bounded by numbers (eg 0-1), nor can they be evaluated by some other number (eg SDs)

Provide relative fit, so you need to compare different models, choosing the model with the lowest IC or the highest expected IC

The criteria are also dependent on sample size, so you cannot compare across models that differ in sample size

You can compare non-nested models eg ones with different sets of predictors

---
## Overfitting

- Fit is relative to our sample, not the population 
- Need to balance between parsimony and completeness
- Ironically, the best fitting model may not be the *best* model. The model will be tuned to our particular random sample
- We are "fitting the noise" or overfitting the specifics of our sample

---
## Regularization

- "penalizing" our model estimates to prevent overfitting
- find coefficients that compromise between (a) minimizing the SS
and (b) minimizing sum of abs value of coefficients
- Tends to "shrink"" coefficients to zero
- Shrinkage/penalization is based on lambda

---
## Mimicking NHST

If you wanted to appease an editor/reviewer/adviser what do you do (other than teach them Bayes)?

CIs! HDPIs! Predictions! Probability of direction!


```r
mr.10 &lt;- readRDS("mr.10.rds")

mr.10 %&gt;%
  spread_draws(r_iv[group,]) %&gt;% 
    mean_qi()
```

```
## Warning: `gather_()` was deprecated in tidyr 1.2.0.
## Please use `gather()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
```

```
## # A tibble: 3 × 7
##   group    r_iv .lower .upper .width .point .interval
##   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1     1 -0.113  -0.954  0.521   0.95 mean   qi       
## 2     2  0.0359 -0.663  0.807   0.95 mean   qi       
## 3     3  0.0782 -0.609  0.847   0.95 mean   qi
```


---


```r
mr.2 &lt;- readRDS("mr.2.rds")
```


```r
mr.2 &lt;- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.2")
```

---

```r
summary(mr.2)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: happiness ~ 1 + SS_c + FQ_c 
##    Data: MR (Number of observations: 118) 
##   Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 2000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.21      0.14     4.92     5.50 1.00     2207     1478
## SS_c          0.35      0.05     0.25     0.44 1.00     1809     1338
## FQ_c          0.18      0.07     0.05     0.30 1.00     1873     1513
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.60      0.11     1.40     1.81 1.00     1946     1457
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---
## update prior


```r
mr.2p &lt;- update(mr.2, prior =  
                c(prior(normal(5, 2), class = Intercept),
                 prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .05), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
                  file = "mr.2p")
```

```
## The desired updates require recompiling the model
```


---


```r
summary(mr.2p)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: happiness ~ 1 + SS_c + FQ_c 
##    Data: MR (Number of observations: 118) 
##   Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 2000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     5.22      0.15     4.92     5.51 1.00     1990     1033
## SS_c          0.30      0.05     0.21     0.40 1.00     1759     1589
## FQ_c          0.06      0.04    -0.02     0.15 1.00     1875     1417
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.62      0.11     1.43     1.86 1.00     1781     1504
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

---

.pull-left[

```r
priors &lt;-
  c(prior(normal(0, .2), class = b, coef = FQ_c),
    prior(normal(0, .05), class = b, coef = FQ_c))

priors %&gt;% 
  parse_dist(prior) %&gt;% 
  ggplot(aes(y=class, dist =.dist, args=.args,  fill = prior)) +
  stat_dist_halfeye(alpha = .6)
```
]



.pull-right[
![](info-6_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

---
## Bayes Factors

Quantifies the support in the data for two competing statistical models. Ratio of the two marginal likelihoods of an observed outcome for these two models. E.g., how likely is a b = .2 vs b = 0, given our data. Gives relative evidence for different positions by comparing the predictive accuracy of two model-prior combinations

`$$\text{BF} = \frac{p(D | m = 1)}{p(D | m = 2)}$$`

---


```
## Warning: Bayes factors might not be precise.
## For precise Bayes factors, sampling at least 40,000 posterior samples is recommended.
```

![](info-6_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;

---
### BF interpretation 
Extent to which the data sway our relative belief from one hypothesis to the other  

Strength of evidence from data about the hypotheses

Relative predictive accuracy of one hypothesis over another


---

`$$\frac{p(m = 1 | D)}{p(m = 2 | D)} = \left (\frac{p(D | m = 1)}{p(D | m = 2)} \right ) \left ( \frac{p(m = 1)}{p(m = 2)} \right)$$`
This equation shows that the posterior odds is determined entirely by the ratio of the marginal likelihoods (Bayes Factors) multiplied by the prior odds

---

```r
mr.2 &lt;- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.2")
```

---
## BF with {brms}


```r
bf.result &lt;- bayestestR::bayesfactor_parameters(mr.2)
```

```
## Sampling priors, please wait...
```

```
## Warning: Bayes factors might not be precise.
## For precise Bayes factors, sampling at least 40,000 posterior samples is recommended.
```

```r
bf.result
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter   |       BF
## ----------------------
## (Intercept) | 2.36e+29
## SS_c        | 1.40e+05
## FQ_c        |    14.41
## 
## * Evidence Against The Null: 0
```


---

```r
plot(bf.result)
```

![](info-6_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;


---


```r
bf.result2 &lt;- bayesfactor_parameters(mr.2, null = rope_range(mr.2))
```

```
## Sampling priors, please wait...
```

```r
bf.result2 
```

```
## Bayes Factor (Null-Interval)
## 
## Parameter   |       BF
## ----------------------
## (Intercept) | 1.18e+29
## SS_c        | 1.14e+03
## FQ_c        |     1.72
## 
## * Evidence Against The Null: [-0.189, 0.189]
```

---
.pull-left[
The Bayes factor represents the degree by which the distribution mass of the posterior has shifted outside or inside the null interval relative to the prior distribution
]

.pull-right[

```r
plot(bf.result2)
```

![](info-6_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;

]

---
### using hypothesis
divides the posterior by the prior (1/evidence to flip)


```r
hypothesis(mr.2, "FQ_c = 0")
```

```
## Hypothesis Tests for class b:
##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star
## 1 (FQ_c) = 0     0.18      0.07     0.05      0.3       0.05      0.05    *
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```


```r
hypothesis(mr.2, "FQ_c &gt; 0")
```

```
## Hypothesis Tests for class b:
##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star
## 1 (FQ_c) &gt; 0     0.18      0.07     0.07     0.29     221.22         1    *
## ---
## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
## '*': For one-sided hypotheses, the posterior probability exceeds 95%;
## for two-sided hypotheses, the value tested against lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.
```

---


```r
plot(hypothesis(mr.2, "FQ_c = 0"))
```

![](info-6_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;

---
###Model comparisons
via {bridgesampling and brms}

```r
bayes_factor(mr.12, mr.11)
```


---
###Model comparisons
via {bayestestR}

```r
comparison &lt;- bayesfactor_models(mr.12, denominator = mr.11)
comparison
```

---
### group comparison
caution for factors with more than 2 levels 


```r
library(emmeans)
groups &lt;- emmeans(mr.11, ~ iv.d)
group_diff &lt;- pairs(groups)
groups_all &lt;- rbind(groups, group_diff)
bayesfactor_parameters(groups_all, prior = mr.11)
```

---
### Proposed cutoffs
.small[
| BF10         | Interpretation               |
|--------------|------------------------------|
| &gt; 100        | Extreme evidence for H1      |
| 30 - 100     | Very strong evidence for H1  |
| 10 - 30      | Strong evidence for H1       |
| 3 - 10       | Moderate evidence for H1     |
| 1 - 3        | Anecdotal evidence for H1    |
| 1            | Equal evidence for H1 and H0 |
| 1/3 - 1      | Anecdotal evidence for H0    |
| 1/10 - 1/3   | Moderate evidence for H0     |
| 1/30 - 1/10  | Strong evidence for H0       |
| 1/100 - 1/30 | Very strong evidence for H0  |
| &lt; 1/100      | Extreme evidence for H0      |
]

---
## Shortcomings of Bayes Factors

.small[
1. Heavy reliance on priors. The marginal likelihood is an average taken with respect to the prior, so bayes factors can be seen as relatively subjective compared to alternative approaches. This is less so a problem for models with more straightforward "nulls" eg a correct/incorrect and less so for more continuous metrics

2. BF provide differences between models without quantifying whether the chosen model is any good (similar issues with waic, loo, p-values -- you can calculate BF with bic!)

3. Favors more parsimonious models and thus is more conservative (could also be a pro) 

4. Potentially reinforces dichotomous thinking 

5. May be used as a measure of effect size (similar shortfalls to using p as such)

6. Null is mostly not useful (!)
]

---



```r
mr.1 &lt;- readRDS("mr.1.rds")
mr.1 &lt;- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, 2), class = b, coef = SS_c),
                prior(normal(0, 2), class = b, coef = FQ_c),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.1")
```


```r
mr.2 &lt;- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.2")
```

---
.pull-left[

```r
bf.result &lt;- bayestestR::bayesfactor_parameters(mr.1)
```

```
## Sampling priors, please wait...
```

```
## Warning: Bayes factors might not be precise.
## For precise Bayes factors, sampling at least 40,000 posterior samples is recommended.
```

```r
bf.result
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter   |       BF
## ----------------------
## (Intercept) | 5.28e+34
## SS_c        | 1.40e+04
## FQ_c        |     2.70
## 
## * Evidence Against The Null: 0
```

]

.pull-right[

```r
bf.result &lt;- bayestestR::bayesfactor_parameters(mr.2)
```

```
## Sampling priors, please wait...
```

```
## Warning: Bayes factors might not be precise.
## For precise Bayes factors, sampling at least 40,000 posterior samples is recommended.
```

```r
bf.result
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter   |       BF
## ----------------------
## (Intercept) | 2.40e+29
## SS_c        | 1.29e+05
## FQ_c        |    14.66
## 
## * Evidence Against The Null: 0
```
]
 
---
## ROPE
Region of Practical Equivalence - a small range of parameter values that are considered to be practically equivalent to the null. E.g,. std b = [-.07, .07]

Compare our 95% (or whatever value you want) CI/HDPI to see if it overlaps with the ROPE. Reject null if it doesn't. 

Note: not all values within the ROPE are rejected, just the null. 


---


```r
bayestestR::rope(mr.2, range = c(-0.07, 0.07))
```

```
## # Proportion of samples inside the ROPE [-0.07, 0.07]:
## 
## Parameter | inside ROPE
## -----------------------
## Intercept |      0.00 %
## SS_c      |      0.00 %
## FQ_c      |      3.42 %
```

---


```r
rope &lt;- rope(mr.2, range = c(-0.07, 0.07), ci = c(0.93))
plot(rope)
```

![](info-6_files/figure-html/unnamed-chunk-44-1.png)&lt;!-- --&gt;


---


```r
result &lt;- equivalence_test(mr.2)
result
```

```
## # Test for Practical Equivalence
## 
##   ROPE: [-0.19 0.19]
## 
## Parameter |        H0 | inside ROPE |     95% HDI
## -------------------------------------------------
## Intercept |  Rejected |      0.00 % | [4.95 5.52]
## SS_c      |  Rejected |      0.00 % | [0.25 0.44]
## FQ_c      | Undecided |     57.08 % | [0.05 0.30]
```


---
## ROPE

Why not just use the CI like you normally would? Because what if CI and ROPE overlap? 

CI completely contains ROPE vs partially contains

This allows you to 1. have a middle ground between accept vs reject. A range of equivalence. 

2. Allows one to affirm a predicted value, which is logically impossible in a standard NHST framework

---
## ROPE

How to come up with ROPE values? 

Difficult. Balance practicality with expertise judgment. DBDA: -0.1 to 0.1 of a standardized parameter (ie negligible effect size, Cohen, 1988). 


```r
rope_range(mr.2)
```

```
## [1] -0.1893523  0.1893523
```


Usually implemented when a decision needs to be made. More common in medicine than psych. 








    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
