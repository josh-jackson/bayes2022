---
title: "Information 2"
author: Josh Jackson
date: "2-27-22"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)

library(tidyverse)
library(broom)
```

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small{ 
  font-size: 80%}
.tiny{
  font-size: 65%}
</style>



## This time: 

Model and parameter evaluation. 
How do we test our coefficients? 
How do we choose the best model?  
What sort of indexes can we use to evaluate fit?  
  

```{r, echo = FALSE, message=FALSE}
library(tidybayes)
library(tidyverse)
library(brms)
```


---
## NHST

Bayesian typically do not use "standard" NHST 

Why?
1. There is no null and thus no p-values
2. NHST is associated with tons of problems eg dichotomous thinking
3. Because we are interested in the posterior distribution, not a single point estimate
4. Model comparison is more elegant



---
## Standard fit indicies and tests

.pull-left[
$R^2$ 
AIC
BIC
Likelihood ratio test (LRT)


```{r, eval = FALSE}
mr.10 <- readRDS("mr.10.rds")

bayes_R2(mr.10, summary = F) %>% 
  data.frame() %>% 
  ggplot(aes(x = R2)) +
  geom_density() +
  scale_y_continuous(NULL, breaks = NULL) 
```
]

.pull-right[
```{r, echo = FALSE}
mr.10 <- readRDS("mr.10.rds")
bayes_R2(mr.10, summary = F) %>% 
  data.frame() %>% 
  ggplot(aes(x = R2)) +
  geom_density() +
  scale_y_continuous(NULL, breaks = NULL) 
```

]

---
## What is likelihood?

.pull-left[
Model fit indexes like likelihood ratio test, AIC, BIC rely on likelihoods.

The distribution of the likelihood of various hypothesis. p(data | $\theta$ ). For likelihood, the data are treated as a given, and value of theta varies. Probability of the data that you got, assuming a particular theta is true.

]

.pull-right[
binomial ~ $p(3|10,p)$ ?
```{r, echo = FALSE, message = FALSE}
library(tidyverse)
ggplot(tibble(x = c(0, 1)), 
       aes(x = x)) + 
  stat_function(fun = dbinom, args = list(x = 3, size = 10)) + 
  labs(x = expression(theta), 
       y = "likelihood")
```

]

---
## Log likelihood
.pull-left[
The log of the likelihood is easier to work with (adding rather than multiplying small values). It will always be negative, with higher values (closer to zero) indicating a better fitting model. In frequentist estimation, the log likelihood is a single number, one that indicates the maximum, thus maximum likelihood estimation.
 
]

.pull-right[
Revisiting grid approximation from week 2, we hand calculated a regression likelihood.
```{r}
library(psychTools)
galton.data <- galton
grid <-
  crossing(mu = seq(from = 66, to = 69, length.out = 200), sigma = seq(from = 2, to = 3, length.out = 200))
grid
```
]

---

.pull-left[
1. After defining the grid space we need to calculate the likelihood (are these numbers likely or not, given some distribution)

2. Likelihood of our data (each child's height) assuming each part of our grid is true. So if we take the first line of our grid (mu = 66, sigma = 2) we can see how likely each child (all 928) by dnorm(height, 66, 2) 
]

.pull-right[
```{r, eval=FALSE}
library(purrr)
grid_function <- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %>%
    sum() 
  }

p_grid <-
  grid %>% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %>%
  unnest(log_likelihood)
  p_grid
```


]


---

We averaged across all 928 participants for each spot in the grid to get the average log-likelihood for each grid spot 

```{r, echo=FALSE}
library(purrr)
grid_function <- function(mu, sigma) {
  dnorm(galton.data$child, mean = mu, sd = sigma, log = T) %>%
    sum() 
  }

p_grid <-
  grid %>% 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) %>%
  unnest(log_likelihood)
  p_grid
```

---
.pull-left[
```{r, echo = FALSE}
library(viridis)
p_grid %>% 
  ggplot(aes(x = mu, y = sigma, fill = log_likelihood)) + 
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "C") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```
]

.pull-right[
```{r, echo = FALSE}

p_grid %>% 
  ggplot(aes(x = mu, y = log_likelihood )) + 
   geom_density_2d_filled() 

```
]

---
## LRT

The likelihood ratio test compares the likelihood ratios of two models (usually likelihood evaluated at the MLE/MAP and at the null). 

If we multiply the difference in log-likelihood by -2, it follows a $\chi^2$ distribution with 1 degrees of freedom. 

$LR = -2 ln\left(\frac{L(m_1)}{L(m_2)}\right) = 2(loglik(m_2)-loglik(m_1))$


---
## Information theory
Ideally we want to learn something new, gain some additional *information* about the world. But how do you quantify learning or information? Not dichotomous decision rules, not variance explained which can be gamed via overfitting, nor by amassing more data if most of it is just noise, nor by picking the most obvious outcome. 

First step is to define what it means to be accurate in our predictions--cannot learn if we don't have some criterion. We above defined accuracy as the log probability of the data.

Second step is to define what a perfect prediction would look like, and thus how much *uncertainty* we are dealing with. While the log probability of the data defines what parameter is most "accurate", it does not tell us how much we are learning. 

---
## Information theory

The reduction uncertainty by learning the outcome is how much *information* we have learned. If we know how much uncertainty we have we can know how much improvement is possible in our prediction. 

"Information is the resolution of uncertainty" - C.S.

Claude Shannon (1916-2001) developed the field of information theory by making an analogy to bits of information, just like within computers. Prior to this, information, and thus learning was poorly defined. Information theory is responsible for many computer/AI advances like jpeg and facial recognition. 

---
## What is information? 
The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. Unlikely = informative. 

information(x) = -log2( p(x) ), where log() is the base-2 logarithm and p(x) is the probability of the event x. Units are bits, where 1 bit halves the possibilities. 

```{r}
## coin flip
 -log2(.5)
```
```{r}
# dice roll
 -log2(1/6)
```

---
# bits

Bits of information are the minimum number of questions required to determine an unknown. Bits  effectively chops down the possible outcomes by a factor of 2^bit. So 2 bits means you have a 1/4 of what you started out with. 

1/2^bit = p 
2^bit = 1/p
bit = log2(1/p)
bit = -1log2(p)

We use bits rather than direct probabilities because it is easier to add bits than very small probabilities multiplied by other very small probabilities. 


---

Calculating the information for a random variable is the same as calculating the information for the probability distribution of the events for the random variable. This is called entropy.

Entropy is the average number of bits required to represent an event drawn from the probability distribution for the random variable. That is the expected value of a distribution. 

Flatter distributions will have higher entropy. Because all values likely any one value is "surprising" and gives you more information compared to a peaked distribution where knowing a value will give you less new information. Distributions with more possibilities will also be higher in entropy than those with fewer possibilities. 

This is opposite of how we set up priors were if we had more "information" we would create a more narrow distribution. 

---
## Maximum information entropy

We can use information theory to help us pick our likelihood distributions. Given what you know, what is the least surprising distribution ie that can arise the most ways with our data? Maximizing information entropy yields the flattest distribution given your data constraints. Gaussian is a maximum entropy distribution (as are uniform, binomial, exponential). 

If nothing is known about a distribution (except measurement), then the distribution with the largest entropy should be chosen as the least-informative default. First, maximizing entropy minimizes the amount of prior information built into the distribution; Second, many physical systems tend to move towards maximal entropy configurations over time. 

---
## Information entropy

Information entropy, H(p), tells us how hard it is to make an accurate prediction. For non uniform distributions we must sum up the all the log probabilities

H(p) = $-\sum p_ilog(p_i)$

Think of tossing rocks into 10 buckets. Each bucket can have a probability associated with getting a rock. H(p) is maximized when ps are equal. This gives us the least surprising distribution ie that can arise the most ways with our data

---

But how far away is our model from an accurate prediction? Divergence (KL) is the uncertainty created by using probabilities from one distribution to describe another distribution. 

$D(m1, m2) = -\sum p_i(log(p_i) - log(q_i))$

The difference between two entropies... or average difference in log probability between model 1 (target) and model 2 (actual)  

We never know the target but that is okay. We can still compute divergences and compare with other divergences because the target is a constant. This means we cannot look at a divergence and know if it is good or bad, but we can use them to compare! The result is called a deviance. 

D(q) = -2 $\sum_i$ $\log$ (q_i)

---
## How is this different from LRT? 

Deviance is a model of relative fit, but with a constant specific to the model. Log likelihoods thus only differ by a constant. Comparing two models with LRT should yield similar scores as comparing two deviances.  

But Bayesians don't like NHST, so they sometimes leave off the -2, which simplifies the deviance equation but then does not allow a chi-square difference test to be completed. 

D(q) = $\sum_i$ $\log$ (q_i) = lppd

Further, with Bayes we need to use the entire posterior to define deviance! If not, we are throwing away information. Often referred to Log Pointwise Predictive Density (lpd or lppd)

---
## Two options that use information

1. Cross validation
2. Information model fit indexes

These are theoretically the same, but in practice seen as very different. That is, cross validation is equivalent to model fit like aic/bic

---
## Cross validation
Compare different models in their prediction in and out of sample (train and a test sample)  

Predictions to NEW DATA are key, as this separates the sample specific influence (irregular features) from population influence (regular features)  

Identify the model with the lowest test set error (deviance). In frequentist we typically use MSE.  

But leaving out data is not necessary ideal, so we split them into folds. 

---
## LOO & PSIS-LOO

Leaving out 1 observation per fold is called  Leave One Out (LOO) cross validation. 

Great in theory, but computationally difficult. Pareto Smoothed Importance Sampling (PSIS). Importance  is similar to "deleted residuals" where on a case by case basis we asked whether this data point impacts the posterior. Instead of rerunning your model N  data points times, importance scores are used to weight each datapoint, resulting in an equivalent to LOO without actually doing LOO. Cross validation for free! 

---
## PSIS-LOO example

.pull-left[
```{r}
mr.12 <- readRDS("mr.12.rds")
mr.12 <- add_criterion(mr.12, "loo")
mr.12$criteria$loo
```
]

.pull-right[

elpd theoretical expected log pointwise predictive density for a new dataset (an lppd equivalent). Larger the better. 

p_loo is effective number of parameters

looic is -2(elpd). Low scores better. 

]

---

```{r}
mr.11 <- readRDS("mr.11.rds")
mr.11 <- add_criterion(mr.11, "loo")
loo(mr.11)
```

```{r}
loo_compare(mr.11, mr.12, criterion = "loo")
```
recommend the elpd_diff > 4(se_diff). While the interaction fits better (mr.12), it is negligibly so. 

---
## Information Criteria

AIC, BIC, WAIC, DIC, etc. 

Information criteria are a theoretical fit for the out of sample cross validation, not unlike psis-loo

AIC = deviance(training) + 2p (p = number of parameters). Low scores better. 

DIC (Deviance information criteria) is more flexible for Bayesian models, but assumes a multivariate Gaussian posterior

---
## WAIC

$$\text{WAIC}(y, \theta) = -2 \big (\text{lppd} - \underbrace{\sum_i \operatorname{var}_\theta \log p(y_i | \theta)}_\text{penalty term} \big)$$

lppd = log pointwise predictive density = deviance over the entire posterior = 

$$\text{lppd}(y, \theta) = \sum_i \log \frac{1}{S} \sum_sp(y_i|\theta_S)$$

$$\text{AIC}(y, \theta) = -2(\text{llpd}) + 2p$$


---
## example

```{r}
waic(mr.12)
```

---
### elpd_waic, llpd, p_waic 

pwaic = penality term (similar to the penality term in AIC)

elpd_waic = lppd - pwaic


---

```{r}
waic(mr.11)
```

```{r}
w.11<-waic(mr.11)
w.11
```

---


```{r}
mr.11 <- add_criterion(mr.11, "waic")
mr.12 <- add_criterion(mr.12, "waic")
loo_compare(mr.11, mr.12, criterion = "waic")
```
Note: looks similar to loo results, even though this is with waic. Both are computed from the llpd, so it makes sense they can be similar. It will not always be this way, however. 

---

## Using information critera

Each of these criteria do not have scales that are bounded by numbers (eg 0-1), nor can they be evaluated by some other number (eg SDs)

Provide relative fit, so you need to compare different models, choosing the model with the lowest IC or the highest expected IC

The criteria are also dependent on sample size, so you cannot compare across models that differ in sample size

You can compare non-nested models eg ones with different sets of predictors

---
## Overfitting

- Fit is relative to our sample, not the population 
- Need to balance between parsimony and completeness
- Ironically, the best fitting model may not be the *best* model. The model will be tuned to our particular random sample
- We are "fitting the noise" or overfitting the specifics of our sample

---
## Regularization

- "penalizing" our model estimates to prevent overfitting
- find coefficients that compromise between (a) minimizing the SS
and (b) minimizing sum of abs value of coefficients
- Tends to "shrink"" coefficients to zero




---

```{r}
mr.2 <- readRDS("mr.2.rds")

```

```{r, eval = FALSE}
mr.2 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.2")
```

---
```{r}
summary(mr.2)
```

---
## update prior

```{r}

mr.2p <- update(mr.2, prior =  
                c(prior(normal(5, 2), class = Intercept),
                 prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .05), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
                  file = "mr.2p")
```


---

```{r}
summary(mr.2p)
```

---

.pull-left[
```{r, eval = FALSE}
priors <-
  c(prior(normal(0, .2), class = b, coef = FQ_c),
    prior(normal(0, .05), class = b, coef = FQ_c))

priors %>% 
  parse_dist(prior) %>% 
  ggplot(aes(y=class, dist =.dist, args=.args,  fill = prior)) +
  stat_dist_halfeye(alpha = .6)
```
]



.pull-right[
```{r, echo = FALSE}
priors <-
  c(prior(normal(0, .2), class = b, coef = FQ_c),
    prior(normal(0, .05), class = b, coef = FQ_c))

priors %>% 
  parse_dist(prior) %>% 
  ggplot(aes(y=class, dist =.dist, args=.args,  fill = prior)) +
  stat_dist_halfeye(alpha = .6)
```
]


---
## Mimicking NHST

If you wanted to appease an editor/reviewer/adviser what do you do (other than teach them Bayes)?

CIs! HDPIs! Predictions! Probability of direction!

```{r, eval=TRUE, message=FALSE, echo = FALSE}
mr.13 <- readRDS("mr.13.rds")
summary(mr.13)
```

---
### Can you describe? 

The difference between frequentist vs bayesian CIs? 
What happens when we choose an 90 or a 99% CI compared to a 95? 
The difference between CI/qi/ETI  vs HDPI/HPI?   
How are each (CI vs HDPI) of these computed?   
When would they (CI vs HDPI) be similar vs when would they be different?   

---

Given the observed data, the effect has 95% probability of falling within this range

---
## Probability of direction

The Probability of Direction (pd) is an index of effect existence representing the certainty with which an effect goes in a particular direction.

Simple: only depends on the posterior distributions (unlike say Bayes Factors) 
Scale independent: interpretation doesn't change with scale change
Similar to p - value: interpretation has parallels, so it helps convince pesky reviewers


---

```{r}
get_variables(mr.13)
mr.13 %>%
  gather_draws(b_SS_c, b_FQ_c, `b_SS_c:FQ_c`) %>%
  ggplot(aes(y = fct_rev(.variable), x = .value)) +
  stat_halfeye(.width = c(.90, .5))
```


---
## How to calculate?

Compute the area under the curve (AUC) of the density curve on the other side of 0.

```{r}
library(bayestestR)
p_direction(mr.13)
```

---
```{r}
describe_posterior(mr.13)
```


---
### Downsides

You can find a high PD for an effect that is quite small. Similar to a p < .05 with a large sample size. Is it still meaningful? 

Lead to other NHST foibles like dichotomous thinking

More similar to a 1-tailed p-value, so equivalence for .05 should really be 90% PD. 

```{r, eval = FALSE}
onesided_p <- 1 - pd / 100
twosided_p <- onesided_p * 2
```

---
## nil hypothesis    

With NHST you typically attempt to reject a H0 of zero. This is a point estimate. However, we know the probability of any point is zero, so conceptually a typical NHST is illogical. 

What we really mean when we set up a standard NHST nil hypothesis is to reject zero -- OR VALUES THAT ARE EQUIVALENT TO ZERO. 

What are those values? In frequentist it depends on your se which is a function of your sample SD and N. We just don't talk about it. 

---
# ROPE
Bayesian inference is not based on NHST. Rather than concluding that an effect is present when it simply differs from zero, we would conclude that the probability of being outside a specific range that can be considered as “practically no effect”. 

Region of Practical Equivalence - a small range of parameter values that are considered to be practically equivalent to the null. E.g,. std b = [-.07, .07]

Let the user define an area around the null value enclosing values that are equivalent to the null 

---

Compare our 95% (or whatever value you want) CI/HDI to see if it overlaps with the ROPE. Some suggest using full posterior rather than a CI. 

If this percentage is sufficiently low, the null hypothesis is rejected. If this percentage is sufficiently high, the null hypothesis is accepted.

Note: not all values within the ROPE are rejected, just the null. 

---

```{r}
rope.1 <- rope(mr.13, range = c(-0.05, 0.05))
rope.1
```
* Note how the rope is the same for all parameters

---
```{r}
plot(rope.1)
```


---

```{r}
```


```{r}
rope.2 <- rope(mr.13, range = c(-0.05, 0.05), ci = c(0.99))
plot(rope.2)
```



---

```{r}
result <- equivalence_test(mr.13, range = c(-0.05, 0.05), ci = c(0.95))
result
```

---
```{r}
result <- equivalence_test(mr.13, range = c(-0.1, 0.1), ci = c(0.99))
result
```

---

```{r}
rope.3 <- rope(mr.13, range = c(-0.1, 0.1), ci = c(0.99))
plot(rope.3)
```



---
## ROPE

Why not just use the CI like you normally would? Because what if CI and ROPE overlap like above for FQ_c?

This allows you to 1. have a middle ground between accept vs reject. A range of equivalence. 

2. Allows one to affirm a predicted value, which is logically impossible in a standard NHST framework. Ie rather than fail to reject null, we can state given the observed data, the effect has XX% probability of being practically zero. 


---
## ROPE

How to come up with ROPE values? 

Difficult. Balance practicality with expertise judgment. DBDA: -0.1 to 0.1 of a standardized parameter (ie negligible effect size, Cohen, 1988). Or [-1*se, +1*se] 

```{r}
rope_range(mr.13)
```

Usually implemented when a decision needs to be made. More common in medicine than psych. 

---
.pull-left[
```{r}

rope.4 <- rope(mr.13)
rope.4

```
]


.pull-right[

```{r}
plot(rope.4)
```

]

---
```{r}
describe_posterior(mr.13, test = "all")
```



---
## Bayes Factors

Quantifies the support in the data for two competing statistical models. Ratio of the two marginal likelihoods of an observed outcome for these two models. E.g., how likely is a b = .2 vs b = 0, given our data. Gives relative evidence for different positions by comparing two marginal likelihoods

$$BF_{12} = \frac{p(D | m = 1)}{p(D | m = 2)}$$

---

Can also think of it as the factor by which some prior odds have been updated after observing the data to posterior odds. Since they can be computed by dividing posterior odds by prior odds.

$$\underbrace{\frac{P(M_1|D)}{P(M_2|D)}}_{\text{Posterior Odds}} = \underbrace{\frac{P(D|M_1)}{P(D|M_2)}}_{\text{Likelihood Ratio}} \times \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{Prior Odds}}$$


$$BF_{12}=\frac{Posterior~Odds_{12}}{Prior~Odds_{12}}$$

---

```{r, message = FALSE, echo = FALSE}
library(bayestestR)
library(see)
prior <- distribution_normal(10000, mean = 0, sd = 1)
posterior <- distribution_normal(10000, mean = 1, sd = 0.7)
bf_plot<- bayesfactor_parameters(posterior, prior, direction = "two-sided", null = 0)
plot(bf_plot)
```

---
```{r}
bayesfactor_parameters(posterior, prior, direction = "two-sided", null = 0)
```
This BF indicates that the data provide .4/0.2 = 2 times more evidence for the effect compared to a model without the effect. 

---
### BF interpretation 
Extent to which the data sway our relative belief from one hypothesis to the other  

Strength of evidence from data about the hypotheses

Relative predictive accuracy of one hypothesis over another

Has the null hypothesis of an absence of an effect become more or less credible?


---

```{r}
library(effectsize)
interpret_bf(1.95)
```



---
What happens if we increase our prior SD? Going from 1 to 5? 
Is our BF going to increase or decrease? 
```{r}
prior2 <- distribution_normal(10000, mean = 0, sd = 5)
posterior2 <- distribution_normal(10000, mean = 1, sd = 0.7)
bf_plot2<- bayesfactor_parameters(posterior2, prior2, direction = "two-sided", null = 0)
```

---
```{r}
plot(bf_plot2)
```


---

```{r}
bayesfactor_parameters(posterior2, prior2, direction = "two-sided", null = 0)
```

---
Note that the method this is computed by is called the Savage–Dickey density ratio. Literally just taking the fraction of these point estimates. Crucially, however, it ignores the shape of the distribution, focusing on a single point estimate. 

This is good in that it parallels NHST conventions. This is bad in that it doesn't tell you much much outside of a specific point estimate. In other words you are ignoring the shape of the posterior. 

---
```{r}
mr.2 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.2")
```


---
```{r}
summary(mr.2)
```


---

```{r}
bf.result <- bayestestR::bayesfactor_parameters(mr.2)
bf.result
```


---
```{r}
plot(bf.result)
```


---
## we can also do this with ROPE! 
```{r}
bf.result2 <- bayesfactor_parameters(mr.2, null = rope_range(mr.2))
bf.result2 
```

---
.pull-left[
The Bayes factor represents the degree by which the distribution mass of the posterior has shifted outside or inside the null interval relative to the prior distribution
]

.pull-right[
```{r}
plot(bf.result2)
```

]

---
### using hypothesis
divides the posterior by the prior (1/evidence to flip)

```{r}
hypothesis(mr.2, "FQ_c = 0")
```

---

```{r}
plot(hypothesis(mr.2, "FQ_c = 0"))
```


---

```{r}
mr.13 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c * FQ_c,
      prior = c(prior(normal(0, 3), class = Intercept),
                prior(normal(0, 2), class = b, coef = SS_c),
                prior(normal(0, 2), class = b, coef = FQ_c),
                prior(normal(0, 2), class = b, coef = SS_c:FQ_c),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      file = "mr.13")
```


---
### Proposed cutoffs
.small[
| BF10         | Interpretation               |
|--------------|------------------------------|
| > 100        | Extreme evidence for H1      |
| 30 - 100     | Very strong evidence for H1  |
| 10 - 30      | Strong evidence for H1       |
| 3 - 10       | Moderate evidence for H1     |
| 1 - 3        | Anecdotal evidence for H1    |
| 1            | Equal evidence for H1 and H0 |
| 1/3 - 1      | Anecdotal evidence for H0    |
| 1/10 - 1/3   | Moderate evidence for H0     |
| 1/30 - 1/10  | Strong evidence for H0       |
| 1/100 - 1/30 | Very strong evidence for H0  |
| < 1/100      | Extreme evidence for H0      |
]

---
## Shortcomings of Bayes Factors

.small[
1. Heavy reliance on priors. The marginal likelihood is an average taken with respect to the prior, so bayes factors can be seen as relatively subjective compared to alternative approaches. This is less so a problem for models with more straightforward "nulls" eg a correct/incorrect and less so for more continuous metrics

2. BF provide differences between models without quantifying whether the chosen model is any good (similar issues with waic, loo, p-values -- you can calculate BF with bic!)

3. Favors more parsimonious models and thus is more conservative (could also be a pro) 

4. Potentially reinforces dichotomous thinking 

5. May be used as a measure of effect size (similar shortfalls to using p as such)

6. Null is mostly not useful (!)
]

---


```{r}
info.1 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, 2), class = b, coef = SS_c),
                prior(normal(0, 2), class = b, coef = FQ_c),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      save_pars = save_pars(all = TRUE),
      file = "info.1")
```

```{r}
info.2 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c + FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      save_pars = save_pars(all = TRUE),
      file = "info.2")
```

---
.pull-left[
```{r}
bf.result <- bayestestR::bayesfactor_parameters(info.1)
bf.result
```

]

.pull-right[
```{r}
bf.result <- bayestestR::bayesfactor_parameters(info.2)
bf.result
```
]
 
 
---

###Model comparisons

```{r}

info.3 <- 
  brm(family = gaussian,
      happiness ~ 1 + SS_c * FQ_c,
      prior = c(prior(normal(5, 2), class = Intercept),
                prior(normal(0, .2), class = b, coef = SS_c),
                prior(normal(0, .2), class = b, coef = FQ_c),
                prior(exponential(.5), class = sigma)),
      iter = 2000, warmup = 1000, chains = 2, cores = 2,
      data = MR,
      sample_prior = T,
      save_pars = save_pars(all = TRUE),
      file = "info.3")
```






---
via {bayestestR}
```{r}

comparison <- bayesfactor_models(info.3, denominator = info.2)
comparison
```


